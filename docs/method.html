<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.552">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Overview of Methodology</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
        <div class="quarto-navbar-tools tools-end">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/sarkadava/FLESH_Effort">
            Source Code
            </a>
          </li>
          <li>
            <a class="dropdown-item quarto-navbar-tools-item" href="https://github.com/sarkadava/FLESH_Effort/issues">
            Report a Bug
            </a>
          </li>
      </ul>
    </div>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./method.html">Overview of Methods</a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./method.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Overview of Methods</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Processing</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01_XDF_processing/xdf_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Pre-Processing I: from XDF to raw files</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_MotionTracking_processing/01_Video_preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion tracking I: Preparation of videos</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_MotionTracking_processing/02_Track_OpenPose.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion tracking II: 2D pose estimation via OpenPose</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_MotionTracking_processing/03_Track_pose2sim.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion tracking III: Triangulation via Pose2sim</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02_MotionTracking_processing/04_Track_InverseKinDyn.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Motion tracking IV: Modeling inverse kinematics and dynamics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_TS_processing/01_TS_processing_motion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Processing I: Motion tracking and balance</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_TS_processing/02_TS_processing_acoustics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Processing II: Acoustics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03_TS_processing/03_TS_merging.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Processing III: Merging multimodal data</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Time Series Annotation</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_TS_movementAnnotation/01_Classify_preparation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Movement annotation I: Preparing training data and data for classifier</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_TS_movementAnnotation/02_MovementClassifier.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Movement annotation II: Training movement classifier, and annotating timeseries data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04_TS_movementAnnotation/03_InterAgreement.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Movement annotation III: Computing interrater agreement between manual and automatic annotation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05_finalMerge/TS_mergeAnnotations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Final merge</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06_ConceptSimilarity/ConceptNet_similarity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Computing concept similarity using ConceptNet word embeddings</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07_TS_featureExtraction/TS_extraction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Extraction of effort-related features</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_Analysis_XGBoost/01_PCA_featureDimensions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exploratory Analysis I: Using PCA to identify effort dimensions</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08_Analysis_XGBoost/02_XGBoost_effortIndicators.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution via XGBoost</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09_Analysis_Modeling/Modelling_syntheticData.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#experimental-setup-and-procedure" id="toc-experimental-setup-and-procedure" class="nav-link active" data-scroll-target="#experimental-setup-and-procedure">Experimental setup and procedure</a>
  <ul class="collapse">
  <li><a href="#procedure" id="toc-procedure" class="nav-link" data-scroll-target="#procedure">Procedure</a></li>
  <li><a href="#stimuli" id="toc-stimuli" class="nav-link" data-scroll-target="#stimuli">Stimuli</a></li>
  <li><a href="#lab-equipment" id="toc-lab-equipment" class="nav-link" data-scroll-target="#lab-equipment">Lab equipment</a></li>
  </ul></li>
  <li><a href="#data-processing" id="toc-data-processing" class="nav-link" data-scroll-target="#data-processing">Data processing</a>
  <ul class="collapse">
  <li><a href="#processing-xdf-file-to-trial-sized-data-streams" id="toc-processing-xdf-file-to-trial-sized-data-streams" class="nav-link" data-scroll-target="#processing-xdf-file-to-trial-sized-data-streams">Processing XDF file to trial-sized data streams</a></li>
  <li><a href="#motion-tracking" id="toc-motion-tracking" class="nav-link" data-scroll-target="#motion-tracking">Motion tracking</a></li>
  <li><a href="#extraction-of-acoustic-features" id="toc-extraction-of-acoustic-features" class="nav-link" data-scroll-target="#extraction-of-acoustic-features">Extraction of acoustic features</a></li>
  <li><a href="#derivatives-and-aggregation-of-all-data" id="toc-derivatives-and-aggregation-of-all-data" class="nav-link" data-scroll-target="#derivatives-and-aggregation-of-all-data">Derivatives and aggregation of all data</a></li>
  <li><a href="#movement-annotation" id="toc-movement-annotation" class="nav-link" data-scroll-target="#movement-annotation">Movement annotation</a></li>
  <li><a href="#sound-annotation" id="toc-sound-annotation" class="nav-link" data-scroll-target="#sound-annotation">Sound annotation</a></li>
  <li><a href="#final-merge" id="toc-final-merge" class="nav-link" data-scroll-target="#final-merge">Final merge</a></li>
  <li><a href="#concept-similarity" id="toc-concept-similarity" class="nav-link" data-scroll-target="#concept-similarity">Concept similarity</a></li>
  <li><a href="#feature-dataset" id="toc-feature-dataset" class="nav-link" data-scroll-target="#feature-dataset">Feature dataset</a></li>
  </ul></li>
  <li><a href="#data-analysis" id="toc-data-analysis" class="nav-link" data-scroll-target="#data-analysis">Data analysis</a>
  <ul class="collapse">
  <li><a href="#top-down-analysis-confirmatory" id="toc-top-down-analysis-confirmatory" class="nav-link" data-scroll-target="#top-down-analysis-confirmatory">Top-down analysis (confirmatory)</a></li>
  <li><a href="#bottom-up-analysis-exploratory" id="toc-bottom-up-analysis-exploratory" class="nav-link" data-scroll-target="#bottom-up-analysis-exploratory">Bottom-up analysis (exploratory)</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Overview of Methodology</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Here we lay out the conceptual overview of the methods we use to collect, process and analyze the data. The methods are preregistered at <a href="https://osf.io/8ajsg">OSF Registries</a>. Where applicable, we refer to the concrete code executing the steps described. <br></p>
<section id="experimental-setup-and-procedure" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setup-and-procedure">Experimental setup and procedure</h2>
<section id="procedure" class="level3">
<h3 class="anchored" data-anchor-id="procedure">Procedure</h3>
<p>This experiment consists of two parts (see <a href="https://osf.io/3nygq">method preregistration</a>), separated by a short break. In both parts, participants form dyads and alternate between the roles of performer and guesser. The performer expresses a concept, while the guesser attempts to identify it. This study focuses on Part 2, where both participants see whether the guesser’s answer was correct. Additionally, the performer sees the exact response and has an opportunity to repair her original performance. Each participant performs novel expressions of 21 concepts, totaling 42 concepts per dyad, across three within-subject Modality conditions: vocal, gesture, and combined. The order of Modality conditions is randomized, and each is introduced with instructions, followed by two practice concepts and seven trials.</p>
<p>After each performance, the guesser types their answer. If correct, they proceed to the next concept. If incorrect, both see the incorrect response, and the performer repeats the concept while the guesser makes another attempt. Up to two repair attempts are allowed before moving on. Correctness is judged by the experimenter, who checks only for typos; synonyms are not accepted to ensure consistency across sessions. Participants switch roles within and between conditions.</p>
</section>
<section id="stimuli" class="level3">
<h3 class="anchored" data-anchor-id="stimuli">Stimuli</h3>
<p>The stimuli were selected from a list of 206 concepts. This list included 100 Leipzig-Jakarta List concepts <span class="citation" data-cites="tadmorBorrowabilityNotionBasic2010">(<a href="#ref-tadmorBorrowabilityNotionBasic2010" role="doc-biblioref">Tadmor, Haspelmath, and Taylor 2010</a>)</span> and 100 concepts varying in sensory expressibility <span class="citation" data-cites="lynottModalityExclusivityNorms2009 lynottLancasterSensorimotorNorms2020">(<a href="#ref-lynottModalityExclusivityNorms2009" role="doc-biblioref">Lynott and Connell 2009</a>; <a href="#ref-lynottLancasterSensorimotorNorms2020" role="doc-biblioref">Lynott et al. 2020</a>)</span>. In a previous experiment - similar to <span class="citation" data-cites="kadava_etal24">Kadavá et al. (<a href="#ref-kadava_etal24" role="doc-biblioref">2024</a>)</span> -, 227 native Dutch participants rated the current concepts on a continuous scale for how well they could be communicated without language across three modalities: gesture, vocal, and combined.</p>
<p>From the 206-item list, we excluded concepts with any expressibility value below a threshold (mean minus one standard deviation) to avoid low-expressible concepts. Using a custom Python script, we constructed three modality-specific 28-item lists of top-ranked expressible concepts, ensuring no overlaps between them. Body-related concepts (e.g., ‘tooth,’ ‘ear,’ ‘tongue’) were excluded to prevent indexical resolution and replaced with taste-related concepts of generally low expressibility, aligning with secondary research interests.</p>
<p>The final 84-item list maintains expressibility statistics comparable to the original list. Each participant performs seven concepts per modality condition. Stimuli lists were pre-randomized and controlled for balanced occurrences across sessions (each concept appears at least 10 times, or 12 if additional dyads are included). Finally, each list was again checked for mean expressibility values to prevent clustering of low-expressible concepts.</p>
</section>
<section id="lab-equipment" class="level3">
<h3 class="anchored" data-anchor-id="lab-equipment">Lab equipment</h3>
<p>Motion Tracking &amp; Video Recording: The recording setup consists of three Elgato Facecam cameras on a movable arch that record at 60 fps with a 1/200s shutter speed to reduce motion blur (ISO 354). A custom Python script (using <a href="https://opencv.org/">OpenCV</a> and <a href="https://github.com/chenxinfeng4/ffmpegcv">ffmpegcv</a>) captures and writes videos <span class="citation" data-cites="kadava_etal24a">(<a href="#ref-kadava_etal24a" role="doc-biblioref">Kadavá, Snelders, and Pouw 2024</a>)</span>. A fourth Logitech webcam (1960x1080, 60 fps) records a high-quality frontal view for general observation.</p>
<p>Balance Board: Designed by the Donders Institute, it incorporates modified Wii Balance Board sensors, synchronized to 1 ms temporal and submillimeter spatial accuracy. Data is collected at 400 Hz via a National Instruments USB-62221 A/D card.</p>
<p>Audio Recording: A C520 head-mounted condenser mic records via a DAP PRE-202 amplifier (gain: 25%). The signal is split: 16 kHz LSL stream is recorded via a Linux-based Minux system and 48 kHz via another PC (using Audacity). The 48 kHz signal is used for acoustic feature extraction. A second split feeds Sony WH-1000XM5 noise-canceling headphones to ensure that the guesser hears the performer clearly despite the separating curtain.</p>
<p>One-Way Screen: One-way screen that has been designed by the Technical Support Group at the Max Planck Institute for Psycholinguistics, similar to <span class="citation" data-cites="trujilloSpeakersExhibitMultimodal2021">Trujillo et al. (<a href="#ref-trujilloSpeakersExhibitMultimodal2021" role="doc-biblioref">2021</a>)</span>. It serves to minimize nonverbal feedback while maintaining co-presence.</p>
<p>Experiment Software: The experiment runs via a custom Python script (<a href="https://github.com/psychopy/psychopy">PsychoPy</a>, <a href="https://www.socsci.ru.nl/wilberth/python/rusocsci.html">RuSocSci</a>), controlling a buttonbox and logging trial accuracy in CSV format.</p>
<p>Data Synchronization: LabStreamLayer (<a href="https://github.com/sccn/labstreaminglayer">LSL</a>) synchronizes the microphone, cameras, balance board, and buttonbox markers (e.g., role changes, trial start). Data is stored in XDF format.</p>
<p>Concrete specifics for all equipment can be found in the <a href="https://osf.io/3nygq">method preregistration</a>.</p>
<p><br></p>
</section>
</section>
<section id="data-processing" class="level2">
<h2 class="anchored" data-anchor-id="data-processing">Data processing</h2>
<p>In this preregistration, we preregister processing and analysis pipeline that has been developed using the pilot data which we call dyad 0. The actual data is not disclosed before preregistering.</p>
<section id="processing-xdf-file-to-trial-sized-data-streams" class="level3">
<h3 class="anchored" data-anchor-id="processing-xdf-file-to-trial-sized-data-streams">Processing XDF file to trial-sized data streams</h3>
<p>Each session resulted in one XDF file containing all recorded streams (i.e., video frame stream, balance board stream, audio stream), which was read and processed via custom Python scripts.</p>
<p>To cut the stream-specific post-processed (e.g., smoothed) time series into trials, we used the buttonbox timestamps to isolate trial segments in each stream. Since the buttonbox timestamps correspond with inputs to our PsychoPy experiment script, we were able to automatically retrieve metadata for each trial segment (e.g., condition, participant number, etc.).</p>
<p>The audio stream was converted into a sound file. The video frame stream is matched with the recorded video by the range of frames of each trial and cut accordingly. However, since the buttonbox events are administered manually by the experimenter, video trials need to be visually inspected and, if needed, adjusted to correct for late starts or early ends.</p>
<p>Lastly, we align the externally recorded 48 kHz audio to the 16 kHz audio, which is synchronized to the rest of the LSL data using cross-correlation and then cut again to the individual trials <span class="citation" data-cites="nalbantoglu_kadava25">(<a href="#ref-nalbantoglu_kadava25" role="doc-biblioref">Nalbantoğlu and Kadavá 2025</a>)</span>. By doing that, we can use the high-quality audio for further analysis, while still having all streams synchronized.</p>
<p><strong>Go to script:</strong> <a href="./01_XDF_processing/xdf_workflow.html">Pre-Processing I: from XDF to raw files</a></p>
</section>
<section id="motion-tracking" class="level3">
<h3 class="anchored" data-anchor-id="motion-tracking">Motion tracking</h3>
<section id="openpose" class="level4">
<h4 class="anchored" data-anchor-id="openpose">OpenPose</h4>
<p>First, we cut each trial video into three individual videos per camera. Each video is then processed with motion capture using OpenPose <span class="citation" data-cites="cao_etal19">(<a href="#ref-cao_etal19" role="doc-biblioref">Cao et al. 2019</a>)</span>. Using the 135 marker model, we obtain a skeleton with <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/41d8c087459fae844e477dda50a6f732e70f2cb8/src/openpose/pose/poseParameters.cpp#L149">135 body keypoints</a>, including hands and face, with a sampling rate of 60 Hz.</p>
<p><strong>Go to script:</strong> <a href="./02_MotionTracking_processing/01_Video_preparation.html">Motion tracking I: Preparation of videos</a><br> <strong>Go to script:</strong> <a href="./02_MotionTracking_processing/02_Track_OpenPose.html">Motion tracking II: 2D pose estimation via OpenPose</a></p>
</section>
<section id="pose2sim" class="level4">
<h4 class="anchored" data-anchor-id="pose2sim">Pose2sim</h4>
<p>OpenPose retrieves 2D skeleton data for each camera. To transfer multiple 2D data streams to 3D position data we use a method called triangulation based on calibrated cameras. Triangulation is performed using Pose2sim <span class="citation" data-cites="pagnon_etal22">(<a href="#ref-pagnon_etal22" role="doc-biblioref">Pagnon, Domalain, and Reveret 2022</a>)</span>. To be able to triangulate all 2D skeleton data, we first calibrate the cameras based on intrinsic and extrinsic angles with a checkerboard, with which short recordings were made for each session. For intrinsic calibration, we obtain an error of 0.24 pixels for each camera (recommended below 0.5 pixels). Residual calibration errors for dyad 0 are 1.8, 1.5, and 1.9 cm for each camera, respectively. The mean reprojection error for all points on all frames across all trials of dyad 0 is 1.2 cm (below 1 cm recommended, but acceptable until 2.5 cm). The triangulated data are directly smoothed with a built-in function by 4th-order, 10Hz low-pass, zero-phase Butterworth filter.</p>
<p><strong>Go to script:</strong> <a href="./02_MotionTracking_processing/03_Track_pose2sim.html">Motion tracking III: Triangulation via Pose2sim</a> <br></p>
</section>
<section id="opensim" class="level4">
<h4 class="anchored" data-anchor-id="opensim">OpenSim</h4>
<p>To obtain inverse kinematics and dynamics, we use the OpenSim package <span class="citation" data-cites="seth_etal18">(<a href="#ref-seth_etal18" role="doc-biblioref">Seth et al. 2018</a>)</span>. First, we scale the 135 model using the static T-pose of a participant and information about their body mass (i.e., height and weight) to create a body model for each individual. The weights are kept at the default values for this model. The scaled model is then used to calculate joint angles for each trial (represented by the coordinates obtained in the previous step). Joint angles are then used to obtain joint moments/torques. To prevent amplification of noise in solving inverse dynamics, we first smooth the joint-angle data using a Savitzky-Golay 3rd-order polynomial filter with a span of 560 ms.</p>
<p>During motion feature extraction, we keep the sampling rate of all motion data at the original values (i.e., 60 Hz), upsampling to 500 Hz only at a later stage when merging all multimodal signals.</p>
<p><strong>Go to script:</strong> <a href="./02_MotionTracking_processing/04_Track_InverseKinDyn.html">Motion tracking IV: Modeling inverse kinematics and dynamics</a> <br></p>
</section>
</section>
<section id="extraction-of-acoustic-features" class="level3">
<h3 class="anchored" data-anchor-id="extraction-of-acoustic-features">Extraction of acoustic features</h3>
<p>We use the high-sampling 48 kHz audio data to extract acoustic features. These include fundamental frequency, amplitude envelope, voice quality measurements, and formants.</p>
<p>To extract the amplitude envelope of the acoustic signal, we follow a method by <span class="citation" data-cites="tilsen_arvaniti13">Tilsen and Arvaniti (<a href="#ref-tilsen_arvaniti13" role="doc-biblioref">2013</a>)</span>, implemented in Python by <span class="citation" data-cites="pouw24">Pouw (<a href="#ref-pouw24" role="doc-biblioref">2024</a>)</span>. We use a bandpass and 2nd-order, 10Hz low-pass, zero-phase Butterworth filter.</p>
<p>Fundamental frequency was extracted using the Python package parselmouth <span class="citation" data-cites="jadoul_etal18">(<a href="#ref-jadoul_etal18" role="doc-biblioref">Jadoul, Thompson, and de Boer 2018</a>)</span>. Based on sex, the F0 range was limited to 186-381 Hz (female) or 75-300 Hz (male). The resulting f0 contours were smoothed with a Savitzky-Golay 3rd-order polynomial filter with a span of 50 ms applied to continuous runs of phonated speech to maintain discontinuities typical of the f0 signal.</p>
<p>To account for the spectral properties of the acoustic signal, we calculated the center of gravity using Python package parselmouth, filtering out the fundamental frequency using a notch filter.</p>
<p>Formants were extracted in Praat <span class="citation" data-cites="boersma_weenink25">(<a href="#ref-boersma_weenink25" role="doc-biblioref">Boersma and Weenink 2025</a>)</span>, using Chris Carignan’s optimization method (see <a href="https://github.com/ChristopherCarignan/formant-optimization">Github</a>). To increase the reliability, we have kept formants only in those formant segments that contain fundamental frequency or happen within a peak of a vocalic energy amplitude..</p>
<p>Note that we keep the sampling rate of all acoustic feature time series data at the original values. We do, however, downsample all of them to 500 Hz at a later stage when merging all signals into a single dataframe per trial.</p>
</section>
<section id="derivatives-and-aggregation-of-all-data" class="level3">
<h3 class="anchored" data-anchor-id="derivatives-and-aggregation-of-all-data">Derivatives and aggregation of all data</h3>
<section id="kinematics" class="level4">
<h4 class="anchored" data-anchor-id="kinematics">Kinematics</h4>
<p>Coordinates from 3D skeleton data were interpolated and smoothed with a Savitzky-Golay 3rd-polynomial filter with a span of 400 ms for positional data of upper-body keypoints, and with a 1st-polynomial filter with a span of ca. 800 ms for positional data of lower-body keypoints. The difference in filter settings was chosen after inspection of the video data alongside the coordinates. This revealed more severe error measurement on the lower-body keypoints, mainly due to their occlusion by clothes. Additionally, as lower body key points tend to stay relatively motionless, they are more prone to noise. We then differentiate all signals with respect to time to retrieve the 3D speed (in cm/s), 2D vertical velocity (cm/s), 3D acceleration (cm/s^2), and 3D jerk (cm/s^3). The derivatives are further smoothed with a Savitzky-Golay 3rd-order polynomial filter with a span of 400 ms.</p>
<p>To create aggregated groups of derivatives for body segments (e.g., speed of whole arm), we compute an Euclidean sum over a derivative of all key points belonging to a group.</p>
</section>
<section id="inverse-kinematics-and-dynamics" class="level4">
<h4 class="anchored" data-anchor-id="inverse-kinematics-and-dynamics">Inverse kinematics and dynamics</h4>
<p>Joint angle data and moment data were smoothed with a Savitzky-Golay 1st-order polynomial filter with a span of 560 ms. Further, we obtained derivatives of the joint angular data, namely joint angle speed (in rad/s), joint angle acceleration (in rad/s^2), joint angle jerk (in rad/s^3), and moment change (in Nm/s). We smoothed all derivatives with a Savitzky-Golay 1st-order polynomial filter with a span of 560 ms. Similar to the kinematic measures, we have created aggregated measures for the same groups of keypoints.</p>
</section>
<section id="balance" class="level4">
<h4 class="anchored" data-anchor-id="balance">Balance</h4>
<p>We computed the change in 2D magnitude in the center of pressure and smoothed it using the Savitzky-Golay 5th-order polynomial filter with a span of 102 ms.</p>
<p><strong>Go to script:</strong> <a href="./03_TS_processing/01_TS_processing_motion.html">Processing I: Motion tracking and balance</a> <br></p>
</section>
<section id="acoustics" class="level4">
<h4 class="anchored" data-anchor-id="acoustics">Acoustics</h4>
<p>We differentiated the amplitude envelope of the acoustic signal to obtain the first derivative, the change in amplitude.</p>
<p><strong>Go to script:</strong> <a href="./03_TS_processing/02_TS_processing_acoustics.html">Processing II: Acoustics</a> <br></p>
</section>
<section id="merging-resampling" class="level4">
<h4 class="anchored" data-anchor-id="merging-resampling">Merging &amp; resampling</h4>
<p>All time series were merged on a common sampling rate of 500 Hz.</p>
<p><strong>Go to script:</strong> <a href="./03_TS_processing/03_TS_merging.html">Processing III: Merging multimodal data</a> <br></p>
</section>
</section>
<section id="movement-annotation" class="level3">
<h3 class="anchored" data-anchor-id="movement-annotation">Movement annotation</h3>
<section id="manual-annotation" class="level4">
<h4 class="anchored" data-anchor-id="manual-annotation">Manual annotation</h4>
<p>For this pre-registration, all trial-sized videos of dyad 0 were annotated for communicative movement in ELAN <span class="citation" data-cites="wittenburg_etal06">(<a href="#ref-wittenburg_etal06" role="doc-biblioref">Wittenburg et al. 2006</a>)</span> by two human annotators. A common annotation scheme consists of four tiers for arms, upper body, lower body, and head. A fifth parent tier concatenates all movement in the trial from the beginning to the end.</p>
<p>We define communicative movement as any gesture that can be attributed to the performance. Because participants were asked to signal the start and end of their performance with locked hands in front of their body, the communicative movement usually starts when people’s arms depart from this position (or other idiosyncratic resting positions) and when they return to it once again. If locking of the hands was missing, the annotators were asked to annotate the start of the movement as a moment where any body part initiates a communicative movement (i.e., when a body part leaves resting position). Similarly, the end of a movement would be signalled by retraction to a resting position.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/ELAN_example.PNG" class="img-fluid figure-img"></p>
<figcaption>Figure 1. Example of manual annotation in ELAN.</figcaption>
</figure>
</div>
</section>
<section id="training-a-movement-classifier" class="level4">
<h4 class="anchored" data-anchor-id="training-a-movement-classifier">Training a movement classifier</h4>
<p>Since the whole dataset contains over 8,000 trials, we have decided to use the manual annotations created for dyad 0 (and further extended, see below) to train a classifier that can automatically annotate a timeseries into movement and no movement. For that, we have created a training dataset containing summarizing features – namely mean, standard deviation, minimum, and maximum – of all movement timeseries in our data (i.e., kinematics, joint angles, moments) and some extra relational features such as the distance between finger indices, distance between a hip and a wrist, distance between a head and a hip an distance between head and an ankle. We fitted four logistic regression models, one for each tier (i.e., arms, upper body, head, lower body), with ground truth values represented by manual annotators. Seventy-five percent of the data was used for training, twenty-five percent for testing. The accuracy on test sets for arm, head, upper body and lower body movement is 0.96, 0.86, 0.86, and 0.89, respectively.</p>
<p>Four classifiers were then used to annotate pre-processed trial timeseries that were summarized into 100 ms chunks with a 25 ms sliding window. Together with the predicted value, we obtained the confidence of the model (i.e., confidence in giving a correct label of movement). Finally, we smoothed the confidence and averaged over the overlapping segments. A threshold of 60% confidence was set as the border between the movement and no movement labels.</p>
<p>To avoid event flickering in the classifications, we have further cleaned the annotations following two rules: 1) if there is a no movement segment between two movement segments that is shorter than 200 ms, it is changed into movement; 2) if there is a movement segment between two no movement segments that is shorter than 200 ms, it is changed into movement. Finally, everything from the first movement to the last movement within a tier was annotated as one movement unit, as we are interested in the whole, holistic movement behavior rather than its segments.</p>
<p><strong>Go to script:</strong> <a href="./04_TS_movementAnnotation/01_Classify_preparation.html">Movement annotation I: Preparing training data and data for classifier</a><br> <strong>Go to script:</strong> <a href="./04_TS_movementAnnotation/02_MovementClassifier.html">Movement annotation II: Training movement classifier, and annotating timeseries data</a> <br></p>
</section>
<section id="interrater-agreement" class="level4">
<h4 class="anchored" data-anchor-id="interrater-agreement">Interrater agreement</h4>
<p>Reliability for overlap in identifying the same time events (movement versus no movement) was calculated using EasyDIAG <span class="citation" data-cites="holle_rein15">(<a href="#ref-holle_rein15" role="doc-biblioref">Holle and Rein 2015</a>)</span> between human annotators and annotation models. Table 1 provides a summary of the interrater agreement between manual annotators R1 and R3 and between manual annotator R1 and the model.</p>
<div id="5a3ee94c" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display" data-execution_count="1">
<div>
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">comparison</th>
<th data-quarto-table-cell-role="th">tier</th>
<th data-quarto-table-cell-role="th">linked_units</th>
<th data-quarto-table-cell-role="th">raw_agreement</th>
<th data-quarto-table-cell-role="th">kappa</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_Auto</td>
<td>arms_60</td>
<td>0.81</td>
<td>0.81</td>
<td>0.65</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_Auto</td>
<td>arms_80</td>
<td>0.81</td>
<td>0.81</td>
<td>0.64</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_Auto</td>
<td>head_60</td>
<td>0.60</td>
<td>0.56</td>
<td>0.32</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_Auto</td>
<td>head_80</td>
<td>0.63</td>
<td>0.58</td>
<td>0.34</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_Auto</td>
<td>lower_60</td>
<td>0.80</td>
<td>0.79</td>
<td>0.59</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_Auto</td>
<td>lower_80</td>
<td>0.75</td>
<td>0.73</td>
<td>0.50</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_Auto</td>
<td>upper_60</td>
<td>0.67</td>
<td>0.66</td>
<td>0.44</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_Auto</td>
<td>upper_80</td>
<td>0.67</td>
<td>0.67</td>
<td>0.43</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>R3_Auto</td>
<td>arms_60</td>
<td>0.75</td>
<td>0.75</td>
<td>0.58</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>R3_Auto</td>
<td>arms_80</td>
<td>0.73</td>
<td>0.72</td>
<td>0.53</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>R3_Auto</td>
<td>head_60</td>
<td>0.62</td>
<td>0.60</td>
<td>0.39</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>R3_Auto</td>
<td>head_80</td>
<td>0.64</td>
<td>0.61</td>
<td>0.39</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>R3_Auto</td>
<td>lower_60</td>
<td>0.66</td>
<td>0.64</td>
<td>0.38</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>R3_Auto</td>
<td>lower_80</td>
<td>0.70</td>
<td>0.68</td>
<td>0.41</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>R3_Auto</td>
<td>upper_60</td>
<td>0.67</td>
<td>0.66</td>
<td>0.44</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>R3_Auto</td>
<td>upper_80</td>
<td>0.63</td>
<td>0.61</td>
<td>0.37</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_R3</td>
<td>arms</td>
<td>0.85</td>
<td>0.84</td>
<td>0.70</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_R3</td>
<td>head</td>
<td>0.67</td>
<td>0.62</td>
<td>0.38</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_R3</td>
<td>lower</td>
<td>0.74</td>
<td>0.71</td>
<td>0.46</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">0</td>
<td>R1_R3</td>
<td>upper</td>
<td>0.70</td>
<td>0.68</td>
<td>0.44</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src=".png" class="img-fluid figure-img"></p>
<figcaption>Table 1. Summary of interrater agreements</figcaption>
</figure>
</div>
<p>The current model reaches substantial agreement - as defined by <span class="citation" data-cites="landis_koch77">Landis and Koch (<a href="#ref-landis_koch77" role="doc-biblioref">1977</a>)</span> - with a human annotator only for arms. To improve its predictions for head, upper body, and lower body, and to avoid the risk of overfitting the model on a specific type of behaviour generated by individuals in dyad 0, we will extend the training data by annotating 10% of behaviour per participant per dyad before the final analysis. If kappa does not result in minimum substantial agreement (k = 0.61), we will annotate a larger portion of the data. However, note that we are mainly interested in upper limb movement, hence, potential difficulties with automatic annotations of the remaining body parts can be partially ignored, while acknowledging the limitations in our analysis.</p>
<p><strong>Go to script:</strong> <a href="./04_TS_movementAnnotation/03_InterAgreement.html">Movement annotation III: Computing interrater agreement between manual and automatic annotation</a> <br></p>
</section>
</section>
<section id="sound-annotation" class="level3">
<h3 class="anchored" data-anchor-id="sound-annotation">Sound annotation</h3>
<p>All sound files were automatically annotated for sound/silence annotation using Praat. These annotations were manually inspected and corrected where necessary. Similarly to movement annotation, we aim to construct an automatic annotation tool that can detect the onset and the offset of a sound. We will use a similar pipeline with acoustic features. We will consider the model successful if the agreement between (manually corrected) Praat annotations and automatic annotation tool results in a minimum substantial agreement (k = 0.61).</p>
</section>
<section id="final-merge" class="level3">
<h3 class="anchored" data-anchor-id="final-merge">Final merge</h3>
<p>At last, all movement and sound annotations were merged with the aggregated acoustic-kinematic data.</p>
<p><strong>Go to script:</strong> <a href="./05_finalMerge/TS_mergeAnnotations.html">Final merge</a> <br></p>
</section>
<section id="concept-similarity" class="level3">
<h3 class="anchored" data-anchor-id="concept-similarity">Concept similarity</h3>
<p>For each target-answer pair from the experiment, we have computed cosine similarity using the Dutch word embeddings in <a href="https://github.com/commonsense/conceptnet-numberbatch/blob/master/conceptnet-numberbatch.png">numberbatch version 19.08</a> from ConceptNet <span class="citation" data-cites="speer_etal18">(<a href="#ref-speer_etal18" role="doc-biblioref">Speer, Chin, and Havasi 2018</a>)</span>. To validate this measure as a proxy of conceptual similarity, we have collected data from 14 Dutch-native people who were asked to rate the same pairs of words in an online study in terms of how similar they felt, and compare the rated and computed (cosine) similarity. Pearson’s r coefficient of .73 reveals a strong positive correlation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/concept_similarity.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2. The relationship between the mean similarity rated by 15 human participants and the cosine similarity computed by ConceptNet.</figcaption>
</figure>
</div>
<p>Note that some answers for dyad 0 are not represented in the embeddings (e.g., two-word answers), and, therefore, we are not able to compute cosine similarity for a number of target-answer pairs. Since this is also expected for the actual data, we will use the identical rating study to collect the missing similarity values.</p>
<p><strong>Go to script:</strong> <a href="./06_ConceptSimilarity/ConceptNet_similarity.html">Computing concept similarity using ConceptNet word embeddings</a> <br></p>
</section>
<section id="feature-dataset" class="level3">
<h3 class="anchored" data-anchor-id="feature-dataset">Feature dataset</h3>
<p>For the analysis, we constructed a dataset compiling effort-related features summarized from the trial-segmented timeseries. Before that, we have z-scored each timeseries by participant. This is to conceptualize effort as a relative measure rather than absolute and account for different uses of movement/vocalization ranges.</p>
<section id="top-down-analysis-features" class="level4">
<h4 class="anchored" data-anchor-id="top-down-analysis-features">Top-down analysis features</h4>
<p>To answer our two main research questions, we collect features related to change in center of pressure (COPc), arm torque change, and amplitude envelope. To investigate the cumulative dimension of effort, we collect integrals of the three time series for each trial. For an instantaneous dimension of effort, we collect the average peak value of each signal per trial. Note that these sets of measures correlate weakly or not at all (r &lt; .3).</p>
</section>
<section id="bottom-up-analysis-features" class="level4">
<h4 class="anchored" data-anchor-id="bottom-up-analysis-features">Bottom-up analysis features</h4>
<p>We further utilize an extended list of time-varying signals related to our exploratory analysis of other potential measures of effort. These signals include:</p>
<ul>
<li>3D kinematics of arms, legs, and head (positions, speed, acceleration, jerk)</li>
<li>Joint angle kinematics of arms, legs, and head (positions, velocity)</li>
<li>Joint angle dynamics of arms, legs, and head (torque, torque change)</li>
<li>Power of arms, legs, head (angular velocity × torque)</li>
<li>Spectral center of gravity</li>
<li>Fundamental frequency</li>
<li>The first three formants, F1-F3, and their rate of change (i.e., velocity)</li>
<li>Change in amplitude envelope</li>
<li>Centre of pressure and its change</li>
</ul>
<p>For each time-varying signal in our dataset relating to acoustics, kinematics, and joint dynamics, we extract a number of statistics that characterize the global, local (instantaneous), and cumulative nature of the signal in terms of effort. These are:</p>
<ul>
<li>Global mean and standard deviation</li>
<li>Amount of peaks, and their mean value and standard deviation</li>
<li>Range of values</li>
<li>Integral</li>
</ul>
<p>Further, we utilize measurements that characterize the trials beyond these statistics. These include:</p>
<ul>
<li>Intermittency (as dimensionless jerk), used in <span class="citation" data-cites="pouw_etal21">Pouw et al. (<a href="#ref-pouw_etal21" role="doc-biblioref">2021</a>)</span>;</li>
<li>Bounding box of movement volume (i.e., gesture space), used in <span class="citation" data-cites="zywiczynski_etal24">Żywiczyński et al. (<a href="#ref-zywiczynski_etal24" role="doc-biblioref">2024</a>)</span>;</li>
<li>Vowel space area (VSA), used in <span class="citation" data-cites="berisha_etal14">Berisha et al. (<a href="#ref-berisha_etal14" role="doc-biblioref">2014</a>)</span>;</li>
<li>Motor complexity (computed as the slope of PCA), similar to <span class="citation" data-cites="yan_etal20">Yan et al. (<a href="#ref-yan_etal20" role="doc-biblioref">2020</a>)</span>;</li>
<li>Number of submovements, used in <span class="citation" data-cites="pouw_etal21">Pouw et al. (<a href="#ref-pouw_etal21" role="doc-biblioref">2021</a>)</span>; <span class="citation" data-cites="trujillo_etal18">Trujillo et al. (<a href="#ref-trujillo_etal18" role="doc-biblioref">2018</a>)</span>;</li>
<li>Number of moving articulators</li>
<li>Coupling and movement symmetry of two arms (computed as a correlation between both arms’ trajectories, and as a difference in distance traveled by every joint of each arm), similar to <span class="citation" data-cites="xiongetal2002">Xiong, Quek, and Mcneill (<a href="#ref-xiongetal2002" role="doc-biblioref">2002</a>)</span>.</li>
</ul>
<p><strong>Go to script:</strong> <a href="./07_TS_featureExtraction/TS_extraction.html">Extraction of effort-related features</a> <br></p>
</section>
</section>
</section>
<section id="data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="data-analysis">Data analysis</h2>
<section id="top-down-analysis-confirmatory" class="level3">
<h3 class="anchored" data-anchor-id="top-down-analysis-confirmatory">Top-down analysis (confirmatory)</h3>
<p>All statistics were performed using R <span class="citation" data-cites="rcoreteam16">(<a href="#ref-rcoreteam16" role="doc-biblioref">Team 2016</a>)</span>. We will fit Bayesian mixed effects models, using the brms R-package <span class="citation" data-cites="burkner17">(<a href="#ref-burkner17" role="doc-biblioref">Bürkner 2017</a>)</span>, to test two hypotheses:</p>
<p>H1: Correction recruits more physical effort than the baseline performance.</p>
<p>H2: A higher degree of misunderstanding will require a performer to engage in more effortful correction.</p>
<p>For H1, we will fit six models for the investigated dependent variables: 1) arm torque integral, 2) envelope integral, 3) COPc integral, 4) arm torque peak mean, 5) envelope peak mean, 6) COPc peak mean. All models will include communicative attempt, familiarity (between a guesser and a performer), personality traits of a performer (measured with BFI), expressibility of a concept, modality, and trial number as predictors. All models will include varying slopes and intercepts for participant and concept. The models will include weakly informative priors (i.e., unbiased with respect to H0/H1).</p>
<p>For H2, we will fit six models for the investigated dependent variables: 1) change in arm torque integral, 2) change in envelope integral, 3) change in COPc integral, 4) change in arm torque peak mean, 5) change in envelope peak mean, 6) change in COPc peak mean. Change here refers to a change in a feature of effort from a performance to the following correction. All models will include cosine similarity of previous answer, familiarity, personality traits, expressibility of a concept, modality, and trial number as predictors. All models will include varying slopes and intercepts for participant and concept. The models will include weakly informative priors.</p>
<p><strong>Go to script:</strong> <a href="./09_Analysis_Modeling/Modelling_syntheticData.html">Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort</a> <br></p>
</section>
<section id="bottom-up-analysis-exploratory" class="level3">
<h3 class="anchored" data-anchor-id="bottom-up-analysis-exploratory">Bottom-up analysis (exploratory)</h3>
<p>The exploratory analysis has two steps. First, using a combination of Principal Component Analysis and Extreme Gradient Boosting <span class="citation" data-cites="chen_guestrin16">(<a href="#ref-chen_guestrin16" role="doc-biblioref">Chen and Guestrin 2016</a>)</span>, we identify features that contribute the most to the difference between baseline, first correction, and second correction along different dimensions (i.e., principal components). The three most contributing features per principal component are then selected for statistical modelling. The Bayesian mixed effects models will have the same structure as the models addressing H1.</p>
<p><strong>Go to script:</strong> <a href="./08_Analysis_XGBoost/01_PCA_featureDimensions.html">Exploratory Analysis I: Using PCA to identify effort dimensions</a><br> <strong>Go to script:</strong> <a href="./08_Analysis_XGBoost/02_XGBoost_effortIndicators.html">Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution</a> <br></p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-berisha_etal14" class="csl-entry" role="listitem">
Berisha, Visar, Steven Sandoval, Rene Utianski, Julie Liss, and Andreas Spanias. 2014. <span>“Characterizing the Distribution of the Quadrilateral Vowel Space Area.”</span> <em>The Journal of the Acoustical Society of America</em> 135 (1): 421–27. <a href="https://doi.org/10.1121/1.4829528">https://doi.org/10.1121/1.4829528</a>.
</div>
<div id="ref-boersma_weenink25" class="csl-entry" role="listitem">
Boersma, Paul, and David Weenink. 2025. <span>“Praat: Doing Phonetics by Computer.”</span> <a href="http://www.praat.org/">http://www.praat.org/</a>.
</div>
<div id="ref-burkner17" class="csl-entry" role="listitem">
Bürkner, Paul-Christian. 2017. <span>“<span class="nocase">brms</span>: An <span>R</span> Package for <span>Bayesian</span> Multilevel Models Using <span>Stan</span>.”</span> <em>Journal of Statistical Software</em> 80 (1): 1–28. <a href="https://doi.org/10.18637/jss.v080.i01">https://doi.org/10.18637/jss.v080.i01</a>.
</div>
<div id="ref-cao_etal19" class="csl-entry" role="listitem">
Cao, Zhe, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2019. <span>“<span>OpenPose</span>: <span>Realtime Multi-Person 2D Pose Estimation</span> Using <span>Part Affinity Fields</span>.”</span> 2019. <a href="https://doi.org/10.48550/arXiv.1812.08008">https://doi.org/10.48550/arXiv.1812.08008</a>.
</div>
<div id="ref-chen_guestrin16" class="csl-entry" role="listitem">
Chen, Tianqi, and Carlos Guestrin. 2016. <span>“<span>XGBoost</span>: <span>A Scalable Tree Boosting System</span>.”</span> In <em>Proceedings of the 22nd <span>ACM SIGKDD International Conference</span> on <span>Knowledge Discovery</span> and <span>Data Mining</span></em>, 785–94. <span>KDD</span> ’16. New York, NY, USA: Association for Computing Machinery. <a href="https://doi.org/10.1145/2939672.2939785">https://doi.org/10.1145/2939672.2939785</a>.
</div>
<div id="ref-holle_rein15" class="csl-entry" role="listitem">
Holle, Henning, and Robert Rein. 2015. <span>“<span>EasyDIAg</span>: <span>A</span> Tool for Easy Determination of Interrater Agreement.”</span> <em>Behavior Research Methods</em> 47 (3): 837–47. <a href="https://doi.org/10.3758/s13428-014-0506-7">https://doi.org/10.3758/s13428-014-0506-7</a>.
</div>
<div id="ref-jadoul_etal18" class="csl-entry" role="listitem">
Jadoul, Yannick, Bill Thompson, and Bart de Boer. 2018. <span>“Introducing <span>Parselmouth</span>: <span>A Python</span> Interface to <span>Praat</span>.”</span> <em>Journal of Phonetics</em> 71: 1–15. <a href="https://doi.org/10.1016/j.wocn.2018.07.001">https://doi.org/10.1016/j.wocn.2018.07.001</a>.
</div>
<div id="ref-kadava_etal24" class="csl-entry" role="listitem">
Kadavá, Šárka, Aleksandra Ćwiek, Susanne Fuchs, and Wim Pouw. 2024. <span>“What Do We Mean When We Say Gestures Are More Expressive Than Vocalizations? <span>An</span> Experimental and Simulation Study.”</span> <em>Proceedings of the Annual Meeting of the Cognitive Science Society</em> 46 (0). <a href="https://escholarship.org/uc/item/2mp1v3v5">https://escholarship.org/uc/item/2mp1v3v5</a>.
</div>
<div id="ref-kadava_etal24a" class="csl-entry" role="listitem">
Kadavá, Šárka, Justin Snelders, and Wim Pouw. 2024. <span>“Recording from <span>Multiple Webcams Synchronously</span> While <span>LSL Streaming</span>.”</span> <a href="https://github.com/sarkadava/multiple_webcam_recording_for3Dtracking">https://github.com/sarkadava/multiple_webcam_recording_for3Dtracking</a>.
</div>
<div id="ref-landis_koch77" class="csl-entry" role="listitem">
Landis, J. R., and G. G. Koch. 1977. <span>“The Measurement of Observer Agreement for Categorical Data.”</span> <em>Biometrics</em> 33 (1). <a href="https://pubmed.ncbi.nlm.nih.gov/843571/">https://pubmed.ncbi.nlm.nih.gov/843571/</a>.
</div>
<div id="ref-lynottModalityExclusivityNorms2009" class="csl-entry" role="listitem">
Lynott, Dermot, and Louise Connell. 2009. <span>“Modality Exclusivity Norms for 423 Object Properties.”</span> <em>Behavior Research Methods</em> 41 (2): 558–64. <a href="https://doi.org/10.3758/BRM.41.2.558">https://doi.org/10.3758/BRM.41.2.558</a>.
</div>
<div id="ref-lynottLancasterSensorimotorNorms2020" class="csl-entry" role="listitem">
Lynott, Dermot, Louise Connell, Marc Brysbaert, James Brand, and James Carney. 2020. <span>“The <span>Lancaster Sensorimotor Norms</span>: Multidimensional Measures of Perceptual and Action Strength for 40,000 <span>English</span> Words.”</span> <em>Behavior Research Methods</em> 52 (3): 1271–91. <a href="https://doi.org/10.3758/s13428-019-01316-z">https://doi.org/10.3758/s13428-019-01316-z</a>.
</div>
<div id="ref-nalbantoglu_kadava25" class="csl-entry" role="listitem">
Nalbantoğlu, Hamza, and Šárka Kadavá. 2025. <span>“Multi-<span>Scenario Video</span> and <span>Audio Synchronization</span> and <span>Segmentation</span>.”</span> <a href="https://github.com/hamzanalbantoglu/flexible_audio_video_sync">https://github.com/hamzanalbantoglu/flexible_audio_video_sync</a>.
</div>
<div id="ref-pagnon_etal22" class="csl-entry" role="listitem">
Pagnon, David, Mathieu Domalain, and Lionel Reveret. 2022. <span>“<span>Pose2Sim</span>: <span class="nocase">An End-to-End Workflow</span> for <span>3D Markerless Sports Kinematics</span>—<span>Part</span> 2: <span>Accuracy</span>.”</span> <em>Sensors</em> 22 (7, 7): 2712. <a href="https://doi.org/10.3390/s22072712">https://doi.org/10.3390/s22072712</a>.
</div>
<div id="ref-pouw24" class="csl-entry" role="listitem">
Pouw, Wim. 2024. <span>“Wim <span>Pouw</span>’s <span>EnvisionBOX</span> Modules for Social Signal Processing.”</span> <a href="https://github.com/WimPouw/envisionBOX_modulesWP">https://github.com/WimPouw/envisionBOX_modulesWP</a>.
</div>
<div id="ref-pouw_etal21" class="csl-entry" role="listitem">
Pouw, Wim, Mark Dingemanse, Yasamin Motamedi, and Aslı Özyürek. 2021. <span>“A <span>Systematic Investigation</span> of <span>Gesture Kinematics</span> in <span>Evolving Manual Languages</span> in the <span>Lab</span>.”</span> <em>Cognitive Science</em> 45 (7): e13014. <a href="https://doi.org/10.1111/cogs.13014">https://doi.org/10.1111/cogs.13014</a>.
</div>
<div id="ref-seth_etal18" class="csl-entry" role="listitem">
Seth, Ajay, Jennifer L. Hicks, Thomas K. Uchida, Ayman Habib, Christopher L. Dembia, James J. Dunne, Carmichael F. Ong, et al. 2018. <span>“<span>OpenSim</span>: <span>Simulating</span> Musculoskeletal Dynamics and Neuromuscular Control to Study Human and Animal Movement.”</span> <em>PLOS Computational Biology</em> 14 (7): e1006223. <a href="https://doi.org/10.1371/journal.pcbi.1006223">https://doi.org/10.1371/journal.pcbi.1006223</a>.
</div>
<div id="ref-speer_etal18" class="csl-entry" role="listitem">
Speer, Robyn, Joshua Chin, and Catherine Havasi. 2018. <span>“<span>ConceptNet</span> 5.5: <span>An Open Multilingual Graph</span> of <span>General Knowledge</span>.”</span> 2018. <a href="https://doi.org/10.48550/arXiv.1612.03975">https://doi.org/10.48550/arXiv.1612.03975</a>.
</div>
<div id="ref-tadmorBorrowabilityNotionBasic2010" class="csl-entry" role="listitem">
Tadmor, Uri, Martin Haspelmath, and Bradley Taylor. 2010. <span>“Borrowability and the Notion of Basic Vocabulary.”</span> <em>Diachronica</em> 27 (2): 226–46. <a href="https://doi.org/10.1075/dia.27.2.04tad">https://doi.org/10.1075/dia.27.2.04tad</a>.
</div>
<div id="ref-rcoreteam16" class="csl-entry" role="listitem">
Team, R Core. 2016. <span>“R: <span>A</span> Language and Environment for Statistical Computing. <span>R Foundation</span> for <span>Statistical Computing</span>, <span>Vienna</span>, <span>Austria</span>.”</span> <a href="http://www.R-project.org/">http://www.R-project.org/</a>.
</div>
<div id="ref-tilsen_arvaniti13" class="csl-entry" role="listitem">
Tilsen, Sam, and Amalia Arvaniti. 2013. <span>“Speech Rhythm Analysis with Decomposition of the Amplitude Envelope: <span>Characterizing</span> Rhythmic Patterns Within and Across Languages.”</span> <em>The Journal of the Acoustical Society of America</em> 134 (1): 628–39. <a href="https://doi.org/10.1121/1.4807565">https://doi.org/10.1121/1.4807565</a>.
</div>
<div id="ref-trujilloSpeakersExhibitMultimodal2021" class="csl-entry" role="listitem">
Trujillo, James, Asli Özyürek, Judith Holler, and Linda Drijvers. 2021. <span>“Speakers Exhibit a Multimodal <span>Lombard</span> Effect in Noise.”</span> <em>Scientific Reports</em> 11 (1, 1): 16721. <a href="https://doi.org/10.1038/s41598-021-95791-0">https://doi.org/10.1038/s41598-021-95791-0</a>.
</div>
<div id="ref-trujillo_etal18" class="csl-entry" role="listitem">
Trujillo, James, Irina Simanova, Harold Bekkering, and Asli Özyürek. 2018. <span>“Communicative Intent Modulates Production and Comprehension of Actions and Gestures: <span>A Kinect</span> Study.”</span> <em>Cognition</em> 180: 38–51. <a href="https://doi.org/10.1016/j.cognition.2018.04.003">https://doi.org/10.1016/j.cognition.2018.04.003</a>.
</div>
<div id="ref-wittenburg_etal06" class="csl-entry" role="listitem">
Wittenburg, Peter, Hennie Brugman, Albert Russel, Alex Klassmann, and Han Sloetjes. 2006. <span>“<span>ELAN</span>: <span>A</span> Professional Framework for Multimodality Research.”</span> <em>Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006)</em>.
</div>
<div id="ref-xiongetal2002" class="csl-entry" role="listitem">
Xiong, Yingen, Francis Quek, and David Mcneill. 2002. <span>“Hand Gesture Symmetric Behavior Detection and Analysis in Natural Conversation.”</span> In, 179–84. <a href="https://doi.org/10.1109/ICMI.2002.1166989">https://doi.org/10.1109/ICMI.2002.1166989</a>.
</div>
<div id="ref-yan_etal20" class="csl-entry" role="listitem">
Yan, Yuke, James M. Goodman, Dalton D. Moore, Sara A. Solla, and Sliman J. Bensmaia. 2020. <span>“Unexpected Complexity of Everyday Manual Behaviors.”</span> <em>Nature Communications</em> 11 (1): 3564. <a href="https://doi.org/10.1038/s41467-020-17404-0">https://doi.org/10.1038/s41467-020-17404-0</a>.
</div>
<div id="ref-zywiczynski_etal24" class="csl-entry" role="listitem">
Żywiczyński, Przemysław, Marek Placiński, Marta Sibierska, Monika Boruta-Żywiczyńska, Sławomir Wacewicz, Michał Meina, and Peter Gärdenfors. 2024. <span>“Praxis, Demonstration and Pantomime: A Motion Capture Investigation of Differences in Action Performances.”</span> <em>Language and Cognition</em>, 1–28. <a href="https://doi.org/10.1017/langcog.2024.8">https://doi.org/10.1017/langcog.2024.8</a>.
</div>
</div>
</section>

</main> <!-- /main -->
document.addEventListener("DOMContentLoaded", function () {

  const links = document.querySelectorAll("a[href^='http']");

  links.forEach(function (link) {

    // Only apply to external links

    if (link.hostname !== window.location.hostname) {

      link.setAttribute("target", "_blank");

      link.setAttribute("rel", "noopener noreferrer");

    }

  });

});
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>