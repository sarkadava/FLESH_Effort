[
  {
    "objectID": "shign/examples/example.html",
    "href": "shign/examples/example.html",
    "title": "",
    "section": "",
    "text": "import os \nimport sys \nsys.path.insert(0, '../')\n\nimport librosa\nfrom matplotlib import pyplot as plt\n\nimport shign\n\n\n# audio_a, sr_a = librosa.load(\"example1_audioA.wav\", sr=None)\n# audio_b, sr_b = librosa.load(\"example1_audioB.wav\", sr=None)\naudio_a, sr_a = librosa.load(\"example2_audioA.wav\", sr=None)\naudio_b, sr_b = librosa.load(\"example2_audioB.wav\", sr=None)\n\nplt.plot(audio_a, label='a')\nplt.plot(audio_b, label='b')\nplt.legend()\nplt.show()\n\naudio_a, audio_b = shign.shift_align(\n    audio_a = audio_a, \n    audio_b = audio_b, \n    sr_a    = sr_a, \n    sr_b    = sr_b, \n    align_how = \"pad_and_crop_one_to_match_other\", \n#     align_how = \"crop_both\",\n    min_overlap_sec=1.\n)\n\nplt.plot(audio_a, label='a')\nplt.plot(audio_b, label='b')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "07_TS_featureExtraction/TS_extraction.html",
    "href": "07_TS_featureExtraction/TS_extraction.html",
    "title": "Extraction of effort-related features",
    "section": "",
    "text": "In this script, we will extract effort-related features from the merged multimodal data we created for each trial in merging script.\nBecause we are dealing with time-varying data, we will extract number of statistics that characterize both instantaneous as well as cumulative nature of each (acoustic, motion, postural) timeseries in terms of effort.\nBefore we collect all relevant features, we normalize all time-varying features in the trial by minimum and maximum observed values for that feature for a participant. This is because we want to account for individual differences/strategies in using the whole range of motion and/or acoustic features. We therefore treat effort as a relative measure - but because we cannot access information about the minimum/maximum possible by a participant, we take the maximum and minimum that were observed within the whole experiment.\n\n\nCode to prepare the environment\n# Import packages\nimport os\nimport glob\nimport pandas as pd\nfrom scipy.signal import find_peaks\nimport numpy as np\nimport antropy as ent\nfrom scipy.spatial import ConvexHull\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport umap\nimport seaborn as sns\nimport builtins\n\n\ncurfolder = os.getcwd()\n\n# Here we store our merged final timeseries data\nmergedfolder = curfolder + \"\\\\..\\\\05_finalMerge\\\\TS_final\\\\\"\nfilestotrack = glob.glob(mergedfolder + \"merged_anno*.csv\")\n\n# Here we store concept similarity data\nconceptsimfolder = curfolder + \"\\\\..\\\\06_ConceptSimilarity\\\\data\\\\\"\n\n# Here we store voice quality measures\nvoicefolder = curfolder + \"\\\\..\\\\03_TS_processing\\\\TS_acoustics\\\\\"\n\n# Here we store other potentially useful data\nmetadatafolder = curfolder + \"\\\\..\\\\00_RAWDATA\\\\\"\n\n# Here we store the final data\ndatafolder = curfolder + \"\\\\Datasets\\\\\"\n\n# These are practice trials\npracticetrials = ['0_2_0', '0_2_1', '0_2_19', '0_2_20', '0_2_21', '0_2_22', '0_2_38', '0_2_39', '0_2_40', '0_2_53', '0_2_54', '0_2_55', '0_2_56', '0_2_67', '0_2_68', '0_2_69', '0_2_70', '0_2_71', '0_2_72', '0_2_92', '0_2_93', '0_2_94', '0_2_95', '0_2_96', '0_2_97']\n\n# We are interested only in data from part 2, and only those that are not practice trials\nfiles_part2 = []  \n\n# Collect to list only files that are in part 2 and are not practice trials\nfor file in filestotrack:\n    exp_part = file.split(\"\\\\\")[-1].split(\"_\")[3]\n    if exp_part == \"2\":\n        if not any([x in file for x in practicetrials]):\n            files_part2.append(file)",
    "crumbs": [
      "Analysis",
      "Extraction of effort-related features"
    ]
  },
  {
    "objectID": "07_TS_featureExtraction/TS_extraction.html#bounding-box-of-movement-volume",
    "href": "07_TS_featureExtraction/TS_extraction.html#bounding-box-of-movement-volume",
    "title": "Extraction of effort-related features",
    "section": "Bounding box of movement volume",
    "text": "Bounding box of movement volume\nBounding box of movement volume (BBMV) is a measure of the space covered by the participant’s gestures (see Żywiczyński et al. (2024)). It is computed as the difference between the maximum and minimum values of the timeseries.\n\n\nCustom functions\n# Function to calcuate bounding box of movement volume     \ndef get_bbmv(df, group, kp_dict):\n    coordinates = [col for col in df.columns if any(x in col for x in ['_x', '_y', '_z'])]\n\n    # Prepare columns that belong to a group (e.g., arm)\n    kp = kp_dict[group]\n    colstoBBMV = [col for col in coordinates if any(x in col for x in kp)]\n\n    # Keep only unique names without coordinates\n    kincols = list(set([col.split('_')[0] for col in colstoBBMV]))\n\n    bbmvs = {}\n    for col in kincols:\n        # Span of x, y, z\n        x_span = df[col + '_x'].max() - df[col + '_x'].min()\n        y_span = df[col + '_y'].max() - df[col + '_y'].min()\n        z_span = df[col + '_z'].max() - df[col + '_z'].min()\n\n        # Calculate BBMV\n        bbmv = x_span * y_span * z_span\n        bbmvs[col] = bbmv\n        \n    # Get the sum for the whole group\n    bbmv_sum = sum(bbmvs.values())\n\n    # Natural logarithm\n    bbmv_sum = np.log(bbmv_sum) \n\n    return bbmv_sum\n\n\nUsing custom function compute_bounding_box, we will compute BBMV for the sample timeseries. (Note that the values are log-transformed.)\n\n\nBBMV of the current trial:  8.089072657442488",
    "crumbs": [
      "Analysis",
      "Extraction of effort-related features"
    ]
  },
  {
    "objectID": "07_TS_featureExtraction/TS_extraction.html#symmetry-of-arm-movement-multi-movement-asymmetry-of-submovements",
    "href": "07_TS_featureExtraction/TS_extraction.html#symmetry-of-arm-movement-multi-movement-asymmetry-of-submovements",
    "title": "Extraction of effort-related features",
    "section": "Symmetry of arm movement / multi-movement asymmetry of submovements",
    "text": "Symmetry of arm movement / multi-movement asymmetry of submovements\nSymmetry of arm movement is a measure of how much the participant uses both arms. Symmetrical movement is expected to be less effortful than asymmetrical movement, as asymmetrical movement might require more cognitive resources to coordinate the two arms. We will compute symmetry as the correlation between left and right arm trajectories, similar to Xiong, Quek, and Mcneill (2002). If it’s close to 1, it means that the participant uses both arms in a symmetrical way.\n\n\ndef compute_ArmCoupling(df, keypoints=['Wrist'], dimensions=['x', 'y', 'z'], absolute=True, z=False):\n    correlations = []\n    \n    for kp in keypoints:\n        for dim in dimensions:\n            if z:\n                left_col = f\"L{kp}_{dim}_z\"\n                right_col = f\"R{kp}_{dim}_z\"\n            else:\n                left_col = f\"L{kp}_{dim}\"\n                right_col = f\"R{kp}_{dim}\"\n                \n            if left_col in df.columns and right_col in df.columns:\n                corr = np.corrcoef(df[left_col], df[right_col])[0, 1]  # Pearson correlation\n                if absolute:\n                    corr = np.abs(corr)\n                correlations.append(corr)\n    \n    return np.nanmean(correlations)  # Average correlation across keypoints and dimensions\n\ndef compute_multimovementAssymetry(df, z=False):\n    joints = ['elbow_flex', 'wrist_flex', 'wrist_dev']\n    \n    integral_diff_left = 0\n    integral_diff_right = 0\n    joint_integral_differences = []\n\n    for joint in joints:\n        if z:\n            joint_r_speed = f\"{joint}_r_speed_z\"\n            joint_l_speed = f\"{joint}_l_speed_z\"\n        else:\n            joint_r_speed = f\"{joint}_r_speed\"\n            joint_l_speed = f\"{joint}_l_speed\"\n            \n        if joint_r_speed in df.columns and joint_l_speed in df.columns:\n            # Compute the integral of velocity (sum of speed over time) for left and right joints\n            integral_velocity_left = np.sum(np.abs(df[joint_l_speed]))  # Sum of absolute speed\n            integral_velocity_right = np.sum(np.abs(df[joint_r_speed]))  # Sum of absolute speed\n\n            # Compute the difference in integral of angular velocity between left and right \n            # the more +, the more distance is left is travelling), \n            # but note that because od z-scoring, 0 does not reflect perfect symmetry in distance travelled by both hands\n            joint_integral_diff = integral_velocity_left - integral_velocity_right\n            joint_integral_differences.append(joint_integral_diff)\n\n            # Accumulate for total integral difference calculation\n            integral_diff_left += integral_velocity_left\n            integral_diff_right += integral_velocity_right\n\n    # Compute the overall difference in integrated velocities\n    total_integral_difference = np.mean(joint_integral_differences)  # Mean of differences across joints\n\n    return {\n        'mean_integral_difference': total_integral_difference,\n        'joint_integral_differences': joint_integral_differences,\n        'total_left_integral_velocity': integral_diff_left,\n        'total_right_integral_velocity': integral_diff_right\n    }\n\n# Example usage\ncoupling_score = compute_ArmCoupling(df_sample, keypoints=['Wrist', 'Elbow'], dimensions=['x', 'y', 'z'], z=True)\narm_asymmetry = compute_multimovementAssymetry(df_sample, z=True)\n\n# Plot the trajectories\nplt.figure(figsize=(10, 8))\nax = plt.axes(projection='3d')\n\n# Plot the left arm\nax.plot3D(df_sample['LWrist_x'], df_sample['LWrist_y'], df_sample['LWrist_z'], 'r')\nax.plot3D(df_sample['LElbow_x'], df_sample['LElbow_y'], df_sample['LElbow_z'], 'r', linestyle='--')\n\n# Plot the right arm\nax.plot3D(df_sample['RWrist_x'], df_sample['RWrist_y'], df_sample['RWrist_z'], 'b')\nax.plot3D(df_sample['RElbow_x'], df_sample['RElbow_y'], df_sample['RElbow_z'], 'b', linestyle='--')\n\nplt.show()\n\n# Print the symmetry score\nprint(\"Coupling Score:\", coupling_score)\nprint(\"Arm Asymmetry:\", arm_asymmetry['mean_integral_difference'])\n\n\n\n\n\n\n\n\nCoupling Score: 0.42867744643596634\nArm Asymmetry: -7651.093615100171",
    "crumbs": [
      "Analysis",
      "Extraction of effort-related features"
    ]
  },
  {
    "objectID": "07_TS_featureExtraction/TS_extraction.html#intermittency",
    "href": "07_TS_featureExtraction/TS_extraction.html#intermittency",
    "title": "Extraction of effort-related features",
    "section": "Intermittency",
    "text": "Intermittency\nIntermittency characterizes the ‘unsmoothess’ of the movement. We follow Pouw et al. (2021) and computer dimensionless jerk measure (also see Hogan and Sternad (2009)). This measure is computed as the integral of the second derivative of the speed (i.e., jerk) squared multiplied by the duration cubed over the maximum squared velocity.\n\n\nCustom functions\n# Function to calculate intermittency \ndef get_intermittency(jerk_values, speed_values):\n    \"\"\"Calculate the dimensionless smoothness measure using precomputed smoothed jerk and speed.\"\"\"\n    smoothed_jerk = jerk_values\n    speed = speed_values\n    \n    if not np.all(speed == 0):\n        integrated_squared_jerk = np.sum(smoothed_jerk ** 2)\n        max_squared_speed = np.max(speed ** 2)\n        D3 = len(speed) ** 3\n        jerk_dimensionless = integrated_squared_jerk * (D3 / max_squared_speed)\n        smoothness = jerk_dimensionless\n    else:\n        smoothness = np.nan\n\n    return smoothness\n\n\nWe can calculate intermittency for kinematics as well as for the joint angles.\n\n\nIntermittency, kinematics:  29.512725544086184\nIntermittency, joint angles:  31.333161498897482",
    "crumbs": [
      "Analysis",
      "Extraction of effort-related features"
    ]
  },
  {
    "objectID": "07_TS_featureExtraction/TS_extraction.html#vowel-space-area",
    "href": "07_TS_featureExtraction/TS_extraction.html#vowel-space-area",
    "title": "Extraction of effort-related features",
    "section": "Vowel space area",
    "text": "Vowel space area\nVowel space area (VSA) is defined as the are of the quadrilateral formed by the four corner vowels /i/, /a/, /u/ and /ɑ/ in the F1-F2 space. It is a measure of the articulatory ‘working’ space and researchers often use it to characterize speech motor control (e.g., Berisha et al. (2014)).\nDespite the fact that we are not dealing with speech data, we can still compute VSA for the first two formants. Following Daniel R. McCloy’s tutorial for R, we will compute VSA for the sample timeseries as the area of the convex hull encompassing all tokens.\n\n# Function to calculate vocal space area (as a convex hull=\ndef getVSA(f1, f2, plot=False):\n   \n    # 2d convex hull\n    points = np.array([f1, f2]).T\n    hull_2d = ConvexHull(points)\n    volume_2d = hull_2d.volume\n    volume_2d = np.log(volume_2d) # natural log\n\n    if plot:\n        plt.figure()\n        plt.plot(points[:, 0], points[:, 1], 'o', markersize=3, label='Formant Points')\n        \n        for simplex in hull_2d.simplices:\n            plt.plot(points[simplex, 0], points[simplex, 1], 'r-', linewidth=1)\n        \n        plt.xlabel('F1 (Hz)')\n        plt.ylabel('F2 (Hz)')\n        plt.gca().invert_yaxis()\n        plt.legend()\n        plt.title('2D Convex Hull of Vowel Space Area')\n        plt.show()\n    \n    return volume_2d\n\n# Calculate on sample\nf1_clean = df_sample['f1_clean_z'].dropna()\nf2_clean = df_sample['f2_clean_z'].dropna()\nvsa = getVSA(f1_clean, f2_clean, plot=True)",
    "crumbs": [
      "Analysis",
      "Extraction of effort-related features"
    ]
  },
  {
    "objectID": "07_TS_featureExtraction/TS_extraction.html#motor-complexity",
    "href": "07_TS_featureExtraction/TS_extraction.html#motor-complexity",
    "title": "Extraction of effort-related features",
    "section": "Motor complexity",
    "text": "Motor complexity\nAs a part of exploratory analysis, we also want to assess the motor complexity of the movements as a proxy of coordinative - therefore cognitive - effort relating to the signal.\nUsing OpenSim (Seth et al. (26. 7. 2018)), we have been able to extract joint angle measurements for our data. We can use them to assess a motor complexity or alternatively, motor performance of a participant’s movements. We will use Principal Component Analysis (PCA) to assess the motor complexity. We will look at the number of principal components that explain a certain amount of variance in the data and the slope of the explained variance. This will give us a measure of how many (uncorrelated) dimensions/modes cover the most (80 or 95%) of the features of a signal, therefore how complex the movement is in terms of its coordination patterns (see for instance Daffertshofer et al. (2004), Yan et al. (2020)).\n\n# Function to get the PCA\ndef get_PCA(df, plot=False):\n    # Step 1: Standardize the Data\n    scaler = StandardScaler()\n    standardized_data = scaler.fit_transform(df)\n    \n    # Step 2: Apply PCA\n    pca = PCA()\n    pca.fit(standardized_data)\n    \n    # Step 3: Explained Variance\n    explained_variance = pca.explained_variance_ratio_\n    cumulative_explained_variance = explained_variance.cumsum()\n\n    # Step 4: Find the number of components that explain 95% variance\n    n_components_for_80_variance = np.argmax(cumulative_explained_variance &gt;= 0.8) + 1\n    \n    # Step 5: Compute the slope for the first n_components_for_95_variance\n    if n_components_for_80_variance &gt; 1:\n        slope_80 = (cumulative_explained_variance[n_components_for_80_variance-1] - cumulative_explained_variance[0]) / (n_components_for_80_variance - 1)\n    else:\n        slope_80 = cumulative_explained_variance[0]  # Only one component case\n\n    # 95% variance\n    n_components_for_95_variance = np.argmax(cumulative_explained_variance &gt;= 0.95) + 1\n    \n    # Step 5: Compute the slope for the first n_components_for_95_variance\n    if n_components_for_95_variance &gt; 1:\n        slope_95 = (cumulative_explained_variance[n_components_for_95_variance-1] - cumulative_explained_variance[0]) / (n_components_for_95_variance - 1)\n    else:\n        slope_95 = cumulative_explained_variance[0]  # Only one component case\n\n\n    # Set Seaborn style for consistency\n    sns.set_style(\"whitegrid\")  \n\n    main_color = \"black\"   # Black for main cumulative variance line\n    red_color = \"red\"      # Basic red for 80% variance threshold\n    blue_color = \"blue\"    # Basic blue for 95% variance threshold\n\n    if plot:\n\n        # get rid of the grid\n        plt.grid(False)\n\n        # gwt rid of the lines in the backroug\n        plt.gca().spines['top'].set_visible(False)\n        \n        # Adjust x-axis indexing: Components start from 1, not 0\n        x_values = range(1, len(cumulative_explained_variance) + 1)\n        \n        # Plot cumulative explained variance (black line with markers)\n        plt.plot(x_values, cumulative_explained_variance, marker='o', linestyle='-', markersize=4, linewidth=2, color=main_color, alpha=0.9, label='Cumulative Explained Variance')\n        \n        # Add vertical dashed lines for 80% and 95% variance thresholds\n        plt.axvline(x=n_components_for_80_variance, color=red_color, linestyle='--', \n                    linewidth=2, alpha=0.9)\n        \n        plt.axvline(x=n_components_for_95_variance, color=blue_color, linestyle='--', \n                    linewidth=2, alpha=0.9)\n\n        # Labels for variance thresholds on the graph\n        plt.text(n_components_for_80_variance + 0.3, 0.8, \"80% Variance\", \n                color=red_color, fontsize=12, fontweight='bold')\n        \n        plt.text(n_components_for_95_variance + 0.3, 0.95, \"95% Variance\", \n                color=blue_color, fontsize=12, fontweight='bold')\n\n        # Labels and title\n        plt.xlabel('Number of Principal Components', fontsize=16, color='black')\n        plt.ylabel('Cumulative Explained Variance', fontsize=16, color='black')\n        \n        # Ensure x-axis starts at 1 and labels are correctly spaced\n        plt.xticks(x_values)  # Explicitly setting x-ticks\n        \n        # Subtle grid\n        plt.grid(True, alpha=0.3)  \n        \n        # Make figure smaller\n        plt.gcf().set_size_inches(6, 4)\n\n        # Tight layout and saving for consistency\n        plt.tight_layout()\n\n        #xlim\n        plt.xlim(0, len(cumulative_explained_variance) +1)  # Adjust x-axis limits\n    \n        plt.show()\n        #plt.close()\n\n    return n_components_for_80_variance, slope_80, n_components_for_95_variance, slope_95\n\n# Calculate on sample\nPCAcolls_all = ['pelvis_tilt_z', 'pelvis_list_z', 'pelvis_rotation_z', 'pelvis_tx_z',\n       'pelvis_ty_z', 'pelvis_tz_z', 'hip_flexion_r_z', 'hip_adduction_r_z',\n       'hip_rotation_r_z', 'knee_angle_r_z', 'knee_angle_r_beta_z', 'ankle_angle_r_z',\n       'subtalar_angle_r_z', 'hip_flexion_l_z', 'hip_adduction_l_z',\n       'hip_rotation_l_z', 'knee_angle_l_z', 'knee_angle_l_beta_z', 'ankle_angle_l_z',\n       'subtalar_angle_l_z', 'L5_S1_Flex_Ext_z', 'L5_S1_Lat_Bending_z',\n       'L5_S1_axial_rotation_z', 'L4_L5_Flex_Ext_z', 'L4_L5_Lat_Bending_z',\n       'L4_L5_axial_rotation_z', 'L3_L4_Flex_Ext_z', 'L3_L4_Lat_Bending_z',\n       'L3_L4_axial_rotation_z', 'L2_L3_Flex_Ext_z', 'L2_L3_Lat_Bending_z',\n       'L2_L3_axial_rotation_z', 'L1_L2_Flex_Ext_z', 'L1_L2_Lat_Bending_z',\n       'L1_L2_axial_rotation_z', 'L1_T12_Flex_Ext_z', 'L1_T12_Lat_Bending_z',\n       'L1_T12_axial_rotation_z', 'neck_flexion_z', 'neck_bending_z',\n       'neck_rotation_z', 'arm_flex_r_z', 'arm_add_r_z', 'arm_rot_r_z', 'elbow_flex_r_z',\n       'pro_sup_r_z', 'wrist_flex_r_z', 'wrist_dev_r_z', 'arm_flex_l_z', 'arm_add_l_z',\n       'arm_rot_l_z', 'elbow_flex_l_z', 'pro_sup_l_z', 'wrist_flex_l_z',\n       'wrist_dev_l_z']\n\nPCAcolls_arm = ['arm_flex_r_z', 'arm_add_r_z', 'arm_rot_r_z', 'elbow_flex_r_z', 'pro_sup_r_z',\n       'wrist_flex_r_z', 'wrist_dev_r_z', 'arm_flex_l_z', 'arm_add_l_z', 'arm_rot_l_z',\n       'elbow_flex_l_z', 'pro_sup_l_z', 'wrist_flex_l_z', 'wrist_dev_l_z']\n\n# Calculate PCA for all\nsubdf = df_sample\nPCA_all = get_PCA(subdf[PCAcolls_all], plot=True)\nprint('PCA for whole-body movement complexity', PCA_all)\n\n# Calculate PCA for arms\nsubdf = df_sample\nPCA_arms = get_PCA(subdf[PCAcolls_arm], plot=True)\nprint('PCA for arm movement complexity', PCA_arms)\n\n\n\n\n\n\n\n\nPCA for whole-body movement complexity (3, 0.18143457258058265, 6, 0.1009103456272205)\nPCA for arm movement complexity (3, 0.11732507841208828, 5, 0.09053436514831179)",
    "crumbs": [
      "Analysis",
      "Extraction of effort-related features"
    ]
  },
  {
    "objectID": "05_finalMerge/TS_mergeAnnotations.html",
    "href": "05_finalMerge/TS_mergeAnnotations.html",
    "title": "Final merge",
    "section": "",
    "text": "Overview\nIn this notebook, we will merge all the data we have been preparing so far, i.e., timeseries data for acoustics and motion (see merging script), and annotations we have been working with in the previous notebook.\nWe will also add annotations of sounding/silence created in Praat (Boersma and Weenink (2025)).\n\n\nCode to prepare the environment\nimport os\nimport glob\nimport xml.etree.ElementTree as ET\nimport pandas as pd\n\ncurfolder = os.getcwd()\n\n# Here we store the merged timeseries data\nmergedfolder = curfolder + '\\\\..\\\\03_TS_processing\\\\TS_merged\\\\'\nmergedfiles = glob.glob(mergedfolder + '/merged*.csv')\nmergedfiles = [x for x in mergedfiles if 'anno' not in x]\n\n# Here we store the predicted motion annotations\nannofolder = curfolder + '\\\\..\\\\04_TS_movementAnnotation\\\\TS_annotated_logreg\\\\'\nannofolders = glob.glob(annofolder + '*0_6\\\\')\n\n# Here we store the annotations of vocalizations (from AC)\nvocannofolder = curfolder + '\\\\..\\\\04_TS_movementAnnotation\\\\ManualAnno\\\\R1\\\\'\nvocfiles = glob.glob(vocannofolder + '\\\\*ELAN_tiers.eaf')\n\n# Create folder for the txt annotations\nif not os.path.exists(curfolder + '\\\\Annotations_txt'):\n    os.makedirs(curfolder + '\\\\Annotations_txt')\n\ntxtannofolder = curfolder + '\\\\Annotations_txt\\\\'\n\n\n\n\nGetting vocalization annotations from ELAN file\nWe have used Praat to annotate the sounding/silence in the trials and loaded it to ELAN file with the remaining annotations of movement to save all in a single file. Now we want to get the annotations of sounding/silence from the ELAN file and merge it with the rest of the data.\n(Note that we are working on automatic annotator of speech that would allow to get the annotations of sounding/silence without the need for external software, similar to our movement annotation pipeline.)\n\n\nCustom functions\n# Function to parse ELAN annotation\ndef parse_eaf_file(eaf_file, rel_tiers):\n    tree = ET.parse(eaf_file)\n    root = tree.getroot()\n\n    time_order = root.find('TIME_ORDER')\n    time_slots = {time_slot.attrib['TIME_SLOT_ID']: time_slot.attrib['TIME_VALUE'] for time_slot in time_order}\n\n    annotations = []\n    relevant_tiers = {rel_tiers}\n    for tier in root.findall('TIER'):\n        tier_id = tier.attrib['TIER_ID']\n        if tier_id in relevant_tiers:\n            for annotation in tier.findall('ANNOTATION/ALIGNABLE_ANNOTATION'):\n                # Ensure required attributes are present\n                if 'TIME_SLOT_REF1' in annotation.attrib and 'TIME_SLOT_REF2' in annotation.attrib:\n                    ts_ref1 = annotation.attrib['TIME_SLOT_REF1']\n                    ts_ref2 = annotation.attrib['TIME_SLOT_REF2']\n                    # Get annotation ID if it exists, otherwise set to None\n                    ann_id = annotation.attrib.get('ANNOTATION_ID', None)\n                    annotation_value = annotation.find('ANNOTATION_VALUE').text.strip()\n                    annotations.append({\n                        'tier_id': tier_id,\n                        'annotation_id': ann_id,\n                        'start_time': time_slots[ts_ref1],\n                        'end_time': time_slots[ts_ref2],\n                        'annotation_value': annotation_value\n                    })\n\n    return annotations\n\n\n\n# Here we store the vocalization annotations\nvocal_anno = txtannofolder + '\\\\vocalization_annotations.txt'\n\nwith open(vocal_anno, 'w') as f:\n    for file in vocfiles:\n        print('working on ' + file)\n        # Filename\n        filename = file.split('\\\\')[-1]\n        filename = filename.replace('_ELAN_tiers.eaf', '')\n        # Parse the ELAN file\n        annotations = parse_eaf_file(file, 'vocalization')\n        # Save it to the file\n        for annotation in annotations:\n            f.write(f\"{annotation['start_time']}\\t{annotation['end_time']}\\t{annotation['annotation_value']}\\t{filename}\\n\")\n\n\n\nPreparing movement annotations\nSimilarly, we also want to get ready our movement annotations to simple txt file. We store all the predicted annotations separately per tier and per trial, so now we merge all the files into a single txt file, per each tier separately.\nAs already mentioned in previous script, we need to handle two issues that stem from the the fact that the classifier can create flickering annotations, as the confidence values continuously vary throughout each trial.\nSimilarly to Pouw et al. (2021), we apply two rules to handle this flickering: - Rule 1: If there is a nomovement event between two movement events that is shorter than 200 ms, this is considered as part of the movement event. - Rule 2: If there is a movement event between two nomovement events that is shorter than 200 ms, this is considered as part of the nomovement event.\nAfterwards, we take the first movement event and the very last movement event, and consider everything in between as a movement.\nThen we write the final movement annotations to a txt file.\n\n\nCustom functions\n# Function to get chunks of annotations\ndef get_chunks(anno_df):\n    anno_df['chunk'] = (anno_df['anno_values'] != anno_df['anno_values'].shift()).cumsum()\n    anno_df['idx'] = anno_df.index\n\n    # Calculate start and end of each chunk, grouped by anno_values, save also the first and last index\n    chunks = anno_df.groupby(['anno_values', 'chunk']).agg(\n        time_ms_min=('time_ms', 'first'),\n        time_ms_max=('time_ms', 'last'),\n        idx_min=('idx', 'first'),\n        idx_max=('idx', 'last')\n    ).reset_index()\n\n    # Order the chunks\n    chunks = chunks.sort_values('idx_min').reset_index(drop=True)\n\n    return chunks\n\n\n\nfor folder in annofolders:\n    # get tierID\n    tier = folder.split('\\\\')[-2].split('_')[0]\n\n    if tier == 'head':\n        tier = 'head'\n    elif tier == 'upperBody':\n        tier = 'upper'\n    elif tier == 'lowerBody':\n        tier = 'lower'\n\n    # This is the file we want to create\n    txtfile = txtannofolder + 'movement_' + tier + '.txt'\n\n    # List all files in the folder\n    files = glob.glob(folder + '*.csv')\n\n    for file in files:\n        print('processing: ' + file)\n\n        # Filename\n        filename = file.split('\\\\')[-1].split('.')[0]\n        filename = filename.split('_')[2:6]\n        filename = '_'.join(filename)\n\n        # Now we process the annotations made by the logreg model\n        anno_df = pd.read_csv(file)\n\n        # Chunk the df to see unique annotated chunks\n        chunks = get_chunks(anno_df)\n\n        # Check for fake pauses (i.e., nomovement annotation that last for less than 200ms)\n        for i in range(1, len(chunks)-1):\n            if chunks.loc[i, 'anno_values'] == 'no movement' and chunks.loc[i-1, 'anno_values'] == 'movement' and chunks.loc[i+1, 'anno_values'] == 'movement':\n                if chunks.loc[i, 'time_ms_max'] - chunks.loc[i, 'time_ms_min'] &lt; 200:\n                    print('found a chunk of no movement between two movement chunks that is shorter than 200 ms')\n                    # Change the chunk into movement\n                    anno_df.loc[chunks.loc[i, 'idx_min']:chunks.loc[i, 'idx_max'], 'anno_values'] = 'movement'\n\n        # Calculate new chunks\n        chunks = get_chunks(anno_df)\n\n        # Now check for fake movement (i.e., movement chunk that is shorter than 200ms)\n        for i in range(1, len(chunks)-1):\n            if chunks.loc[i, 'anno_values'] == 'movement' and chunks.loc[i-1, 'anno_values'] == 'no movement' and chunks.loc[i+1, 'anno_values'] == 'no movement':\n                if chunks.loc[i, 'time_ms_max'] - chunks.loc[i, 'time_ms_min'] &lt; 200:\n                    print('found a chunk of movement between two no movement chunks that is shorter than 250 ms')\n                    # change the chunk to no movement in the original df\n                    anno_df.loc[chunks.loc[i, 'idx_min']:chunks.loc[i, 'idx_max'], 'anno_values'] = 'no movement'\n\n        \n        # Now, similarly to our human annotators, we consider movement anything from the very first movement to the very last movement\n        if 'movement' in anno_df['anno_values'].unique():\n            # Get the first and last index of movement\n            first_idx = anno_df[anno_df['anno_values'] == 'movement'].index[0]\n            last_idx = anno_df[anno_df['anno_values'] == 'movement'].index[-1]\n            # Change all between to movement\n            anno_df.loc[first_idx:last_idx, 'anno_values'] = 'movement'\n\n        # Calculate new chunks\n        chunks = get_chunks(anno_df)\n\n        # Rewrite \"no movement\" in anno_values to \"nomovement\" (to match the manual annotations)\n        chunks['anno_values'] = chunks['anno_values'].apply(\n            lambda x: 'nomovement' if x == 'no movement' else x\n        )\n\n        # TrialID\n        chunks['TrialID']  = str(filename)\n\n        # Write to the text file\n        with open(txtfile, 'a') as f:\n            for _, row in chunks.iterrows():\n                f.write(\n                    f\"{row['time_ms_min']}\\t{row['time_ms_max']}\\t{row['anno_values']}\\t{row['TrialID']}\\n\")\n                \n\n\n\nFinal merge\nNow we take the merged timeseries with acoustic and movement data, and add columns for vocalization annotations and movement annotations. We will also add tier for general movement, concatenating the movement annotations from all tiers to see when a movement (of any articulator) starts and when it ends.\nFinally, we save the merged data to a single csv file per each trial\n\n\nCustom functions\n# Function to load annotations from txt file to timeseries\ndef anno_to_df(df, anno, anno_col):\n    for row in anno.iterrows():\n        start = row[1][0]\n        end = row[1][1]\n        value = str(row[1][2])\n        df.loc[(df['time'] &gt;= start) & (df['time'] &lt;= end), anno_col] = value\n\n\n\n# Here we will store the merged timeseries with annotations\nTSfinal = curfolder + '\\\\TS_final\\\\'\n\n# Here we store the annotations of vocalizations (from AC)\nvoc_anno = txtannofolder + '\\\\vocalization_annotations.txt'\n# Here we store the annotations of the movement\nhead_anno = txtannofolder + '\\\\movement_head.txt'\nupper_anno = txtannofolder + '\\\\movement_upper.txt'\nlower_anno = txtannofolder + '\\\\movement_lower.txt'\narms_anno = txtannofolder + '\\\\movement_arms.txt'\n\n# Load the annotatins\nvoc_df = pd.read_csv(voc_anno, sep='\\t', header=None)\nhead_df = pd.read_csv(head_anno, sep='\\t', header=None)\nupper_df = pd.read_csv(upper_anno, sep='\\t', header=None)\nlower_df = pd.read_csv(lower_anno, sep='\\t', header=None)\narms_df = pd.read_csv(arms_anno, sep='\\t', header=None)\n\nfor file in mergedfiles:\n    print('working on ' + file)\n\n    # TrialID\n    trialid = file.split('\\\\')[-1].split('.')[0]\n    trialid = trialid.replace('merged_', '')\n    \n    # Load the file\n    merged = pd.read_csv(file)\n\n    # Get the annotations for this trialID\n    voc_anno_trial = voc_df[voc_df[3] == trialid]\n    #print(voc_anno_trial)\n    head_anno_trial = head_df[head_df[3] == trialid]\n    upper_anno_trial = upper_df[upper_df[3] == trialid]\n    lower_anno_trial = lower_df[lower_df[3] == trialid]\n    arms_anno_trial = arms_df[arms_df[3] == trialid]\n\n    # Prepare error log\n    error_log = []\n\n    # If any of the annotations is empty, we skip this trial and save a message - these should be practice trials in our case\n    if any([voc_anno_trial.empty, head_anno_trial.empty, upper_anno_trial.empty, lower_anno_trial.empty, arms_anno_trial.empty]):\n        print('no annotations for ' + trialid)\n        error_log.append('no annotations for ' + trialid)\n        continue\n\n    else:\n        merged['vocalization'] = ''\n        anno_to_df(merged, voc_anno_trial, 'vocalization')\n        merged['head_mov'] = ''\n        anno_to_df(merged, head_anno_trial, 'head_mov')\n        merged['upper_mov'] = ''\n        anno_to_df(merged, upper_anno_trial, 'upper_mov')\n        merged['lower_mov'] = ''\n        anno_to_df(merged, lower_anno_trial, 'lower_mov')\n        merged['arms_mov'] = ''\n        anno_to_df(merged, arms_anno_trial, 'arms_mov')\n\n    # Also create a column 'movement_in_trial' that combines all movement annotations\n    merged['movement_in_trial'] = None\n    # Loop through rows and if any of the movement columns is 'movement', then fill the movement_in_trial column with 'movement'\n    try:\n        first_movement = merged[(merged['head_mov'] == 'movement') | (merged['upper_mov'] == 'movement') | (merged['lower_mov'] == 'movement') | (merged['arms_mov'] == 'movement')].index[0]\n        last_movement = merged[(merged['head_mov'] == 'movement') | (merged['upper_mov'] == 'movement') | (merged['lower_mov'] == 'movement') | (merged['arms_mov'] == 'movement')].index[-1]\n    except IndexError:\n        print('no movement annotations for ' + trialid)\n        # fill the movement_in_trial column with 'nomovement'\n        merged['movement_in_trial'] = 'nomovement'\n        \n    # Fill the movement_in_trial column\n    merged.loc[first_movement:last_movement, 'movement_in_trial'] = 'movement'\n    # Fill the rest with 'nomovement'\n    merged['movement_in_trial'] = merged['movement_in_trial'].fillna('nomovement')\n\n    # Save the merged file\n    merged.to_csv(TSfinal + '/merged_anno_' + trialid + '.csv', index=False)\n\n    # Save the error log\n    with open(TSfinal + '\\\\error_log.txt', 'a') as f:\n        for line in error_log:\n            f.write(line + '\\n')\n\n\nThis is how our final multimodal dataset with annotation looks like for a single trial.\n\n\n\n\n\n\n\n\n\n\ntime\nleft_back\nright_forward\nright_back\nleft_forward\nCOPXc\nCOPYc\nCOPc\nTrialID\nFileInfo\n...\nlowerbody_power\nleg_power\nhead_power\narm_power\nvocalization\nhead_mov\nupper_mov\nlower_mov\narms_mov\nmovement_in_trial\n\n\n\n\n0\n0.0\n1.086809\n0.830746\n1.491993\n1.384194\n0.000019\n-0.000184\n0.000185\n0_2_103_p1\np1_zout_geluiden_c1\n...\n23.657219\n4.718786\n2.372685\n19.838907\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n1\n2.0\n1.087116\n0.830946\n1.492349\n1.384381\n0.000043\n-0.000173\n0.000178\n0_2_103_p1\np1_zout_geluiden_c1\n...\n23.701134\n4.724236\n2.376751\n19.895821\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n2\n4.0\n1.087440\n0.831186\n1.492731\n1.384605\n0.000065\n-0.000165\n0.000178\n0_2_103_p1\np1_zout_geluiden_c1\n...\n23.745049\n4.729686\n2.380816\n19.952735\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n3\n6.0\n1.087778\n0.831459\n1.493137\n1.384858\n0.000085\n-0.000160\n0.000181\n0_2_103_p1\np1_zout_geluiden_c1\n...\n23.788964\n4.735136\n2.384881\n20.009650\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n4\n8.0\n1.088125\n0.831761\n1.493562\n1.385137\n0.000102\n-0.000157\n0.000188\n0_2_103_p1\np1_zout_geluiden_c1\n...\n23.832878\n4.740586\n2.388947\n20.066564\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n5\n10.0\n1.088481\n0.832086\n1.494006\n1.385435\n0.000118\n-0.000156\n0.000196\n0_2_103_p1\np1_zout_geluiden_c1\n...\n23.876793\n4.746036\n2.393012\n20.123478\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n6\n12.0\n1.088841\n0.832430\n1.494464\n1.385748\n0.000132\n-0.000156\n0.000204\n0_2_103_p1\np1_zout_geluiden_c1\n...\n23.920708\n4.751486\n2.397077\n20.180392\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n7\n14.0\n1.089204\n0.832789\n1.494934\n1.386073\n0.000145\n-0.000156\n0.000213\n0_2_103_p1\np1_zout_geluiden_c1\n...\n23.964623\n4.756936\n2.401143\n20.237306\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n8\n16.0\n1.089568\n0.833159\n1.495415\n1.386406\n0.000157\n-0.000156\n0.000222\n0_2_103_p1\np1_zout_geluiden_c1\n...\n24.008538\n4.762386\n2.405208\n20.294220\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n9\n18.0\n1.089931\n0.833538\n1.495903\n1.386744\n0.000169\n-0.000156\n0.000230\n0_2_103_p1\np1_zout_geluiden_c1\n...\n24.052452\n4.767836\n2.409273\n20.351134\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n10\n20.0\n1.090291\n0.833922\n1.496398\n1.387084\n0.000179\n-0.000156\n0.000238\n0_2_103_p1\np1_zout_geluiden_c1\n...\n24.096367\n4.773286\n2.413339\n20.408049\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n11\n22.0\n1.090647\n0.834309\n1.496896\n1.387423\n0.000189\n-0.000155\n0.000245\n0_2_103_p1\np1_zout_geluiden_c1\n...\n24.140282\n4.778736\n2.417404\n20.464963\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n12\n24.0\n1.090998\n0.834698\n1.497396\n1.387761\n0.000199\n-0.000153\n0.000251\n0_2_103_p1\np1_zout_geluiden_c1\n...\n24.184197\n4.784186\n2.421469\n20.521877\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n13\n26.0\n1.091343\n0.835085\n1.497897\n1.388096\n0.000207\n-0.000150\n0.000256\n0_2_103_p1\np1_zout_geluiden_c1\n...\n24.228112\n4.789636\n2.425535\n20.578791\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n14\n28.0\n1.091680\n0.835471\n1.498397\n1.388425\n0.000216\n-0.000146\n0.000261\n0_2_103_p1\np1_zout_geluiden_c1\n...\n24.272027\n4.795086\n2.429600\n20.635705\nsilent\nnomovement\nnomovement\nnomovement\nnomovement\nnomovement\n\n\n\n\n15 rows × 534 columns\n\n\n\n\n\n\nReferences\n\n\n\n\nBoersma, Paul, and David Weenink. 2025. “Praat: Doing Phonetics by Computer.” http://www.praat.org/.\n\n\nPouw, Wim, Jan de Wit, Sara Bögels, Marlou Rasenberg, Branka Milivojevic, and Asli Ozyurek. 2021. “Semantically Related Gestures Move Alike: Towards a Distributional Semantics of Gesture Kinematics.” In Digital Human Modeling and Applications in Health, Safety, Ergonomics and Risk Management. Human Body, Motion and Behavior, edited by Vincent G. Duffy, 269–87. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-77817-0_20.",
    "crumbs": [
      "Time Series Annotation",
      "Final merge"
    ]
  },
  {
    "objectID": "04_TS_movementAnnotation/02_MovementClassifier.html",
    "href": "04_TS_movementAnnotation/02_MovementClassifier.html",
    "title": "Movement annotation II: Training movement classifier, and annotating timeseries data",
    "section": "",
    "text": "This notebook has been adapted from machine learning workshop by Esam Ghaleb (see Github repository), and adapted mostly by Hamza Nalbantoglu.\nIn this notebook, we train a movement classifier using logistic regression. We train four models for four tiers of movement - head, arms, upper body, and lower body. The training datasets have been prepared in the previous script.\nAfterwards, we use the models to annotate the original timeseries data that have been prepared in the previous script. The timeseries data have been adapted to the same format as the training data, and contain overlapping time windows of 25 ms. We use these overlaps to build a continuum consisting of the model’s confidence, and use a threshold of the confidence to determine whether the movement is present or not.\n\n\nCode to prepare the environment\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport joblib\nwarnings.filterwarnings(\"ignore\")\n\ncurfolder = os.getcwd()\n\n# Here we store training data\ntrainingfolder = curfolder + '\\\\TrainingData\\\\'\n# Here we store data for classifying\nclassifyingfolder = curfolder + '\\\\TS_forClassifying\\\\'\n# Here we store models\nmodelsfolder = curfolder + '\\\\Models\\\\'",
    "crumbs": [
      "Time Series Annotation",
      "Movement annotation II: Training movement classifier, and annotating timeseries data"
    ]
  },
  {
    "objectID": "04_TS_movementAnnotation/02_MovementClassifier.html#arm-model",
    "href": "04_TS_movementAnnotation/02_MovementClassifier.html#arm-model",
    "title": "Movement annotation II: Training movement classifier, and annotating timeseries data",
    "section": "Arm model",
    "text": "Arm model\n\n# Load arm data\ncsv_path = trainingfolder + 'dataset_arms_features.csv'\n\n# Create a MovementClassifier object\nmodel_arms = MovementClassifier(csv_path)\n\n# load in the model\nmodel_path = modelsfolder + 'model_arms.pkl'\nmodel_arms = joblib.load(model_path)\n\n# Load features\nfeature_indices = list(range(model_arms.df.shape[1] - 3))\n\n# Load the feature and label data\nX, y = model_arms.load_and_process_data(X_columns=feature_indices, y_column='anno_value')\n\n# Evaluate and plot using the selected features\nmodel_arms.evaluate_and_plot(\"All\", X, y)\n\n# Save the model\nmodel_path = modelsfolder + 'model_arms.pkl'\njoblib.dump(model_arms, model_path)\n\nThis is the evaluation of the model\n\nAll Features\nAccuracy on training set: 0.9760348583877996\nAccuracy on test set: 0.9575163398692811\nConfusion Matrix\n[[237  18]\n [  8 349]]\nClassification Report\n              precision    recall  f1-score   support\n\n    movement       0.97      0.93      0.95       255\n  nomovement       0.95      0.98      0.96       357\n\n    accuracy                           0.96       612\n   macro avg       0.96      0.95      0.96       612\nweighted avg       0.96      0.96      0.96       612",
    "crumbs": [
      "Time Series Annotation",
      "Movement annotation II: Training movement classifier, and annotating timeseries data"
    ]
  },
  {
    "objectID": "04_TS_movementAnnotation/02_MovementClassifier.html#head-model",
    "href": "04_TS_movementAnnotation/02_MovementClassifier.html#head-model",
    "title": "Movement annotation II: Training movement classifier, and annotating timeseries data",
    "section": "Head model",
    "text": "Head model\n\n# Load head data\ncsv_path = trainingfolder + 'dataset_head_mov_features.csv'\n\n# Create a MovementClassifier object\nmodel_head = MovementClassifier(csv_path)\n\n# load in the model\nmodel_path = modelsfolder + 'model_head_mov.pkl'\nmodel_head = joblib.load(model_path)\n\n# Load features\nfeature_indices = list(range(model_head.df.shape[1] - 3))\n\n# Load the feature and label data\nX, y = model_head.load_and_process_data(X_columns=feature_indices, y_column='anno_value')\n\n# Evaluate and plot using the selected features\nmodel_head.evaluate_and_plot(\"All\", X, y)\n\n# Save the model\nmodel_path = modelsfolder + 'model_head_mov.pkl'\njoblib.dump(model_head, model_path)\n\nThis is the evaluation of the model\n\nAll Features\nAccuracy on training set: 0.9368146214099217\nAccuracy on test set: 0.863849765258216\nConfusion Matrix\n[[243  29]\n [ 58 309]]\nClassification Report\n              precision    recall  f1-score   support\n\n    movement       0.81      0.89      0.85       272\n  nomovement       0.91      0.84      0.88       367\n\n    accuracy                           0.86       639\n   macro avg       0.86      0.87      0.86       639\nweighted avg       0.87      0.86      0.86       639",
    "crumbs": [
      "Time Series Annotation",
      "Movement annotation II: Training movement classifier, and annotating timeseries data"
    ]
  },
  {
    "objectID": "04_TS_movementAnnotation/02_MovementClassifier.html#upper-body-model",
    "href": "04_TS_movementAnnotation/02_MovementClassifier.html#upper-body-model",
    "title": "Movement annotation II: Training movement classifier, and annotating timeseries data",
    "section": "Upper body model",
    "text": "Upper body model\n\n# Load upper body data\ncsv_path = trainingfolder + 'dataset_upper_body_features.csv'\n\n# Create a MovementClassifier object\nmodel_upper = MovementClassifier(csv_path)\n\n# load in the model\nmodel_path = modelsfolder + 'model_upper_body.pkl'\nmodel_upper = joblib.load(model_path)\n\n# Load features\nfeature_indices = list(range(model_upper.df.shape[1] - 3))\n\n# Load the feature and label data\nX, y = model_upper.load_and_process_data(X_columns=feature_indices, y_column='anno_value')\n\n# Evaluate and plot using the selected features\nmodel_upper.evaluate_and_plot(\"All\", X, y)\n\n# Save the model\nmodel_path = modelsfolder + 'model_upper_body.pkl'\njoblib.dump(model_upper, model_path)\n\nThis is the evaluation of the model\n\nAll Features\nAccuracy on training set: 0.9372409709887507\nAccuracy on test set: 0.8632326820603907\nConfusion Matrix\n[[171  37]\n [ 40 315]]\nClassification Report\n              precision    recall  f1-score   support\n\n    movement       0.81      0.82      0.82       208\n  nomovement       0.89      0.89      0.89       355\n\n    accuracy                           0.86       563\n   macro avg       0.85      0.85      0.85       563\nweighted avg       0.86      0.86      0.86       563",
    "crumbs": [
      "Time Series Annotation",
      "Movement annotation II: Training movement classifier, and annotating timeseries data"
    ]
  },
  {
    "objectID": "04_TS_movementAnnotation/02_MovementClassifier.html#lower-body-model",
    "href": "04_TS_movementAnnotation/02_MovementClassifier.html#lower-body-model",
    "title": "Movement annotation II: Training movement classifier, and annotating timeseries data",
    "section": "Lower body model",
    "text": "Lower body model\n\n# Load lower body data\ncsv_path = trainingfolder + 'dataset_lower_body_features.csv'\n\n# Create a MovementClassifier object\nmodel_lower = MovementClassifier(csv_path)\n\n# load in the model\nmodel_path = modelsfolder + 'model_lower_body.pkl'\nmodel_lower = joblib.load(model_path)\n\n# Load features\nfeature_indices = list(range(model_lower.df.shape[1] - 3))\n\n# Load the feature and label data\nX, y = model_lower.load_and_process_data(X_columns=feature_indices, y_column='anno_value')\n\n# Evaluate and plot using the selected features\nmodel_lower.evaluate_and_plot(\"All\", X, y)\n\n# Save the model\nmodel_path = modelsfolder + 'model_lower_body.pkl'\njoblib.dump(model_lower, model_path)\n\nThis is the evaluation of the model\n\nAll Features\nAccuracy on training set: 0.9594921402660218\nAccuracy on test set: 0.8913043478260869\nConfusion Matrix\n[[165  33]\n [ 27 327]]\nClassification Report\n              precision    recall  f1-score   support\n\n    movement       0.86      0.83      0.85       198\n  nomovement       0.91      0.92      0.92       354\n\n    accuracy                           0.89       552\n   macro avg       0.88      0.88      0.88       552\nweighted avg       0.89      0.89      0.89       552",
    "crumbs": [
      "Time Series Annotation",
      "Movement annotation II: Training movement classifier, and annotating timeseries data"
    ]
  },
  {
    "objectID": "03_TS_processing/03_TS_merging.html",
    "href": "03_TS_processing/03_TS_merging.html",
    "title": "Processing III: Merging multimodal data",
    "section": "",
    "text": "Overview\nIn the previous scripts, we have preprocessed various motion and acoustic data. In this script, we will merge the data into a single file per trial. These data include:\n\nBalance Board data\nKinematics\nJoint angles\nJoint moments\nAmplitude envelope\nf0\nFormants\nSpectral centroid\n\n\n\nCode to prepare environment\n# packages\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncurfolder = os.getcwd()\n\n# folders with processed data\nMTfolder_processed = curfolder + '\\\\TS_motiontracking\\\\'\nACfolder_processed = curfolder + '\\\\TS_acoustics\\\\'\n# folder to save merged data\nTSmerged = curfolder + '\\\\TS_merged\\\\'\n\n# prepare all files\nbbfiles = glob.glob(MTfolder_processed + '/bb*.csv')\nidfiles = glob.glob(MTfolder_processed + '/id*.csv')\nikfiles = glob.glob(MTfolder_processed + '/ik*.csv')\nmtfiles = glob.glob(MTfolder_processed + '/mt*.csv')\nenvfiles = glob.glob(ACfolder_processed + '/env*.csv')\nf0files = glob.glob(ACfolder_processed + '/f0*.csv')\nformants = glob.glob(ACfolder_processed + '/praat_formants*.csv')\nscfiles = glob.glob(ACfolder_processed + '/cog*.csv')\n\n\nWhen extracting and processing the acoustic and motion signals, we kept the sampling rates untouched. This means that now we have a variety of timeseries that each samples at different frequency. Inspecting a trial per each signal, we see the following sampling rates:\n\n\nbalance board: 499.986924434759\ninverse dynamics: 60.00535247744097\ninverse kinematics: 59.999988000002396\nkinematics: 60.00000000000024\nenvelope: 48000.00000000008\nf0: 487.16350507461976\nformants: 38415.36614645814\nspectral centroid: 472.65436024146993\n\n\nWe opt for 500 Hz as the final sampling rate we will merge on. That means that we will interpolate all missing data (using linear interpolation) to match this frequency.\nAdditionally, we will adapt the formants such that we only consider values that are present within a range of an amplitude peak (see acoustics processing script for more details), or where f0 is present, or both. These two situations can be considered as yielding in the most reliable formant values.\nFinally, we will also use inverse kinematics and dynamics to calculate power (as joint moment × joint velocity) and smooth it with 1st-polynomial Savitzky-Golay filter with span of 560 ms.\n\n\nCustom functions\n# Function to create chunks of non-NaN values in a dataframe\ndef create_chunks(df, var):\n\n    df['chunk'] = None\n\n    # annotate chunks of non-NaN values\n    chunk = 0\n    for index, row in df.iterrows():\n        if np.isnan(row[var]):\n            continue\n        else:\n            df.loc[index, 'chunk'] = chunk\n            # if the next value is NaN or this is the last row, increase the chunk\n            if index == len(df)-1:\n                continue\n            elif np.isnan(df.loc[index+1, var]):\n                chunk += 1\n\n    chunks = df['chunk'].unique()\n\n    if len(chunks) &gt; 1: # skip if chunks are empty (that means that there is no f0 trace)\n        # ignore the first chunk (None)\n        chunks = chunks[1:]\n\n    return df, chunks\n\n# Function to interpolate chunks of non-NaN values in a dataframe to maintain discontinuities in the signal\ndef interpolate_chunks(df, chunks, var):\n    # we ignore the None chunk above, so if there is some trace, None should not be within chunks\n    if None not in chunks:\n        for chunk in chunks:\n            # get the first and last row of the chunk\n            firstrow = df[df['chunk'] == chunk].index[0]\n            lastrow = df[df['chunk'] == chunk].index[-1]\n            # fill all inbetween with the chunk number\n            df.loc[firstrow:lastrow, 'chunk'] = chunk\n            # get the rows of the chunk\n            chunkrows = df[df['chunk'] == chunk].copy()\n            # interpolate\n            chunkrows[var] = chunkrows[var].interpolate(method='linear', x = chunkrows['time'])\n            # put the interpolated chunk back to the df\n            df.loc[df['chunk'] == chunk, var] = chunkrows[var]\n\n    # get rid of the chunk column\n    df.drop('chunk', axis=1, inplace=True)\n\n    return df\n\n\n\n\nMerging signals on a common sampling rate\n\ndesired_sr = 0.5    # this is the sr we are going to merge on (in Hz/sec)\n\nerror_log = []\n\nfor file in bbfiles:\n\n    bb_df = pd.read_csv(file)\n    # get trial id\n    trialid = bb_df['TrialID'][0]\n\n    print('working on ' + trialid)\n    \n    # find the same trialid in idfiles\n    id_file = [x for x in idfiles if trialid in x]\n    try:\n        id_df = pd.read_csv(id_file[0])\n    except IndexError:\n        print('IndexError: ' + trialid + ' not found')\n        errormessage = 'IndexError: ' + trialid + ' not found for ID'\n        error_log.append(errormessage)\n        continue\n    \n    # find the same trialid in mtfiles\n    mt_file = [x for x in mtfiles if trialid in x]\n    try:\n        mt_df = pd.read_csv(mt_file[0])\n    except IndexError:\n        print('IndexError: ' + trialid + ' not found')\n        errormessage = 'IndexError: ' + trialid + ' not found for MT'\n        error_log.append(errormessage)\n        continue\n    # rename Time to time\n    mt_df.rename(columns={'Time': 'time'}, inplace=True)\n\n    # find the same trialid in envfiles\n    env_file = [x for x in envfiles if trialid in x]\n    try:\n        env_df = pd.read_csv(env_file[0])\n    except IndexError:\n        print('IndexError: ' + trialid + ' not found')\n        errormessage = 'IndexError: ' + trialid + ' not found for ENV'\n        error_log.append(errormessage)\n        continue\n    \n    # rename trialID to TrialID\n    env_df.rename(columns={'trialID': 'TrialID'}, inplace=True)\n\n    # find the same trialid in f0files\n    f0_file = [x for x in f0files if trialid in x]\n    try:\n        f0_df = pd.read_csv(f0_file[0])\n    except IndexError:\n        print('IndexError: ' + trialid + ' not found')\n        errormessage = 'IndexError: ' + trialid + ' not found for F0'\n        error_log.append(errormessage)\n        continue\n\n    # rename time_ms to time\n    f0_df.rename(columns={'time_ms': 'time'}, inplace=True)\n    # rename ID to TrialID\n    f0_df.rename(columns={'ID': 'TrialID'}, inplace=True)\n\n    # find the same trialid in ikfiles\n    ik_file = [x for x in ikfiles if trialid in x]\n    try:\n        ik_df = pd.read_csv(ik_file[0])\n    except IndexError:\n        print('IndexError: ' + trialid + ' not found')\n        errormessage = 'IndexError: ' + trialid + ' not found for IK'\n        error_log.append(errormessage)\n        continue\n\n    # find the same trialid in formants\n    formants_file = [x for x in formants if trialid in x]\n    try:\n        formants_df = pd.read_csv(formants_file[0])\n    except IndexError:\n        print('IndexError: ' + trialid + 'not found')\n        errormessage = 'IndexError: ' + trialid + ' not found for formants'\n        error_log.append(errormessage)\n        continue\n\n    # rename triald to TrialID\n    formants_df.rename(columns={'trialid': 'TrialID'}, inplace=True)\n    formants_df = formants_df[['time', 'f1', 'f2', 'f3', 'TrialID']]\n    formants_df['time'] = formants_df['time'] * 1000\n\n    # find the same triaalid in CoG\n    sc_file = [x for x in scfiles if trialid in x]\n    try:\n        sc_df = pd.read_csv(sc_file[0])\n    except IndexError:\n        print('IndexError: ' + trialid + 'not found')\n        errormessage = 'IndexError: ' + trialid + ' not found for CoG'\n        error_log.append(errormessage)\n        continue\n\n    # write error log\n    with open(TSmerged + '/error_log.txt', 'w') as f:\n        for item in error_log:\n            f.write(\"%s\\n\" % item)\n\n\n    ############## MERGING ########################\n\n    #### regularize sr in bb\n    time_new = np.arange(0, max(bb_df['time']), 1/desired_sr)\n    bb_interp = pd.DataFrame({'time': time_new})\n    \n    # interpolate all columns in samplebb \n    colstoint = bb_df.columns\n    colstoint = [x for x in colstoint if 'time' not in x]\n    colstoint = [x for x in colstoint if 'TrialID' not in x]\n    colstoint = [x for x in colstoint if 'FileInfo' not in x]\n\n    for col in colstoint:\n        bb_interp[col] = bb_df[col].interpolate(method='linear', x = bb_interp['time'])\n\n    # add trialid and time\n    bb_interp['TrialID'] = trialid\n    bb_interp['FileInfo'] = bb_df['FileInfo'][0]\n    \n    ########### merge the bb_interp with env\n    # merge the two dataframes\n    merge1 = pd.merge(bb_interp, env_df, on=['time', 'TrialID'], how='outer')\n\n    # interpolate missing values of envelope and audio\n    colstoint = merge1.columns\n    colstoint = [x for x in colstoint if 'audio' in x or 'envelope' in x]\n\n    for col in colstoint: \n        merge1[col] = merge1[col].interpolate(method='linear', x = merge1['time'])\n\n    # now we can kick out all values where COPc is NaN\n    merge1 = merge1[~np.isnan(merge1['COPc'])]\n\n    ########### merge with ID\n    # merge the two dataframes\n    merge2 = pd.merge(merge1, id_df, on=['time', 'TrialID'], how='outer')\n\n    # get cols of sampleid\n    colstoint = id_df.columns\n    colstoint = [x for x in colstoint if 'time' not in x]\n    colstoint = [x for x in colstoint if 'TrialID' not in x]\n\n    # interpolate \n    for col in colstoint:\n        merge2[col] = merge2[col].interpolate(method='linear', x = merge2['time'])\n\n    # now we can kick out all values where COPc is NaN to get sampling rate back to 500 \n    merge2 = merge2[~np.isnan(merge2['COPc'])]\n\n    ########### merge with MT\n    # merge the two dataframes\n    merge3 = pd.merge(merge2, mt_df, on=['time', 'TrialID'], how='outer')\n\n    # get cols of samplemt\n    colstoint = mt_df.columns\n    colstoint = [x for x in colstoint if 'time' not in x]\n    colstoint = [x for x in colstoint if 'TrialID' not in x]\n\n    # interpolate missing values of from mt\n    for col in colstoint:\n        merge3[col] = merge3[col].interpolate(method='linear', x = merge3['time'])\n\n    # now we can kick out all values where COPc is NaN\n    merge3 = merge3[~np.isnan(merge3['COPc'])]\n\n    ########### merge with F0\n    # for interpolation, we need to again parse f0 into chunks of non-NaN values\n    f0_df, chunks = create_chunks(f0_df, 'f0')\n    \n    # now we can merge\n    merge4 = pd.merge(merge3, f0_df, on=['time', 'TrialID'], how='outer')\n\n\n    # interpolate f0 signal, while maintaining discontinuities\n    merge4 = interpolate_chunks(merge4, chunks, 'f0')\n\n    # now we can drop all rows where COPc is NaN\n    merge4 = merge4[~np.isnan(merge4['COPc'])]\n\n    ########### merge with IK\n    merge5 = pd.merge(merge4, ik_df, on=['time', 'TrialID'], how='outer')\n\n    # get cols of sampleik\n    colstoint = ik_df.columns\n    colstoint = [x for x in colstoint if 'time' not in x]\n    colstoint = [x for x in colstoint if 'TrialID' not in x]\n\n    # interpolate missing values of from ik\n    for col in colstoint:\n        merge5[col] = merge5[col].interpolate(method='linear', x = merge5['time'])\n\n    # now we can kick out all values where COPc is NaN\n    merge5 = merge5[~np.isnan(merge5['COPc'])]\n\n    ########### merge with formants\n    merge6 = pd.merge(merge5, formants_df, on=['time', 'TrialID'], how='outer')\n\n    # get cols of sampleformants\n    colstoint = formants_df.columns\n    colstoint = [x for x in colstoint if 'time' not in x]\n    colstoint = [x for x in colstoint if 'TrialID' not in x]\n\n    # interpolate missing values of from formants - currently they do not have NaNs so we can interpolate the whole signal instead of only non-NaN chunks\n    for col in colstoint:\n        merge6[col] = merge6[col].interpolate(method='linear', x = merge6['time'])\n\n    # now we can kick out all values where COPc is NaN\n    merge6 = merge6[~np.isnan(merge6['COPc'])]\n\n    ########### merge with CoG\n    merge7 = pd.merge(merge6, sc_df, on=['time', 'TrialID'], how='outer')\n\n    # get cols of samplespecCentroid\n    colstoint = sc_df.columns\n    colstoint = [x for x in colstoint if 'time' not in x]\n    colstoint = [x for x in colstoint if 'TrialID' not in x]\n\n    # for interpolation, we need to again parse specCentroid into chunks of non-NaN values\n    sc, chunks = create_chunks(sc_df, 'CoG')\n    \n    # now we merge\n    merge7 = pd.merge(merge6, sc, on=['time', 'TrialID'], how='outer')\n\n    # interpolate CoG signal, while maintaining discontinuities\n    merge7 = interpolate_chunks(merge7, chunks, 'CoG')\n\n    # now we can kick out all values where COPc is NaN\n    merge7 = merge7[~np.isnan(merge7['COPc'])]\n\n    # this is final df\n    merge_final = merge7     \n\n    ############## FORMANT ADAPTATION ########################\n\n    # find peaks in envelope, with min=mean\n    peaks, _ = scipy.signal.find_peaks(merge_final['envelope'], height=np.mean(merge_final['envelope']))\n    # get widths of the peaks\n    widths = scipy.signal.peak_widths(merge_final['envelope'], peaks, rel_height=0.95)\n    # peak width df with starts and ends\n    peak_widths = pd.DataFrame({'start': widths[2], 'end': widths[3]})\n\n    # now create a new column env_weak_width, and put 0s everywhere, and 1s in the intervals of the width\n    merge_final['env_peak_width'] = 0\n    for index, row in peak_widths.iterrows():\n        merge_final.loc[int(row['start']):int(row['end']), 'env_peak_width'] = 1\n\n    # now we will create formant columns, where we will keep only formants in the intervals of env_pak_width OR where f0 is not NaN\n    merge_final['f1_clean_f0'] = merge_final['f1']\n    merge_final['f2_clean_f0'] = merge_final['f2']\n    merge_final['f3_clean_f0'] = merge_final['f3']\n\n    # where f0 is NaN, we will put NaN - these are formants during f0 only\n    merge_final.loc[np.isnan(merge_final['f0']), 'f1_clean_f0'] = np.nan\n    merge_final.loc[np.isnan(merge_final['f0']), 'f2_clean_f0'] = np.nan\n    merge_final.loc[np.isnan(merge_final['f0']), 'f3_clean_f0'] = np.nan\n\n    # we will also create formants, where we will keep only those in the intervals of env_pak_width\n    merge_final['f1_clean_env'] = merge_final['f1']\n    merge_final['f2_clean_env'] = merge_final['f2']\n    merge_final['f3_clean_env'] = merge_final['f3']\n\n    # where env_peak_width is 0, we will put NaN - these are formants during envelope peaks only\n    merge_final.loc[merge_final['env_peak_width'] == 0, 'f1_clean_env'] = np.nan\n    merge_final.loc[merge_final['env_peak_width'] == 0, 'f2_clean_env'] = np.nan\n    merge_final.loc[merge_final['env_peak_width'] == 0, 'f3_clean_env'] = np.nan\n\n    ## now we create formants where we copy values from clean_env and clean_f0\n    merge_final['f1_clean'] = merge_final['f1_clean_env']\n    merge_final['f2_clean'] = merge_final['f2_clean_env']\n    merge_final['f3_clean'] = merge_final['f3_clean_env']\n\n    # where formant is now NaN, copy values from f_clean_f0 in case there is a value\n    merge_final.loc[np.isnan(merge_final['f1_clean']), 'f1_clean'] = merge_final['f1_clean_f0']\n    merge_final.loc[np.isnan(merge_final['f2_clean']), 'f2_clean'] = merge_final['f2_clean_f0']\n    merge_final.loc[np.isnan(merge_final['f3_clean']), 'f3_clean'] = merge_final['f3_clean_f0']\n\n    # now calculate formant velocities (but only for the f_clean)\n    merge_final['f1_clean_vel'] = np.insert(np.diff(merge_final['f1_clean']), 0, 0)\n    merge_final['f2_clean_vel'] = np.insert(np.diff(merge_final['f2_clean']), 0, 0)\n    merge_final['f3_clean_vel'] = np.insert(np.diff(merge_final['f3_clean']), 0, 0)\n\n    # smooth\n    merge_final['f1_clean_vel'] = scipy.signal.savgol_filter(merge_final['f1_clean_vel'], 5, 3)\n    merge_final['f2_clean_vel'] = scipy.signal.savgol_filter(merge_final['f2_clean_vel'], 5, 3)\n    merge_final['f3_clean_vel'] = scipy.signal.savgol_filter(merge_final['f3_clean_vel'], 5, 3)\n\n    ########## POWER ####################\n    groups = ['lowerbody', 'leg', 'head', 'arm']\n\n    for group in groups:\n        # get all columns that contain group\n        cols = [x for x in merge_final.columns if group in x]\n\n        # get all columns that contain 'moment_sum'\n        torque = [x for x in cols if 'moment_sum' in x]\n        # but not change\n        torque = [x for x in torque if 'change' not in x][0]\n\n        # get all columns that contain 'angSpeed_sum'\n        angSpeed = [x for x in cols if 'angSpeed_sum' in x][0]\n\n        # get power which is moment * angSpeed\n        merge_final[group + '_power'] = merge_final[torque] * merge_final[angSpeed]\n        # smooth\n        merge_final[group + '_power'] = scipy.signal.savgol_filter(merge_final[group + '_power'], 281, 1) # window 281 corresponds to 562 ms (we add +1 to have odd length of window)\n    \n    # write to csv\n    merge_final.to_csv(TSmerged + '/merged_' + trialid + '.csv', index=False)  \n\nHere is an example of the file containing all the data\n\n\n\n\n\n\n\n\n\n\ntime\nleft_back\nright_forward\nright_back\nleft_forward\nCOPXc\nCOPYc\nCOPc\nTrialID\nFileInfo\n...\nf1_clean\nf2_clean\nf3_clean\nf1_clean_vel\nf2_clean_vel\nf3_clean_vel\nlowerbody_power\nleg_power\nhead_power\narm_power\n\n\n\n\n0\n0.0\n1.040234\n0.922185\n1.447832\n1.439272\n0.000153\n-1.427206e-05\n0.000154\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n22.337284\n1.403824\n8.267574\n6.538422\n\n\n1\n2.0\n1.040128\n0.922236\n1.448219\n1.439506\n0.000218\n-4.098744e-06\n0.000218\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n22.285626\n1.400045\n8.244890\n6.524016\n\n\n2\n4.0\n1.040021\n0.922294\n1.448528\n1.439647\n0.000270\n5.396631e-08\n0.000270\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n22.233968\n1.396267\n8.222205\n6.509609\n\n\n3\n6.0\n1.039911\n0.922355\n1.448768\n1.439706\n0.000311\n-5.992720e-07\n0.000311\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n22.182311\n1.392488\n8.199520\n6.495203\n\n\n4\n8.0\n1.039796\n0.922414\n1.448945\n1.439691\n0.000342\n-4.980859e-06\n0.000342\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n22.130653\n1.388709\n8.176836\n6.480797\n\n\n5\n10.0\n1.039676\n0.922470\n1.449066\n1.439612\n0.000365\n-1.214347e-05\n0.000365\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n22.078995\n1.384930\n8.154151\n6.466391\n\n\n6\n12.0\n1.039550\n0.922519\n1.449138\n1.439477\n0.000381\n-2.126330e-05\n0.000382\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n22.027338\n1.381152\n8.131466\n6.451984\n\n\n7\n14.0\n1.039417\n0.922559\n1.449166\n1.439293\n0.000391\n-3.163325e-05\n0.000392\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.975680\n1.377373\n8.108782\n6.437578\n\n\n8\n16.0\n1.039277\n0.922589\n1.449157\n1.439068\n0.000396\n-4.265618e-05\n0.000398\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.924022\n1.373594\n8.086097\n6.423172\n\n\n9\n18.0\n1.039130\n0.922607\n1.449115\n1.438808\n0.000397\n-5.383815e-05\n0.000401\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.872364\n1.369816\n8.063413\n6.408766\n\n\n10\n20.0\n1.038976\n0.922613\n1.449046\n1.438519\n0.000395\n-6.478160e-05\n0.000400\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.820707\n1.366037\n8.040728\n6.394359\n\n\n11\n22.0\n1.038816\n0.922605\n1.448954\n1.438206\n0.000390\n-7.517861e-05\n0.000397\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.769049\n1.362258\n8.018043\n6.379953\n\n\n12\n24.0\n1.038648\n0.922583\n1.448844\n1.437876\n0.000383\n-8.480413e-05\n0.000392\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.717391\n1.358479\n7.995359\n6.365547\n\n\n13\n26.0\n1.038475\n0.922548\n1.448720\n1.437532\n0.000374\n-9.350916e-05\n0.000386\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.665734\n1.354701\n7.972674\n6.351140\n\n\n14\n28.0\n1.038295\n0.922499\n1.448586\n1.437179\n0.000365\n-1.012140e-04\n0.000378\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.614076\n1.350922\n7.949989\n6.336734\n\n\n15\n30.0\n1.038111\n0.922437\n1.448445\n1.436821\n0.000354\n-1.079016e-04\n0.000370\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.562418\n1.347143\n7.927305\n6.322328\n\n\n16\n32.0\n1.037923\n0.922362\n1.448301\n1.436461\n0.000343\n-1.136105e-04\n0.000361\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.510761\n1.343364\n7.904620\n6.307922\n\n\n17\n34.0\n1.037731\n0.922276\n1.448155\n1.436101\n0.000332\n-1.184283e-04\n0.000352\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.459103\n1.339586\n7.881936\n6.293515\n\n\n18\n36.0\n1.037537\n0.922179\n1.448012\n1.435745\n0.000321\n-1.224848e-04\n0.000343\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.407445\n1.335807\n7.859251\n6.279109\n\n\n19\n38.0\n1.037341\n0.922072\n1.447872\n1.435395\n0.000309\n-1.259452e-04\n0.000334\n0_1_10_p1\np1_auto_geluiden_corrected\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n21.355788\n1.332028\n7.836566\n6.264703\n\n\n\n\n20 rows × 528 columns\n\n\n\n\nAnd here we visualize some of the timeseries. We can also see that our interpolation did maintain the original discontinuities in the f0 signal.",
    "crumbs": [
      "Processing",
      "Processing III: Merging multimodal data"
    ]
  },
  {
    "objectID": "03_TS_processing/01_TS_processing_motion.html",
    "href": "03_TS_processing/01_TS_processing_motion.html",
    "title": "Processing I: Motion tracking and balance",
    "section": "",
    "text": "Overview\nIn the previous notebook, we have ran pose estimation on the trial videos using OpenPose, and triangulated the coordinates to get 3D coordinates for each trial using Pose2sim. Furthermore, we have performed inverse kinematics and dynamics to extract joint angles and moments using OpenSim.\nIn this script, we will clean the 3D coordinates and joint angle data, and extract further information (such as speed, acceleration, etc.).\n\n\nCode to load packages and prepare the environment\n# packages\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport random\n\ncurfolder = os.getcwd()\n\n# files to work with\nMTfolder = curfolder + '\\\\..\\\\02_MotionTracking_processing\\\\projectdata\\\\' ## FLAGGED CHANGE\nBBfolder = curfolder + '\\\\..\\\\01_XDF_processing\\\\data\\\\Data_processed\\\\Data_trials\\\\'\n\n# folders to save the processed data\nMTfolder_processed = curfolder + '\\\\TS_motiontracking\\\\'\n\n\n\n\nMotion processing - kinematics\nHere we use the keypoint coordinates estimated via OpenPose and triangulated via Pose2Sim. While Pose2sim does provide in-built filter, it is not particularly strong and the data can be still noisy.\nTo decide on the smoothing strength, we can use a custom function check_smooth_strength to check the effect of different smoothing strengths on the data.\n\n\nCode to prepare files to process\nMTtotrack = glob.glob(MTfolder + '*/P*/*', recursive=True)\n\n# get rid of all the folders that are not the ones we want to track, like .sto files\n    ## FLAG! why not do glob.glob(MTfolder + '*/P*/*butterworth*.csv', recursive=True)\nMTtotrack = [x for x in MTtotrack if 'sto' not in x]\nMTtotrack = [x for x in MTtotrack if 'txt' not in x]\nMTtotrack = [x for x in MTtotrack if 'xml' not in x]\nMTtotrack = [x for x in MTtotrack if 'opensim' not in x]\nMTtotrack = [x for x in MTtotrack if 'Results' not in x]\nMTtotrack = [x for x in MTtotrack if 'toml' not in x]\n\nprint(MTtotrack[1:10])\n\nMTfiles_all = []\n\nfor folder in MTtotrack:\n    # last element is trialid\n    trialid = folder.split('\\\\')[-1]\n    \n    # get all csv files in the folder\n    csvfiles = glob.glob(folder + '\\\\**\\\\*.csv', recursive=True)\n    # keep only the ones that have butterworth in the name - those are filtered with native Pose2sim function\n    csvfiles = [x for x in csvfiles if 'butterworth' in x]\n    butterfile = csvfiles[0]\n    # append to list with trialid\n    MTfiles_all.append([trialid, butterfile])\n\n\n\n# function to check different smoothing windows and orders\ndef check_smooth_strength(df, windows, orders, keytoplot):\n\n    # prepare new df\n    df_smooth = pd.DataFrame()\n\n    for win in windows:\n        for ord in orders:\n            df_smooth[keytoplot + '_savgol' + str(win) + '_' + str(ord)] = scipy.signal.savgol_filter(df[keytoplot], win, ord)\n\n    # make R_Hand_x from df_sample a list\n    keytoplot_unsmoothed = df[keytoplot].tolist()\n\n    # load these values into df_smooth as a new column\n    df_smooth[keytoplot] = keytoplot_unsmoothed\n\n    # plot keytoplot in all strngths\n    colstoplot = [x for x in df_smooth.columns if keytoplot in x]\n    plt.figure()\n    for col in colstoplot:\n        plt.plot(df_smooth[col], label=col)\n    plt.legend()\n    plt.show()\n\nHere we can see a timeseries of vertical dimension of the left knee (note that pose2sim gave us y and z dimensions flipped, we will deal with this in a second). Each color represents the timeseries in different smoothed version, pink one is the raw signal (which is smoothed only with the Butterworth 10Hz cut-off filter). The first number in the legend corresponds to window length and the second number to polynomial order.\n\n\ne:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\..\\02_MotionTracking_processing\\projectdata\\Session_0_1\\P0\\0_1_24_p0\\pose-3d\\0_1_24_p0_0-342_filt_butterworth.trc.csv\n\n\n\n\n\n\n\n\n\nHere we can see different setting options for wrist.\n\n\n\n\n\n\n\n\n\nLegs seem to be more noisy than arms. One reason could be that legs are more commonly covered by clothes, which can make the pose estimation more prone to errors. Also, legs often stay without movement, making them more sensitive to noise.\nFor that reason, we opt for different smoothing strengths for leg-related keypoints than for upper body.\nFor lower body positional data, we will use 2nd polynomial Savitzky-Golay filter with window of 816 ms.\nFor upper body positional data, 3rd order Savitzky-Golay filter with window of 400 ms seems to be a good choice. We will use it both for raw coordinates as well as for the derivatives.\nFurther, we obtain the first, second and third derivative of the timeseries, namely speed, acceleration, and jerk. For derivatives, we will use 3rd order Savitzky-Golay filter with window of 400 ms for both.\nLastly, to be able to work with timeseries that represent bigger segment of body than a single joint, we aggregate the kinematic derivatives for each body group (i.e., head, upperbody, arms, lowerbody) by computing euclidian sum over every derivative belonging to the group. This gives us, for instance, a measure for arm speed that represents a sum of speeds of all keypoints associated with the arm (i.e., wrist, elbow, shoulder, index)\n\n\nCode with functions for processing kinematic data\n# function to get euclidian sum of associated keypoints\ndef aggregate_keypoints(df, measurement, finalcolname, use):\n\n    if use == 'kinematics':\n        # group keypoints that belong together\n        lowerbodycols = ['RHip', 'LHip']\n        legcols = ['RKnee', 'RAnkle', 'LAnkle', 'LKnee', 'RHeel', 'LHeel']\n        headcols = ['Head', 'Neck', 'Nose']\n        armcols = ['RShoulder', 'RElbow', 'RWrist', 'LShoulder', 'LElbow', 'LWrist', 'RIndex', 'LIndex']\n\n        groups = [lowerbodycols, legcols, headcols, armcols]\n\n    elif use == 'angles':\n        pelviscols = ['pelvis']\n        spinecols = ['L5_S1', 'L4_L5', 'L3_L4', 'L2_L3', 'L1_L2', 'L1_T12']\n        lowerbodycols = ['pelvis', 'hip']\n        legcols = ['knee', 'ankle', 'subtalar']\n        headcols = ['neck']\n        armcols = ['arm', 'elbow', 'wrist', 'pro_sup']\n\n        groups = [lowerbodycols, legcols, headcols, armcols, pelviscols, spinecols]\n\n    # make subdf only with speed\n    subdf = df[[x for x in df.columns if measurement in x]]\n\n    # loop through each joint group\n    for group in groups:\n        # get cols\n        cols = [x for x in subdf.columns if any(y in x for y in group)]\n        subdf_temp = subdf[cols]\n\n        for index, row in subdf_temp.iterrows():\n            # get all values of that row\n            values = row.values\n            # calculate euclidian sum\n            euclidian_sum = np.sqrt(np.sum(np.square(values))) ## FLAGGED: possibly normalize\n            # get a name for new col\n            if group == lowerbodycols:\n                colname = 'lowerbody'\n            elif group == legcols:\n                colname = 'leg'\n            elif group == headcols:\n                colname = 'head'\n            elif group == armcols:\n                colname = 'arm'\n            elif group == pelviscols:\n                colname = 'pelvis'\n            elif group == spinecols:\n                colname = 'spine'\n                \n\n            df.loc[index, colname + finalcolname] = euclidian_sum\n\n    return df\n\n\n# get kinematic derivatives\ndef get_derivatives(df, sr, upperbodycols, lowerbodycols, use):\n\n    mtcols = df.columns\n    if use == 'kinematics':\n        # get rid of cols that are not x, y or z\n        mtcols = [x for x in mtcols if '_x' in x or '_y' in x or '_z' in x]\n    \n\n        # prepare cols for speed\n        cols = [x.split('_')[0] for x in mtcols]\n        colsforspeed = list(set(cols))\n\n        # for each unique colname (cols), calculate speed \n        for col in colsforspeed:\n            # get x and y columns\n            x = df[col + '_x']\n            y = df[col + '_y']\n            z = df[col + '_z'] # note that y and z are flipped\n            # calculate speed\n            speed = np.insert(np.sqrt(np.diff(x)**2 + np.diff(y)**2 + np.diff(z)**2),0,0)\n            # multiply the values by sr, because now we have values in m/(s/sr)\n            speed = speed*sr\n\n            # smooth\n            speed = scipy.signal.savgol_filter(speed, 25, 3)\n\n            # if the col contains wrist, we will alco calculate the vertical velocity (z dimension)\n            if 'Wrist' in col:\n                verticvel = np.insert(np.diff(z), 0, 0)\n                verticvel = verticvel*sr\n                verticvel = scipy.signal.savgol_filter(verticvel, 25, 3)\n\n            # derive acceleration   \n            acceleration = np.insert(np.diff(speed), 0, 0)\n            acceleration = scipy.signal.savgol_filter(acceleration, 25, 3)\n\n            # derive jerk\n            jerk = np.insert(np.diff(acceleration), 0, 0)\n            jerk = scipy.signal.savgol_filter(jerk, 25, 3)\n\n            # new_data\n            new_data = pd.DataFrame({col + '_speed': speed, col + '_acc': acceleration, col + '_jerk': jerk})\n            df = pd.concat([df, new_data], axis=1)\n\n    elif use == 'angles':\n        # get rid of cols that are not angles (so skip time)\n        mtcols = mtcols[1:]\n\n        # derive speed\n        for col in mtcols:\n            speed = np.insert(np.diff(df[col]), 0, 0)\n            speed = speed*sr\n            speed = scipy.signal.savgol_filter(speed, 35, 1)\n\n            # derive acceleration\n            acceleration = np.insert(np.diff(speed), 0, 0)\n            acceleration = scipy.signal.savgol_filter(acceleration, 35, 1)\n            \n            # derive jerk\n            jerk = np.insert(np.diff(acceleration), 0, 0)\n            jerk = scipy.signal.savgol_filter(jerk, 35, 1)\n\n            # new_data\n            new_data = pd.DataFrame({col + '_speed': speed, col + '_acc': acceleration, col + '_jerk': jerk})\n            df = pd.concat([df, new_data], axis=1)\n\n    return df\n\n\n\n# upper body cols\nupperbodycols = ['Head', 'Neck', 'RShoulder', 'RElbow', 'RWrist', 'LShoulder', 'LElbow', 'LWrist', 'Nose', 'RIndex', 'LIndex']\n# lower body cols\nlowerbodycols = ['RHip', 'RKnee', 'RAnkle', 'RHeel', 'LHip', 'LKnee', 'LAnkle', 'LHeel']\n\nfor folder in MTtotrack:\n    # last element is trialid\n    trialid = folder.split('\\\\')[-1]\n    print('working on:' + trialid)\n    \n    # get all csv files in the folder\n    csvfiles = glob.glob(folder + '/**/*.csv', recursive=True)\n    # keep only the ones that have butterworth in the name\n    csvfiles = [x for x in csvfiles if 'butterworth' in x]\n    butterfile = csvfiles[0]\n\n    # load it\n    mt = pd.read_csv(butterfile)\n\n    # the mt is missing 0 ms timepoint, so we need to create a row that copies the first row of mt and time = 0\n    padrow = mt.iloc[0].copy()\n    padrow['Time'] = 0\n\n    # concatenate it to the beginning of mt \n    mt = pd.concat([pd.DataFrame(padrow).T, mt], ignore_index=True)\n\n    # keep only cols of interest\n    colstokeep = [\"Time\", \"RHip\", \"RKnee\", \"RAnkle\", \"RHeel\", \"LHip\", \"LKnee\", \"LAnkle\", \"LHeel\", \"Neck\", \"Head\", \"Nose\", \"RShoulder\", \"RElbow\", \"RWrist\", \"RIndex\", \"LShoulder\", \"LElbow\", \"LWrist\",\n    \"LIndex\",\n]\n    mt = mt[[col for col in mt.columns if any(x in col for x in colstokeep)]]\n\n    # flip y and z dimension as they are reversed from OpenPose/Pose2sim\n\n    # if col has _y in it, replace it by _temp\n    mt.columns = [x.replace('_y', '_temp') for x in mt.columns]\n    # replace _z by _y\n    mt.columns = [x.replace('_z', '_y') for x in mt.columns]\n    # replace _temp by _z\n    mt.columns = [x.replace('_temp', '_z') for x in mt.columns]\n\n    ####### SMOOTHING ######\n\n    # smooth all columns except time with savgol\n    mtcols = mt.columns\n    colstosmooth = mtcols[:-1]\n\n    mt_smooth = pd.DataFrame()\n\n    for col in colstosmooth:\n        colname = col.split('_')[0] # to get rid of _x, _y, _z\n        if colname in upperbodycols:\n            mt_smooth[col] = scipy.signal.savgol_filter(mt[col], 25, 3)\n        elif colname in lowerbodycols:\n            mt_smooth[col] = scipy.signal.savgol_filter(mt[col], 51, 2)\n\n    # And put them all to cms\n    mt_smooth = mt_smooth*100\n\n    # add back time column\n    mt_smooth['Time'] = mt['Time']\n\n    # get sampling rate\n    sr = 1/np.mean(np.diff(mt['Time']))\n\n    ###### DERIVATIVES ######\n\n    # get kinematic derivatives\n    mt_smooth = get_derivatives(mt_smooth, sr, upperbodycols, lowerbodycols, 'kinematics')\n\n    ###### AGGREGATING ######\n\n    # getting aggreagated sums for groups of cols\n    mt_smooth = aggregate_keypoints(mt_smooth, 'speed', '_speedKin_sum', 'kinematics')\n    mt_smooth = aggregate_keypoints(mt_smooth, 'acc', '_accKin_sum', 'kinematics')\n    mt_smooth = aggregate_keypoints(mt_smooth, 'jerk', '_jerkKin_sum', 'kinematics')\n\n    # add trialid\n    mt_smooth['TrialID'] = trialid\n    # convert time to ms\n    mt_smooth['Time'] = mt_smooth['Time']*1000\n\n    # write to csv\n    mt_smooth.to_csv(MTfolder_processed + '/mt_' + trialid + '.csv', index=False)\n\nHere is an example of the file\n\n\n\n\n\n\n\n\n\n\nRHip_x\nRHip_z\nRHip_y\nRKnee_x\nRKnee_z\nRKnee_y\nRAnkle_x\nRAnkle_z\nRAnkle_y\nRHeel_x\n...\narm_speedKin_sum\nlowerbody_accKin_sum\nleg_accKin_sum\nhead_accKin_sum\narm_accKin_sum\nlowerbody_jerkKin_sum\nleg_jerkKin_sum\nhead_jerkKin_sum\narm_jerkKin_sum\nTrialID\n\n\n\n\n0\n12.186808\n23.129494\n1.692073\n15.110699\n22.532687\n-38.543548\n16.437185\n22.694474\n-74.834063\n18.996019\n...\n26.083522\n0.225640\n0.590822\n0.161665\n1.651498\n0.022804\n0.046088\n0.092108\n0.341816\n0_2_31_p1\n\n\n1\n12.195677\n23.108011\n1.675542\n15.147758\n22.574564\n-38.544712\n16.440867\n22.691252\n-74.855128\n18.976731\n...\n23.961073\n0.187954\n0.502362\n0.027073\n1.414454\n0.024985\n0.058605\n0.082632\n0.300132\n0_2_31_p1\n\n\n2\n12.204523\n23.086249\n1.660162\n15.183910\n22.615387\n-38.545741\n16.444388\n22.688281\n-74.874888\n18.957985\n...\n22.197559\n0.154021\n0.417187\n0.100069\n1.315854\n0.028136\n0.068901\n0.072654\n0.261180\n0_2_31_p1\n\n\n3\n12.213347\n23.064209\n1.645932\n15.219154\n22.655155\n-38.546635\n16.447747\n22.685561\n-74.893344\n18.939780\n...\n20.747395\n0.123840\n0.335606\n0.187785\n1.297296\n0.030733\n0.076200\n0.062197\n0.224556\n0_2_31_p1\n\n\n4\n12.222149\n23.041892\n1.632852\n15.253492\n22.693868\n-38.547393\n16.450944\n22.683094\n-74.910496\n18.922118\n...\n19.573630\n0.097610\n0.258204\n0.252369\n1.303122\n0.032213\n0.080447\n0.051345\n0.190306\n0_2_31_p1\n\n\n5\n12.230929\n23.019295\n1.620923\n15.286921\n22.731526\n-38.548016\n16.453979\n22.680877\n-74.926343\n18.904998\n...\n18.646790\n0.075868\n0.186154\n0.295498\n1.297513\n0.032427\n0.081814\n0.040245\n0.159050\n0_2_31_p1\n\n\n6\n12.239686\n22.996421\n1.610143\n15.319444\n22.768130\n-38.548503\n16.456853\n22.678913\n-74.940886\n18.888420\n...\n17.942303\n0.059671\n0.122439\n0.319353\n1.262916\n0.031396\n0.080561\n0.029151\n0.132192\n0_2_31_p1\n\n\n7\n12.248421\n22.973268\n1.600514\n15.351059\n22.803679\n-38.548855\n16.459565\n22.677200\n-74.954125\n18.872383\n...\n17.436996\n0.050448\n0.077330\n0.326231\n1.193408\n0.029216\n0.076991\n0.018637\n0.112128\n0_2_31_p1\n\n\n8\n12.257134\n22.949838\n1.592035\n15.381767\n22.838172\n-38.549072\n16.462115\n22.675738\n-74.966060\n18.856889\n...\n17.105498\n0.048612\n0.076136\n0.318539\n1.089850\n0.026020\n0.071432\n0.010923\n0.101805\n0_2_31_p1\n\n\n9\n12.265825\n22.926129\n1.584706\n15.411567\n22.871611\n-38.549153\n16.464503\n22.674528\n-74.976690\n18.841937\n...\n16.917416\n0.052072\n0.111844\n0.298855\n0.957421\n0.021964\n0.064229\n0.012538\n0.102606\n0_2_31_p1\n\n\n10\n12.274493\n22.902141\n1.578528\n15.440461\n22.903995\n-38.549099\n16.466730\n22.673569\n-74.986016\n18.827527\n...\n16.835927\n0.057658\n0.155242\n0.270077\n0.805033\n0.017219\n0.055746\n0.020884\n0.112358\n0_2_31_p1\n\n\n11\n12.283139\n22.877876\n1.573500\n15.468446\n22.935325\n-38.548910\n16.468795\n22.672862\n-74.994038\n18.813659\n...\n16.817918\n0.063030\n0.196240\n0.235745\n0.646906\n0.011987\n0.046385\n0.030254\n0.126842\n0_2_31_p1\n\n\n12\n12.291763\n22.853332\n1.569622\n15.495525\n22.965599\n-38.548585\n16.470699\n22.672407\n-75.000755\n18.800334\n...\n16.815337\n0.066837\n0.231722\n0.200699\n0.508104\n0.006598\n0.036625\n0.039232\n0.142296\n0_2_31_p1\n\n\n13\n12.300364\n22.828510\n1.566894\n15.521696\n22.994819\n-38.548124\n16.472440\n22.672203\n-75.006168\n18.787550\n...\n14.656360\n0.081540\n0.284913\n0.187160\n0.383159\n0.002108\n0.025516\n0.053032\n0.173949\n0_2_31_p1\n\n\n14\n12.308943\n22.803410\n1.565317\n15.546960\n23.022984\n-38.547529\n16.474020\n22.672250\n-75.010277\n18.775308\n...\n15.012604\n0.080498\n0.279402\n0.182591\n0.443300\n0.007590\n0.018007\n0.059507\n0.170806\n0_2_31_p1\n\n\n\n\n15 rows × 128 columns\n\n\n\n\nLet’s check one file to see how the data looks like by plotting RWrist and its kinematics, and also the euclidian sum for the whole arm along with it (as dashed black line)\nNote that aggregates will always be directionless (i.e., in positive numbers) as they are squared when computed.\n\n\n\n\n\n\n\n\n\n\n\nMotion processing - inverse kinematics\nIn the previous notebook, we have extracted joint angles using OpenSim (Seth et al. (26. 7. 2018)). Now again, we clean the data, smooth them, and extract further information before saving it into csv file per trial\nWe can once again check what would be the proper filter\n\n\nCode to prepare environment\n# get all mot files in the folder\nmot_files = glob.glob(MTfolder + '*/P*/*/*.mot', recursive=True)\nkeypoints = ['wrist', 'pro_sup', 'elbow', 'arm', 'neck', 'subtalar', 'ankle', 'knee', 'hip', 'pelvis', 'L5_S1', 'L4_L5', 'L3_L4', 'L2_L3', 'L1_L2', 'L1_T12']\n\n\n\n\n\n\n\n\n\n\n\nAnd for legs\n\n\n\n\n\n\n\n\n\nWe will apply a bit stronger filter of 1st order with span of 560 ms because the data are more noisy than the kinematics.\n\n# get all mot files in the folder\nmot_files = glob.glob(MTfolder + '*/P*/*/*.mot', recursive=True)\nkeypoints = ['wrist', 'pro_sup', 'elbow', 'arm', 'neck', 'subtalar', 'ankle', 'knee', 'hip', 'pelvis', 'L5_S1', 'L4_L5', 'L3_L4', 'L2_L3', 'L1_L2', 'L1_T12']\n\nfor mot in mot_files:\n    # get trialid\n    trialid = mot.split('\\\\')[-1].split('.')[0]\n    print('working on ' + trialid)\n\n    # get rid of the first element before _\n    trialid = '_'.join(trialid.split('_')[1:])\n\n    # load it\n    mot_df = pd.read_csv(mot, sep='\\t', skiprows=10)\n    \n    # pad 0 ms row\n    padrow = mot_df.iloc[0].copy()\n    padrow['time'] = 0\n\n    # concatenate it to the beginning of mot_df\n    mot_df = pd.concat([pd.DataFrame(padrow).T, mot_df], ignore_index=True)\n    \n    # get the sr\n    sr = 1/np.mean(np.diff(mot_df['time']))\n\n    ##### SMOOTHING ######\n\n    # smooth all columns except the firts time (time) and last (trialid)\n    colstosmooth = [x for x in mot_df.columns if 'time' not in x]\n\n    # smooth\n    for col in colstosmooth:\n        mot_df[col] = scipy.signal.savgol_filter(mot_df[col], 35, 1)\n        # convert to radians\n        mot_df[col] = np.deg2rad(mot_df[col])\n\n    # keep only columns you might use\n    coi = [x for x in mot_df.columns if any(y in x for y in keypoints) or 'time' in x or 'TrialID' in x]\n    mot_df2 = mot_df[coi]\n\n    ##### DERIVATIVES ######\n\n    # get derivatives\n    mot_df2 = get_derivatives(mot_df2, sr, [], [], 'angles')\n\n    #### AGGREGATING #####\n\n    # aggregate data\n    mot_df2 = aggregate_keypoints(mot_df2, 'speed', '_angSpeed_sum', 'angles')\n    mot_df2 = aggregate_keypoints(mot_df2, 'acc', '_angAcc_sum', 'angles')\n    mot_df2 = aggregate_keypoints(mot_df2, 'jerk', '_angJerk_sum', 'angles')\n\n    # add time and trialid\n    mot_df2['time'] = mot_df['time']\n    # convert time to ms\n    mot_df2['time'] = mot_df2['time']*1000\n    mot_df2['TrialID'] = trialid\n\n    # write to csv\n    mot_df2.to_csv(MTfolder_processed + '/ik_' + trialid + '.csv', index=False)\n    \n\nHere is an example file\n\n\n\n\n\n\n\n\n\n\ntime\npelvis_tilt\npelvis_list\npelvis_rotation\npelvis_tx\npelvis_ty\npelvis_tz\nhip_flexion_r\nhip_adduction_r\nhip_rotation_r\n...\narm_angAcc_sum\npelvis_angAcc_sum\nspine_angAcc_sum\nlowerbody_angJerk_sum\nleg_angJerk_sum\nhead_angJerk_sum\narm_angJerk_sum\npelvis_angJerk_sum\nspine_angJerk_sum\nTrialID\n\n\n\n\n0\n0.000\n-0.494779\n1.465690\n-1.092904\n0.003865\n0.002655\n0.002283\n0.088633\n0.017501\n-0.290667\n...\n0.030528\n0.016054\n0.002946\n0.000372\n0.000162\n0.000206\n0.000611\n0.000305\n0.000047\n0_1_13_p1\n\n\n1\n16.667\n-0.491722\n1.464707\n-1.094887\n0.003866\n0.002649\n0.002285\n0.087100\n0.017773\n-0.290882\n...\n0.030335\n0.016026\n0.002918\n0.000340\n0.000164\n0.000205\n0.000585\n0.000274\n0.000046\n0_1_13_p1\n\n\n2\n33.333\n-0.488665\n1.463725\n-1.096871\n0.003868\n0.002642\n0.002288\n0.085566\n0.018045\n-0.291098\n...\n0.030151\n0.015998\n0.002890\n0.000309\n0.000168\n0.000204\n0.000562\n0.000243\n0.000044\n0_1_13_p1\n\n\n3\n50.000\n-0.485609\n1.462742\n-1.098855\n0.003870\n0.002636\n0.002290\n0.084033\n0.018318\n-0.291313\n...\n0.029977\n0.015971\n0.002863\n0.000281\n0.000172\n0.000204\n0.000542\n0.000214\n0.000043\n0_1_13_p1\n\n\n4\n66.667\n-0.482552\n1.461759\n-1.100839\n0.003872\n0.002630\n0.002293\n0.082500\n0.018590\n-0.291528\n...\n0.029813\n0.015944\n0.002837\n0.000256\n0.000178\n0.000203\n0.000526\n0.000186\n0.000043\n0_1_13_p1\n\n\n5\n83.333\n-0.479495\n1.460777\n-1.102823\n0.003874\n0.002623\n0.002295\n0.080967\n0.018862\n-0.291743\n...\n0.029658\n0.015919\n0.002811\n0.000235\n0.000185\n0.000202\n0.000514\n0.000160\n0.000042\n0_1_13_p1\n\n\n6\n100.000\n-0.476438\n1.459794\n-1.104807\n0.003876\n0.002617\n0.002298\n0.079434\n0.019134\n-0.291958\n...\n0.029514\n0.015894\n0.002786\n0.000219\n0.000194\n0.000202\n0.000505\n0.000136\n0.000042\n0_1_13_p1\n\n\n7\n116.667\n-0.473381\n1.458812\n-1.106791\n0.003878\n0.002610\n0.002300\n0.077901\n0.019407\n-0.292173\n...\n0.029380\n0.015870\n0.002761\n0.000209\n0.000202\n0.000201\n0.000502\n0.000118\n0.000043\n0_1_13_p1\n\n\n8\n133.333\n-0.470325\n1.457829\n-1.108775\n0.003880\n0.002604\n0.002303\n0.076367\n0.019679\n-0.292388\n...\n0.029256\n0.015847\n0.002737\n0.000206\n0.000212\n0.000200\n0.000502\n0.000108\n0.000043\n0_1_13_p1\n\n\n9\n150.000\n-0.467268\n1.456846\n-1.110759\n0.003882\n0.002598\n0.002305\n0.074834\n0.019951\n-0.292603\n...\n0.029142\n0.015824\n0.002714\n0.000211\n0.000223\n0.000200\n0.000507\n0.000107\n0.000044\n0_1_13_p1\n\n\n10\n166.667\n-0.464211\n1.455864\n-1.112743\n0.003883\n0.002591\n0.002308\n0.073301\n0.020223\n-0.292818\n...\n0.029039\n0.015802\n0.002691\n0.000223\n0.000233\n0.000199\n0.000517\n0.000116\n0.000046\n0_1_13_p1\n\n\n11\n183.333\n-0.461154\n1.454881\n-1.114727\n0.003885\n0.002585\n0.002310\n0.071768\n0.020496\n-0.293033\n...\n0.028947\n0.015782\n0.002669\n0.000240\n0.000245\n0.000198\n0.000530\n0.000132\n0.000048\n0_1_13_p1\n\n\n12\n200.000\n-0.458098\n1.453899\n-1.116711\n0.003887\n0.002578\n0.002313\n0.070235\n0.020768\n-0.293248\n...\n0.028866\n0.015761\n0.002648\n0.000262\n0.000257\n0.000198\n0.000548\n0.000155\n0.000050\n0_1_13_p1\n\n\n13\n216.667\n-0.455041\n1.452916\n-1.118695\n0.003889\n0.002572\n0.002316\n0.068701\n0.021040\n-0.293463\n...\n0.028796\n0.015742\n0.002628\n0.000288\n0.000269\n0.000197\n0.000569\n0.000181\n0.000052\n0_1_13_p1\n\n\n14\n233.333\n-0.451984\n1.451933\n-1.120679\n0.003891\n0.002565\n0.002318\n0.067168\n0.021312\n-0.293678\n...\n0.028737\n0.015724\n0.002608\n0.000317\n0.000281\n0.000196\n0.000592\n0.000208\n0.000054\n0_1_13_p1\n\n\n\n\n15 rows × 240 columns\n\n\n\n\nHere we can see the joint angle speed next to kinematic speed.\n\n\n\n\n\n\n\n\n\n\n\nMotion processing - inverse dynamics\nNow we do exactly the same also for inverse dynamics data (joint torques/moments).\n\n\nCode to prepare environment\n# in MTfolders, find all sto files\nsto_files = glob.glob(MTfolder + '*/P*/*/*.sto', recursive=True)\nsto_files = [x for x in sto_files if 'ID' in x]\n\n\nLet’s once again check the different smoothing strengths\n\n\n\n\n\n\n\n\n\nAnd for legs\n\n\n\n\n\n\n\n\n\nWe will again reuse 1st order Savitzky-Golay filter with window of 560 ms for the moments and their first derivate (torque/moment change).\n\n# in MTfolders, find all sto files\nsto_files = glob.glob(MTfolder + '*/P*/*/*.sto', recursive=True)\nsto_files = [x for x in sto_files if 'ID' in x]\n\nfor sto in sto_files:\n\n    # from the filename, get the trialid\n    trialid = sto.split('\\\\')[-1].split('.')[0]\n    trialid = '_'.join(trialid.split('_')[:-1])\n    trialid = '_'.join(trialid.split('_')[1:])\n\n    print('working on ' + trialid)\n\n    # load it\n    id_df = pd.read_csv(sto, sep='\\t', skiprows=6)\n\n    # pad 0 ms row\n    padrow = id_df.iloc[0].copy()\n    padrow['time'] = 0\n\n    # concatenate it to the beginning of id_df\n    id_df = pd.concat([pd.DataFrame(padrow).T, id_df], ignore_index=True)\n\n    ##### SMOOTHING #####\n\n    # smooth all columns except the firts time (time) and last (trialid)\n    colstosmooth = [x for x in id_df.columns if 'time' not in x]\n    colstosmooth = [x for x in colstosmooth if 'TrialID' not in x]\n\n    # smooth\n    for col in colstosmooth:\n        id_df[col] = scipy.signal.savgol_filter(id_df[col], 35, 1)\n\n    ##### AGGREGATING #####\n\n    # get aggregated euclidian sum for each joint group\n    id_df = aggregate_keypoints(id_df, 'moment', '_moment_sum', 'angles')\n\n    #### TORQUE CHANGE #####\n\n    # for each moment col, we will also calculate the change \n    torquestodiff = [x for x in id_df.columns if 'moment' in x]\n\n    for col in torquestodiff:\n        torquechange = np.abs(np.insert(np.diff(id_df[col]), 0, 0))\n        torquechange_smoothed = scipy.signal.savgol_filter(torquechange, 35, 1)\n        # new data\n        new_data = pd.DataFrame({col + '_change': torquechange_smoothed})\n        id_df = pd.concat([id_df, new_data], axis=1)\n    \n    # convert time to ms\n    id_df['time'] = id_df['time']*1000\n        # add trialid\n    id_df['TrialID'] = trialid\n\n    # write to csv\n    id_df.to_csv(MTfolder_processed + '/id_' + trialid + '.csv', index=False)\n\nHere is an example file\n\n\n\n\n\n\n\n\n\n\ntime\npelvis_tilt_moment\npelvis_list_moment\npelvis_rotation_moment\npelvis_tx_force\npelvis_ty_force\npelvis_tz_force\nhip_flexion_r_moment\nhip_adduction_r_moment\nhip_rotation_r_moment\n...\nwrist_dev_r_moment_change\nwrist_flex_l_moment_change\nwrist_dev_l_moment_change\nlowerbody_moment_sum_change\nleg_moment_sum_change\nhead_moment_sum_change\narm_moment_sum_change\npelvis_moment_sum_change\nspine_moment_sum_change\nTrialID\n\n\n\n\n0\n0.000\n0.491035\n55.218816\n1.864887\n8.991912\n613.897817\n8.243875\n-42.179898\n0.803486\n1.159189\n...\n-0.000400\n-0.000851\n0.001524\n0.251383\n0.003900\n0.047152\n-0.023827\n0.255015\n-0.017313\n0_2_96_p1\n\n\n1\n16.547\n0.568692\n54.981085\n1.960780\n8.479003\n613.965803\n7.845832\n-42.124053\n0.736541\n1.167199\n...\n-0.000327\n-0.000700\n0.001662\n0.245041\n0.004395\n0.047129\n-0.020931\n0.248606\n-0.012716\n0_2_96_p1\n\n\n2\n33.213\n0.646350\n54.743353\n2.056672\n7.966094\n614.033789\n7.447790\n-42.068208\n0.669597\n1.175208\n...\n-0.000254\n-0.000549\n0.001800\n0.238700\n0.004890\n0.047106\n-0.018034\n0.242197\n-0.008119\n0_2_96_p1\n\n\n3\n49.879\n0.724007\n54.505622\n2.152564\n7.453185\n614.101775\n7.049747\n-42.012363\n0.602652\n1.183218\n...\n-0.000181\n-0.000397\n0.001938\n0.232359\n0.005385\n0.047083\n-0.015137\n0.235789\n-0.003522\n0_2_96_p1\n\n\n4\n66.545\n0.801664\n54.267891\n2.248457\n6.940276\n614.169761\n6.651704\n-41.956518\n0.535708\n1.191228\n...\n-0.000108\n-0.000246\n0.002076\n0.226018\n0.005880\n0.047061\n-0.012240\n0.229380\n0.001075\n0_2_96_p1\n\n\n5\n83.211\n0.879322\n54.030159\n2.344349\n6.427368\n614.237747\n6.253662\n-41.900673\n0.468764\n1.199237\n...\n-0.000035\n-0.000094\n0.002214\n0.219677\n0.006375\n0.047038\n-0.009343\n0.222971\n0.005673\n0_2_96_p1\n\n\n6\n99.877\n0.956979\n53.792428\n2.440241\n5.914459\n614.305733\n5.855619\n-41.844828\n0.401819\n1.207247\n...\n0.000038\n0.000057\n0.002351\n0.213336\n0.006870\n0.047015\n-0.006446\n0.216562\n0.010270\n0_2_96_p1\n\n\n7\n116.543\n1.034636\n53.554696\n2.536133\n5.401550\n614.373719\n5.457576\n-41.788983\n0.334875\n1.215257\n...\n0.000111\n0.000209\n0.002489\n0.206995\n0.007365\n0.046993\n-0.003549\n0.210153\n0.014867\n0_2_96_p1\n\n\n8\n133.209\n1.112294\n53.316965\n2.632026\n4.888641\n614.441705\n5.059533\n-41.733138\n0.267930\n1.223266\n...\n0.000184\n0.000360\n0.002627\n0.200654\n0.007860\n0.046970\n-0.000653\n0.203744\n0.019464\n0_2_96_p1\n\n\n9\n149.875\n1.189951\n53.079233\n2.727918\n4.375732\n614.509691\n4.661491\n-41.677293\n0.200986\n1.231276\n...\n0.000257\n0.000512\n0.002765\n0.194313\n0.008355\n0.046947\n0.002244\n0.197335\n0.024061\n0_2_96_p1\n\n\n10\n166.541\n1.267609\n52.841502\n2.823810\n3.862823\n614.577677\n4.263448\n-41.621448\n0.134041\n1.239286\n...\n0.000330\n0.000663\n0.002903\n0.187972\n0.008850\n0.046924\n0.005141\n0.190927\n0.028658\n0_2_96_p1\n\n\n11\n183.207\n1.345266\n52.603770\n2.919703\n3.349915\n614.645663\n3.865405\n-41.565603\n0.067097\n1.247295\n...\n0.000403\n0.000815\n0.003041\n0.181630\n0.009345\n0.046902\n0.008038\n0.184518\n0.033256\n0_2_96_p1\n\n\n12\n199.873\n1.422923\n52.366039\n3.015595\n2.837006\n614.713649\n3.467363\n-41.509758\n0.000152\n1.255305\n...\n0.000476\n0.000966\n0.003179\n0.175289\n0.009840\n0.046879\n0.010935\n0.178109\n0.037853\n0_2_96_p1\n\n\n13\n216.539\n1.500581\n52.128307\n3.111487\n2.324097\n614.781635\n3.069320\n-41.453913\n-0.066792\n1.263315\n...\n0.000549\n0.001118\n0.003317\n0.168948\n0.010335\n0.046856\n0.013832\n0.171700\n0.042450\n0_2_96_p1\n\n\n14\n233.205\n1.578238\n51.890576\n3.207380\n1.811188\n614.849621\n2.671277\n-41.398068\n-0.133737\n1.271324\n...\n0.000622\n0.001269\n0.003455\n0.162607\n0.010830\n0.046833\n0.016729\n0.165291\n0.047047\n0_2_96_p1\n\n\n\n\n15 rows × 131 columns\n\n\n\n\nNow we can check by ploting the joint moment change against kinematic acceleration\n\n\n\n\n\n\n\n\n\n\n\nBalance Board (Ground reaction forces) - processing\nLastly, we need to process the balance board data. We apply 5th order Savitzky-Golay filter to windows of 102 ms. To have a measure for postural adjustments, we compute the change in 2D magnitude (L2 norm of the center of pressure x and y) in center of pressure. The code is adaptation from Pouw et al. (2023).\n\nBB_files = glob.glob(BBfolder + '*BalanceBoard*.csv', recursive=True)\n\nfor bb in BB_files:\n    # get trialid\n    trialid = bb.split('\\\\')[-1].split('.')[0]\n    # get the first, second, fourth, nineth elements\n    trialid = '_'.join(trialid.split('_')[:2] + trialid.split('_')[3:4] + trialid.split('_')[8:9])\n\n    print('working on ' + trialid)\n\n    # because we are going to merge on bb, we will store also more information\n    fileinfo = bb.split('\\\\')[-1].split('.')[0]\n\n    # if second element is 1, we will store last three elements\n    if fileinfo.split('_')[1] == '1':\n        # if there is not 'corrected' in the name, we will store last three elements\n        if 'corrected' not in fileinfo:\n            info = '_'.join(fileinfo.split('_')[-3:])\n        else:\n            info = '_'.join(fileinfo.split('_')[-4:])\n    elif fileinfo.split('_')[1] == '2':\n        # otherwise we store last four elements (5 when corrected)\n        if 'corrected' not in fileinfo:\n            info = '_'.join(fileinfo.split('_')[-4:])\n        else:\n            info = '_'.join(fileinfo.split('_')[-5:])\n\n    # Load the balanceboard data\n    df_bb = pd.read_csv(bb)\n\n    # Rename columns\n    df_bb.columns = ['time_s', 'left_back', 'right_forward', 'right_back', 'left_forward']\n\n    # Calculate sampling rate\n    bbsamp = 1 / np.mean(np.diff(df_bb['time_s'] - min(df_bb['time_s'])))\n\n    # Apply Savitzky-Golay filter to smooth the data\n    for col in df_bb.columns[1:]:\n        df_bb[col] = scipy.signal.savgol_filter(df_bb[col], 51, 5) # window of 102 ms\n\n    # Calculate COPX and COPY\n    COPX = (df_bb['right_forward'] + df_bb['right_back']) - (df_bb['left_forward'] + df_bb['left_back'])\n    COPY = (df_bb['right_forward'] + df_bb['left_forward']) - (df_bb['left_back'] + df_bb['right_back'])\n\n    # Calculate COPXc and COPYc \n    df_bb['COPXc'] = scipy.signal.savgol_filter(np.insert(np.diff(COPX), 0, 0), 51, 5) \n    df_bb['COPYc'] = scipy.signal.savgol_filter(np.insert(np.diff(COPY), 0, 0), 51, 5)\n\n    # Calculate COPc\n    df_bb['COPc'] = np.sqrt(df_bb['COPXc']**2 + df_bb['COPYc']**2)\n\n    # restart the time so that starts from 0\n    df_bb['time_s'] = df_bb['time_s'] - min(df_bb['time_s'])\n    # convert to ms\n    df_bb['time_s'] = df_bb['time_s']*1000\n\n    # rename time_s to time\n    df_bb.rename(columns={'time_s': 'time'}, inplace=True)\n\n    # Add trialid\n    df_bb['TrialID'] = trialid\n    # Add info\n    df_bb['FileInfo'] = info\n\n    # Write as csv to MTfolder_processed\n    df_bb.to_csv(MTfolder_processed + '/bb_' + trialid + '.csv', index=False)\n\nHere is an example of a file\n\n\n\n\n\n\n\n\n\n\ntime\nleft_back\nright_forward\nright_back\nleft_forward\nCOPXc\nCOPYc\nCOPc\nTrialID\nFileInfo\n\n\n\n\n0\n0.000000\n1.171571\n0.723754\n1.447923\n1.408978\n-0.000137\n-0.000089\n0.000164\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n1\n2.000053\n1.170970\n0.722923\n1.446523\n1.407724\n-0.000254\n-0.000015\n0.000255\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n2\n4.000105\n1.170294\n0.722047\n1.445160\n1.406567\n-0.000338\n0.000051\n0.000342\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n3\n6.000158\n1.169555\n0.721139\n1.443832\n1.405492\n-0.000394\n0.000109\n0.000408\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n4\n8.000211\n1.168764\n0.720209\n1.442540\n1.404488\n-0.000425\n0.000159\n0.000454\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n5\n10.000264\n1.167931\n0.719268\n1.441284\n1.403544\n-0.000437\n0.000201\n0.000481\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n6\n12.000316\n1.167069\n0.718325\n1.440065\n1.402653\n-0.000432\n0.000236\n0.000492\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n7\n14.000369\n1.166187\n0.717390\n1.438886\n1.401807\n-0.000413\n0.000264\n0.000491\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n8\n16.000422\n1.165295\n0.716471\n1.437750\n1.401002\n-0.000385\n0.000286\n0.000480\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n9\n18.000475\n1.164405\n0.715577\n1.436658\n1.400234\n-0.000349\n0.000302\n0.000461\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n10\n20.000527\n1.163525\n0.714716\n1.435614\n1.399501\n-0.000307\n0.000311\n0.000437\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n11\n22.000580\n1.162665\n0.713894\n1.434623\n1.398801\n-0.000262\n0.000315\n0.000410\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n12\n24.000633\n1.161834\n0.713118\n1.433687\n1.398136\n-0.000216\n0.000314\n0.000381\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n13\n26.000685\n1.161041\n0.712394\n1.432812\n1.397506\n-0.000170\n0.000307\n0.000351\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n14\n28.000738\n1.160293\n0.711729\n1.432002\n1.396912\n-0.000125\n0.000296\n0.000321\n0_2_20_p1\np1_glimlach_combinatie_c1\n\n\n\n\n\n\n\n\nHere is an example of a timeseries representing change in center of pressure (COPc)\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nPouw, Wim, Raphael Werner, Lara Burchardt, and Luc Selen. 2023. “The Human Voice Aligns with Whole-Body Kinetics.” November 28, 2023. https://doi.org/10.1101/2023.11.28.568991.\n\n\nSeth, Ajay, Jennifer L. Hicks, Thomas K. Uchida, Ayman Habib, Christopher L. Dembia, James J. Dunne, Carmichael F. Ong, et al. 26. 7. 2018. “OpenSim: Simulating Musculoskeletal Dynamics and Neuromuscular Control to Study Human and Animal Movement.” PLOS Computational Biology 14 (7): e1006223. https://doi.org/10.1371/journal.pcbi.1006223.",
    "crumbs": [
      "Processing",
      "Processing I: Motion tracking and balance"
    ]
  },
  {
    "objectID": "02_MotionTracking_processing/03_Track_pose2sim.html",
    "href": "02_MotionTracking_processing/03_Track_pose2sim.html",
    "title": "Motion tracking III: Triangulation via Pose2sim",
    "section": "",
    "text": "Overview\nIn the previous script, we have extracted 2D keypoints for each trial (per each video, 3 in total). In this script, we will use Pose2sim (Pagnon, Domalain, and Reveret (2022)) to calibrate the 3 cameras present in the lab setup, and triangulate the 3D position of the keypoints.\nDemo of this pipeline has been published on EnvisionBOX\nFor more information on Pose2sim, please refer to the Pose2sim documentation\n\n\nCode to prepare the environment\nfrom Pose2Sim import Pose2Sim\nimport os\nimport glob\nimport pandas as pd\nfrom trc import TRCData\nimport pandas as pd\nimport shutil\nimport cv2\nimport numpy as np\nimport toml\n\ncurfolder = os.getcwd()\n\n# Here is our config.file\npose2simprjfolder = curfolder + '\\Pose2Sim\\Empty_project_FLESH_settings\\\\'\n\n# Here we store the data\ninputfolder = curfolder + '\\projectdata\\\\'\nfolderstotrack = glob.glob(curfolder+'\\\\projectdata\\*')\n#print(folderstotrack)\n\n# Here we store mass information (weight, height) about participants\nMETA = pd.read_csv(curfolder + '\\\\..\\\\00_RAWDATA\\META_mass.txt', sep='\\t') # Note that we actually need the weight only if we do the marker augmentation\n\n# Initiate empty list\npcnfolders = []\n\n# Get all the folders per session, per participant\nfor i in folderstotrack:\n    pcn1folders = glob.glob(i + '/P0/*')\n    pcn2folders = glob.glob(i + '/P1/*')\n    pcnfolders_in_session = pcn1folders + pcn2folders\n\n    pcnfolders = pcnfolders + pcnfolders_in_session\n\n\n# Get rid of all pontetially confusing files/folders\npcnfolders = [x for x in pcnfolders if 'Config' not in x]\npcnfolders = [x for x in pcnfolders if 'opensim' not in x]\npcnfolders = [x for x in pcnfolders if 'xml' not in x]\npcnfolders = [x for x in pcnfolders if 'ResultsInverseDynamics' not in x]\npcnfolders = [x for x in pcnfolders if 'ResultsInverseKinematics' not in x]\npcnfolders = [x for x in pcnfolders if 'sto' not in x]\npcnfolders = [x for x in pcnfolders if 'txt' not in x]\nprint(pcnfolders[0:10])\n\n\n\n\nCustom functions\ndef load_toml(file_path):\n    with open(file_path, 'r') as file:\n        return toml.load(file)\n\ndef save_toml(data, file_path):\n    with open(file_path, 'w') as file:\n        toml.dump(data, file)\n\ndef update_participant_info(toml_data, height, mass):\n    if 'markerAugmentation' in toml_data:\n        toml_data['markerAugmentation']['participant_height'] = height\n        toml_data['markerAugmentation']['participant_mass'] = mass\n    else:\n        raise KeyError(\"The key 'markerAugmentation' is not present in the TOML data.\")\n    return toml_data\n\ndef saveFrame_fromVideo(framepick, output_dir, input_video):    \n\n    cap = cv2.VideoCapture(input_video)\n            \n    # check if the video file was opened successfully\n    if not cap.isOpened():\n        print(\"Error: Couldn't open the video file.\")\n        exit()\n    \n               \n    frame_count = 0\n    while True:\n    # read the next frame\n        ret, frame = cap.read()\n        if not ret:\n            break  # break the loop if we reach the end of the video\n            \n        frame_count += 1\n\n        # save every n-th frame\n        if frame_count % framepick == 0:\n            frame_filename = f\"{output_dir}frame_{frame_count}.png\"\n            cv2.imwrite(frame_filename, frame, [cv2.IMWRITE_PNG_COMPRESSION, 0])\n\n    # release the video capture object and close the video file\n    cap.release()\n    cv2.destroyAllWindows()\n\n\n\n\nPose2sim triangulation\nThe Pose2sim pipeline comes in three steps: - calibration - triangulation - filtering\nIn calibration, we will use the calibration videos with checkerboard to calibrate the intrinsic and extrinsic parameters of the cameras. Note that we calibrate intrinsic parameters only once, and copy the file to the rest of the sessions. Extrinsic parameters are calibrated for each session (in part 1, and copied into part 2)\nAs noted in the Pose2sim documentation, intrinsic error should be below 0.5 pixels, and extrinsic error should be below 1 cm (but acceptable until 2.5 cm)\nNote that extrinsic are sometimes not automatically detected so the corners need to be annotated manually.\nIn triangulation, we will use the keypoints extracted in the previous script to triangulate the 3D position of the keypoints. The output will be a 3D position for each keypoint in each frame.\nIn filtering, we will filter the 3D position of the keypoints to remove noise and outliers with the in-build Butterworth filter (order 4, cut-off frequency 10 Hz).\nRefer to the Config.toml file in ’2Sim_project_FLESH_settings\\’ for the configuration of the pipeline.\nThere are additional three steps available in Pose2sim that we will not utilize in this script - synchronization, person association, and marker augmentation.\n\n# Set framerate\nframerate = 60\n\n# How many x-th frame do we extract from the calibration video? \nframepick = 3\n\n# Copy a folder in pose2simprjfolder and its contents to folders\nsource1 = pose2simprjfolder+'/Config.toml'\nsource2 = pose2simprjfolder+'/opensim/'\n\n\nfor i in folderstotrack:\n    os.chdir(i)\n\n    sessionID = i.split('\\\\')[-1].split('_')[1]\n\n    # First we need to prepare Config.file to all levels of folders (plus opensim to P0 and P1)\n\n    # Copy to session folder\n    shutil.copy(source1, i + '/')\n\n    input_toml = load_toml(i+'/Config.toml')\n\n    # Update the p0 info\n    mass_p0 = META.loc[(META['session'] == int(sessionID)) & (META['pcn'] == 'p0'), 'weight'].values[0]\n    height_p0 = META.loc[(META['session'] == int(sessionID)) & (META['pcn'] == 'p0'), 'height'].values[0]\n    updated_toml_p0 = update_participant_info(input_toml, height_p0, mass_p0)\n\n    # Update p1 info\n    mass_p1 = META.loc[(META['session'] == int(sessionID)) & (META['pcn'] == 'p1'), 'weight'].values[0]\n    height_p1 = META.loc[(META['session'] == int(sessionID)) & (META['pcn'] == 'p1'), 'height'].values[0]\n    updated_toml_p1 = update_participant_info(input_toml, height_p1, mass_p1)\n    \n    # Save the updated TOML data\n    save_toml(updated_toml_p0, i+'/P0/Config.toml')\n    save_toml(updated_toml_p1, i+'/P1/Config.toml')\n\n    p0_source = i+'/P0/Config.toml'\n    p1_source = i+'/P1/Config.toml'\n\n    # Copy necessary files \n    for j in pcnfolders:\n        if 'P0' in j:\n            shutil.copy(p0_source, j + '/')\n            print('source = ' + source1 + ' to destination: ' + j+'/')\n\n        if 'P1' in j:\n            shutil.copy(p1_source, j + '/')\n            print('source = ' + source1 + ' to destination: ' + j+'/')\n\n    if not os.path.exists(i+'/P0/opensim/'):\n        shutil.copytree(source2, i+'/P0/opensim/')\n        print('source = ' + source2 + ' to destination: ' + i+'/P0/opensim/')\n\n    if not os.path.exists(i+'/P1/opensim/'):\n        shutil.copytree(source2, i+'/P1/opensim/')\n        print('source = ' + source2 + ' to destination: ' + i+'/P1/opensim/')\n\n    # Now we calibrate\n    print('Step: Calibration')\n\n    # Calibrate only if there is no toml file in the calibration folder\n    if not os.path.exists(i+'/calibration/Calib_board.toml'):\n        print('Calibration file not found')\n        \n        # Now we prepare images from calibration videos\n        calib_folders = glob.glob(i+'/calibration/*/*')\n\n        for c in calib_folders:\n            print(c)\n            split = c.split(os.path.sep)\n            camIndex = split[-1]\n            # Extrinsic calibration\n            if 'extrinsics' in c:\n                input_video = c+'/'+ sessionID +'_checker_extrinsics_'+camIndex+'.avi' \n            # Intrinsic\n            else:\n                input_video = c+'/'+ sessionID +'_checker_intrinsics_'+camIndex+'.avi'\n\n            output_dir = c + '/'\n            \n            print('We are now saving frames extracted from calibration videos')\n            saveFrame_fromVideo(framepick, output_dir, input_video)\n    \n        print('Calibration file does not exist, calibrating...')\n        Pose2Sim.calibration() \n\n        # Get the last element of the i\n        split = i.split(os.path.sep)\n        parts = split[-1].split('_')\n        # Get the sessionID\n        session_id = parts[1]\n        session_part = parts[-1]\n\n        # If session_part is 1, we copy trc and calib file to the session that has some id, but part 2\n        if session_part == '1':\n            # Copy the calibration file to the session with the same id, but part 2\n            copy_to_part = '2'\n            # Get the new folder name\n            new_folder = 'Session_'+session_id+'_'+copy_to_part\n            # Get the new folder path\n            new_folder_path = inputfolder + new_folder\n            # In new_folder_path, create folder calibration if it doesn't exist\n            if not os.path.exists(new_folder_path+'\\\\calibration\\\\'):\n                os.makedirs(new_folder_path+'\\\\calibration\\\\')\n            \n            # Get the calibration file path\n            calib_file = i + '/calibration/Calib_board.toml'\n            # Get the trc file path\n            trc_file = i + '/calibration/Object_points.trc'\n            \n            # Copy the files to the new folder\n            shutil.copy(calib_file, new_folder_path + '/calibration/')\n            shutil.copy(trc_file, new_folder_path + '/calibration/')\n        \n        # Part 2 does not need to be calibrated so we can just proceed\n        else:\n            continue\n\n    # If calibration file exists, then we can skip calibration\n    else:\n        print('Calibration file found, no need to calibrate')\n    \n    # Camera synchronization (our cameras are natively synchronized so we do not need this step)\n    #print('Step: synchronization')\n    #Pose2Sim.synchronization()\n\n    # Person association if there is more people in a video\n    #print('Step: person association')\n    #Pose2Sim.personAssociation()\n\n    # Prepare special log txt\n    error_log = curfolder + '/error_log.txt'\n\n    try:\n        print('Step: triangulation')\n        Pose2Sim.triangulation()\n    except:\n        print('Triangulation failed')\n        with open(error_log, 'a') as f:\n            f.write(f'Triangulation failed for {j}\\n')\n\n        continue\n\n    try:\n        print('Step: filtering')\n        Pose2Sim.filtering()\n    except:\n        print('Filtering failed')\n        # Print the folder\n        with open(error_log, 'a') as f:\n            f.write(f'Filtering failed for {j}\\n')\n\n        continue\n\n    # Marker augmentation (note that this works only with model 25)\n    #print('Step: marker augmentation')\n    #Pose2Sim.markerAugmentation()\n\n\n\n\nConverting trc files to csv\nNote that all output errors per each trial are saved in logs.txt file in projectdata/Session_x\nBecause the output files are in .trc format, we also want to convert them to .csv to have more convenient format for later processing\n\ntrctoconvert = []\n\nfor j in pcnfolders:\n    # Here we store the 3D pose data\n    posefolder = '/pose-3d/'\n    # Check any .trc files in the folder\n    trcfiles = glob.glob(j+posefolder + '*.trc')\n    #print(trcfiles)\n    \n    # Append\n    trctoconvert = trctoconvert + trcfiles\n\n# Loop through files and convert to csv\nfor file in trctoconvert:\n    print(file)\n    # There is a mistake in LSTM files formatting (those are output of marker augmentation), se we want to skip them\n    if 'LSTM' not in file:\n        mocap_data = TRCData()\n        mocap_data.load(os.path.abspath(file))\n        num_frames = mocap_data['NumFrames']\n        markernames = mocap_data['Markers'] # the marker names are not\n\n        # Convert mocap_data to pandas dataframe\n        mocap_data_df = pd.DataFrame(mocap_data, columns=mocap_data['Markers'])\n        # Each value within the dataframe consists a list of x,y,z coordinates, we want to seperate these out so that each marker and dimension has its own column\n        colnames = []\n        for marker in markernames:\n            colnames.append(marker + '_x')\n            colnames.append(marker + '_y')\n            colnames.append(marker + '_z')\n\n        # Create a new DataFrame to store separated values\n        new_df = pd.DataFrame()\n\n        # Iterate through each column in the original DataFrame\n        for column in mocap_data_df.columns:\n            # Extract the x, y, z values from each cell\n            xyz = mocap_data_df[column].tolist()\n            # Create a new DataFrame with the values in the cell separated into their own columns\n            xyz_df = pd.DataFrame(xyz, columns=[column + '_x', column + '_y', column + '_z'])\n            # Add the new columns to the new DataFrame\n            new_df = pd.concat([new_df, xyz_df], axis=1)\n\n        # Add a new time column to the new dataframe assuming the framerate was 60 fps\n        time = []\n        ts = 0\n        for i in range(0, int(num_frames)):\n            ts = ts + 1/framerate\n            time.append(ts)\n\n        # Add the time column to the new dataframe\n        new_df['Time'] = time\n\n        # Write pd dataframe to csv\n        new_df.to_csv(file+'.csv', index=False)\n\n    else:\n        continue\n\nHere is an animation of the keypoints plotted in 3D space.\n\n\n\n \n Your browser does not support the video tag.\n \n\n\nNote that here we see that the Y and Z axis are flipped. We will therefore need to adjust the columns in motion processing script.\nThis is the raw video that it corresponds to\n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\nInspecting measurement error\nNow let’s get information about the intrinsic and extrinsic error for each trial. The error log is saved in logs.txt file in the first session of the list that we have processed, here Session_0_1. To get more comprehensive overview of the error for each file, we will extract the relevant information for each trial by acessing regular expressions in the logs.txt file.\n\nimport re\n\nlog = curfolder + '/projectdata/Session_0_1/logs.txt'\n\n# Function to load text file\ndef load_log_file(filename):\n    with open(filename, 'r') as file:\n        text = file.read()\n    return text\n\n# Sample file path (replace with the actual path to your log file)\nfile_path = log  # Replace with your actual file path\n\n# Load the log file text\nlog_data = load_log_file(file_path)\n\n# Pattern to extract trials (trial name, date, triangulation, mean reprojection error, and camera exclusions)\ntrial_pattern = r\"(Triangulation of 2D points for (\\S+).*?11. December.*?Mean reprojection error for all points on all frames is (\\d+\\.\\d+) px.*?which roughly corresponds to (\\d+\\.\\d+) mm.*?Camera (cam\\d) was excluded (\\d+)%.*?)(?=Triangulation|$)\"\n\n# Find all trials for 11 December\ntrial_chunks = re.findall(trial_pattern, log_data, flags=re.DOTALL)\n\n# Store results\ntrial_results = []\n\n# Process each trial\nfor trial_chunk in trial_chunks:\n    full_trial_info = trial_chunk[0]  # Entire chunk related to a trial\n    trial_name = trial_chunk[1]  # Extract trial name (e.g., 0_2_38_p0)\n    mean_reprojection_error_px = trial_chunk[2]  # Mean reprojection error in px\n    mean_reprojection_error_mm = trial_chunk[3]  # Mean reprojection error in mm\n    \n    # Add extracted information to results\n    trial_results.append({\n        'trial_name': trial_name,\n        'mean_reprojection_error_px': mean_reprojection_error_px,\n        'mean_reprojection_error_mm': mean_reprojection_error_mm\n    })\n\n# trial_results to df\ndf = pd.DataFrame(trial_results)\n\ndf.head(15)\n\n\n\n\n\n\n\n\n\ntrial_name\nmean_reprojection_error_px\nmean_reprojection_error_mm\n\n\n\n\n0\n0_1_tpose_p0,\n2.4\n10.6\n\n\n1\n0_1_4_p0,\n2.6\n11.5\n\n\n2\n0_1_20_p0,\n2.5\n11.0\n\n\n3\n0_1_21_p0,\n2.4\n10.6\n\n\n4\n0_1_22_p0,\n2.3\n10.2\n\n\n5\n0_1_23_p0,\n2.3\n10.2\n\n\n6\n0_1_24_p0,\n2.5\n11.0\n\n\n7\n0_1_25_p0,\n2.4\n10.6\n\n\n8\n0_1_26_p0,\n2.5\n11.0\n\n\n9\n0_1_36_p0,\n2.7\n11.9\n\n\n10\n0_1_37_p0,\n2.8\n12.4\n\n\n11\n0_1_38_p0,\n2.6\n11.5\n\n\n12\n0_1_39_p0,\n2.6\n11.5\n\n\n13\n0_1_40_p0,\n2.8\n12.4\n\n\n14\n0_1_41_p0,\n2.4\n10.6\n\n\n\n\n\n\n\n\nWhat is the mean reprojection error across trials in mm?\n\ndf['mean_reprojection_error_mm'] = df['mean_reprojection_error_mm'].astype(float)\nmean_repro = df['mean_reprojection_error_mm'].mean()\nprint('Mean reprojection error for all trials is ', mean_repro, ' mm')\n\nMean reprojection error for all trials is  12.284709480122325  mm\n\n\nIs there any trial that has an error above 2 cm?\n\ndf['mean_reprojection_error_mm'] = df['mean_reprojection_error_mm'].astype(float)\n\nif len(df[df['mean_reprojection_error_mm'] &gt; 20]) &gt; 0:\n    print('There are trials with reprojection error larger than 20 mm')\n    df[df['mean_reprojection_error_mm'] &gt; 20]\nelse:\n    print('There are no trials with reprojection error larger than 20 mm')\n\nThere are no trials with reprojection error larger than 20 mm\n\n\n\n\nReferences\n\n\n\n\nPagnon, David, Mathieu Domalain, and Lionel Reveret. 2022. “Pose2Sim: An End-to-End Workflow for 3D Markerless Sports Kinematics—Part 2: Accuracy.” Sensors 22 (7, 7): 2712. https://doi.org/10.3390/s22072712.",
    "crumbs": [
      "Processing",
      "Motion tracking III: Triangulation via Pose2sim"
    ]
  },
  {
    "objectID": "02_MotionTracking_processing/01_Video_preparation.html",
    "href": "02_MotionTracking_processing/01_Video_preparation.html",
    "title": "Motion tracking I: Preparation of videos",
    "section": "",
    "text": "Overview\nIn the previous script, we have prepared all the videos as trial-sized files that we can use for motion capture. However, during the experimental recording, we have concatenated the three cameras into one file. Now we need to cut these videos into three and prepare them into folders as OpenPose (and later Pose2sim) requires.\nWe will do the same for calibration videos.\n\n\nCode to setup the environment\nimport os\nimport cv2\nimport glob\nimport tempfile\nimport subprocess\nimport random\nfrom IPython.display import Video\n\n# Currect folder\ncurfolder = os.getcwd()\n\n# Videodata \nvideodata = curfolder + '\\\\..\\\\01_XDF_processing\\\\data\\\\Data_processed\\\\Data_trials'\n\n# If it doesn't exist, create folder projectdata\nif not os.path.exists(curfolder + '\\\\projectdata\\\\'):\n    os.makedirs(curfolder + '\\\\projectdata\\\\')\n\n# Refer to it  \noutputfolder = curfolder + '\\\\projectdata\\\\'\n\n# Load in the videos (avi)\nvideos = []\nfor file in os.listdir(videodata):\n    if file.endswith(\".avi\"):\n        videos.append(os.path.join(videodata, file))\n\nprint(videos[0:10])\n\n# Calibration videos are in rawdata folder\ncalibfolder = curfolder + '\\\\..\\\\00_RAWDATA\\\\'\n\n# Extrinsic calibration\nvideos_ex = glob.glob(calibfolder + '*\\\\*extrinsics.avi', recursive=True)\n\n# Intrinsic calibration\nvideo_in = glob.glob(calibfolder + '*\\\\*intrinsics.avi', recursive=True)\n\n\nFirst, we need to take care that we are not working with wrongly cut videos, but only with the corrected version (if it exists). From the list of videos, we will hence exclude all videos that have also corrected version in the list.\n\n# Get all corrected videos from the list\nvideos_corr = []\nfor file in videos:\n    if 'corrected' in file:\n        videos_corr.append(file)\n\n# Now get the name of this trial without the corrected part\nvideos_old = []\nfor file in videos_corr:\n    videos_old.append(file.replace('_corrected', ''))\n\n# From videos, remove trials that are in videos_old\nvideos = [x for x in videos if x not in videos_old]\n\nThis is how the video looks like when the cameras are still concatenated\n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\nFunction to split the videos into 3 camera views\ndef split_camera_views(input_file, output_files):\n    cap = cv2.VideoCapture(input_file)\n\n    # Divide the width by 3 to get each camera separately\n    num_cameras = 3\n    width_per_camera = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) // num_cameras\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n\n    # Create VideoWriters for each camera\n    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n    out_cam1 = cv2.VideoWriter(output_files[0], fourcc, frame_rate, (width_per_camera, height))\n    out_cam2 = cv2.VideoWriter(output_files[1], fourcc, frame_rate, (width_per_camera, height))\n    out_cam3 = cv2.VideoWriter(output_files[2], fourcc, frame_rate, (width_per_camera, height))\n\n    while True:\n        ret, frame = cap.read()\n\n        # Check if the frame is None (end of video)\n        if frame is None:\n            break\n\n        # Break the frame into three parts\n        camera1_frame = frame[:, :width_per_camera, :]\n        camera2_frame = frame[:, width_per_camera:2*width_per_camera, :]\n        camera3_frame = frame[:, 2*width_per_camera:, :]\n\n        # Display each camera view separately (optional)\n        cv2.imshow('Camera 1', camera1_frame)\n        cv2.imshow('Camera 2', camera2_frame)\n        cv2.imshow('Camera 3', camera3_frame)\n\n        # Write frames to video files\n        out_cam1.write(camera1_frame)\n        out_cam2.write(camera2_frame)\n        out_cam3.write(camera3_frame)\n\n        if cv2.waitKey(1) == 27:\n            break\n\n    # Release VideoWriters and VideoCapture\n    out_cam1.release()\n    out_cam2.release()\n    out_cam3.release()\n    cap.release()\n    cv2.destroyAllWindows()\n\n\n\n\nCutting trial videos\nWe will first start with the trial videos, as they require a bit different structuring into folders than the calibration videos. Each video-tryad will be saved into following structure /sessionID/pcnID/trialID/raw-2d with its original name and identificator of each camera (i.e., 1-3)\n\n# Loop over files in folder and split them\nfor file in videos:\n    print(\"working on file: \"+ file)\n\n    # Get the name of the file without the extension\n    filename = os.path.splitext(os.path.basename(file))[0]\n    \n    # Get trialID\n    # If it's a tpose, the name goes a bit differently than the rest\n    if 'tpose' in filename: \n        trialID = filename.split(\"_\")[0] + \"_\" + filename.split(\"_\")[1] + \"_\" + filename.split(\"_\")[2]+ \"_p\" + filename.split(\"_\")[3]\n    else:\n        # session, part, trial number and participant as trial ID\n        trialID = filename.split(\"_\")[0] + \"_\" + filename.split(\"_\")[1] + \"_\" + filename.split(\"_\")[3] + \"_\" + filename.split(\"_\")[4]\n\n    # Get sessionID\n    sessionID = 'Session' + '_' + filename.split(\"_\")[0] + \"_\" + filename.split(\"_\")[1]\n\n    # If a sessionID folder doesn't exist yet, create it\n    if not os.path.exists(os.path.join(outputfolder, sessionID)):\n        os.makedirs(os.path.join(outputfolder, sessionID))    \n\n    # Same for folders P0 and P1\n    if not os.path.exists(os.path.join(outputfolder, sessionID, 'P0')):\n        os.makedirs(os.path.join(outputfolder, sessionID, 'P0'))\n    if not os.path.exists(os.path.join(outputfolder, sessionID, 'P1')):\n        os.makedirs(os.path.join(outputfolder, sessionID, 'P1'))\n\n    # Now trialID folder within respective participant\n    if 'p0' in filename or 'tpose_0' in filename:\n        try:\n            os.makedirs(os.path.join(outputfolder, sessionID, 'P0', trialID))\n            # Inside this folder, create empty folder 'raw-2d'\n            os.makedirs(os.path.join(outputfolder, sessionID, 'P0', trialID, 'raw-2d'))\n        except FileExistsError:\n            continue\n\n        # This is how the final video is named\n        output_files = [\n            os.path.join(outputfolder, sessionID, 'P0', trialID, 'raw-2d', filename + '_cam1.avi'),\n            os.path.join(outputfolder, sessionID, 'P0', trialID, 'raw-2d', filename + '_cam2.avi'),\n            os.path.join(outputfolder, sessionID, 'P0', trialID, 'raw-2d', filename + '_cam3.avi')\n        ]\n\n    elif 'p1' in filename or 'tpose_1' in filename:\n        try:\n            os.makedirs(os.path.join(outputfolder, sessionID, 'P1', trialID))\n            os.makedirs(os.path.join(outputfolder, sessionID, 'P1', trialID, 'raw-2d'))\n        except FileExistsError:\n            continue    \n\n        # This is how the final video is named\n        output_files = [\n            os.path.join(outputfolder, sessionID, 'P1', trialID, 'raw-2d', filename + '_cam1.avi'),\n            os.path.join(outputfolder, sessionID, 'P1', trialID, 'raw-2d', filename + '_cam2.avi'),\n            os.path.join(outputfolder, sessionID, 'P1', trialID, 'raw-2d', filename + '_cam3.avi')\n        ]\n\n    else:\n        try:\n            os.makedirs(os.path.join(outputfolder, sessionID, trialID))\n            os.makedirs(os.path.join(outputfolder, sessionID, trialID, 'raw-2d'))\n        except FileExistsError:\n            continue\n\n        output_files = [\n            os.path.join(outputfolder, sessionID, trialID, 'raw-2d', filename + '_cam1.avi'),\n            os.path.join(outputfolder, sessionID, trialID, 'raw-2d', filename + '_cam2.avi'),\n            os.path.join(outputfolder, sessionID, trialID, 'raw-2d', filename + '_cam3.avi')]\n\n    # Split the camera views\n    split_camera_views(file, output_files)\n\n\n\n\nCutting calibration videos\nNow we also need to cut the video for calibration.\nIn the very beginning, we need to calibrate intrinsic parameters of the cameras (e.g., focal length). This needs to be done only once, and for that purpose we have special calibration video that is optimized for a low intrinsic error. For extrinsic parameters, we have calibration per each session.\n\n# Loop over files in folder and split them\nfor file in video_in:\n    # Get the name of the file without the extension\n    filename = os.path.splitext(os.path.basename(file))[0]\n    \n    # sessionID\n    sessionID = filename.split(\"_\")[0]\n    # note that we save it only to session 0_1 because intrinsic calibration needs to be done only once\n    sessionID = 'Session_' + sessionID + \"_1\" \n\n    # Inside this folder, create empty folder 'calibration'\n    if not os.path.exists(os.path.join(outputfolder, sessionID, 'calibration')):\n        os.makedirs(os.path.join(outputfolder, sessionID, 'calibration'))\n    # Inside, make three folders: cam1, cam2, cam3\n    os.makedirs(os.path.join(outputfolder, sessionID, 'calibration', 'intrinsics', 'cam1'))\n    os.makedirs(os.path.join(outputfolder, sessionID, 'calibration', 'intrinsics','cam2'))\n    os.makedirs(os.path.join(outputfolder, sessionID, 'calibration', 'intrinsics','cam3'))\n    \n    # Create the output file names\n    output_files = [\n        os.path.join(outputfolder, sessionID, 'calibration', 'intrinsics','cam1', filename + '_cam1.avi'),\n        os.path.join(outputfolder, sessionID, 'calibration', 'intrinsics','cam2', filename + '_cam2.avi'),\n        os.path.join(outputfolder, sessionID, 'calibration', 'intrinsics','cam3', filename + '_cam3.avi')\n    ]\n    \n    # Split the camera views\n    split_camera_views(file, output_files)\n\nNow we can also cut the video for extrinsic calibration\n\n# Loop over files in folder and split them\nfor file in videos_ex:\n    # Get the name of the file without the extension\n    filename = os.path.splitext(os.path.basename(file))[0]\n\n    sessionID = filename.split(\"_\")[0]\n    # Note that we save it only to session x_1 because then we will just copy the finished calibratio toml file\n    sessionID = 'Session_' + sessionID + \"_1\" \n\n    # Inside this folder, create empty folder 'calibration' \n    if not os.path.exists(os.path.join(outputfolder, sessionID, 'calibration')):\n        os.makedirs(os.path.join(outputfolder, sessionID, 'calibration'))\n\n    # Inside, make three folders: cam1, cam2, cam3\n    os.makedirs(os.path.join(outputfolder, sessionID, 'calibration', 'extrinsics', 'cam1'))\n    os.makedirs(os.path.join(outputfolder, sessionID, 'calibration', 'extrinsics','cam2'))\n    os.makedirs(os.path.join(outputfolder, sessionID, 'calibration', 'extrinsics','cam3'))\n    \n    # Create the output file names\n    output_files = [\n        os.path.join(outputfolder, sessionID, 'calibration', 'extrinsics','cam1', filename + '_cam1.avi'),\n        os.path.join(outputfolder, sessionID, 'calibration', 'extrinsics','cam2', filename + '_cam2.avi'),\n        os.path.join(outputfolder, sessionID, 'calibration', 'extrinsics','cam3', filename + '_cam3.avi')\n    ]\n    \n    # Split the camera views\n    split_camera_views(file, output_files)\n\nNow we are ready to proceed to motion tracking with OpenPose.",
    "crumbs": [
      "Processing",
      "Motion tracking I: Preparation of videos"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "",
    "text": "In this script, we will be modelling the causal relation between effort and correction to confirm/reject our hypothesis.\nThese are:\nH1: In corrections, people enhance some effort-related kinematic and/or acoustic features of their behaviour relative to the baseline.\nH2: The enhancement depends on similarity of the guesser’s answer and the original meaning. More similar answer will require/result in smaller enhancement (but still enhancement) than less similar answer.\nTo assess the design and performance of the model, we will use synthetic data that we create to have certain interdependencies and where the effects have pre-defined values. We use this approach instead of using our dyad 0 data, because the pilot data do not include enough data to have a sensible testing sample.\nThe confirmatory analysis concerns six outcome variables, namely integral of torque change, integral of amplitude envelope, integral of change in center of pressure, and mean peak value of torque change, mean peak value of amplitude envelope, and mean peak value of change in center of pressure.\nFor our exploratory analysis, we will model variables that will result as most predictive in relation to communicative attempt, assessed by XGBoost modelling. This is why in our synthetic data, we create a variable Effort that is continuous, but oterwise free of any other assumptions.\n\n\nCode to prepare the environment\nlibrary(here)\nlibrary(dplyr) # for data-wrangling\nlibrary(tidyr)  # for reshaping data (if needed)\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(rcartocolor)\nlibrary(patchwork)\n\nlibrary(ggdag) # for dag\nlibrary(dagitty)\n\nlibrary(bayesplot) # bayesian stuff\nlibrary(brms)\nlibrary(beepr)\nlibrary(bayestestR)\nlibrary(tidyverse)\n\nlibrary(splines) # for bsplines\nlibrary(emmeans) # for lognormal transformations\n\n\n# current folder (first go to session -&gt; set working directory -&gt; to source file location)\ncurfolder &lt;- getwd()\nparentfolder &lt;- dirname(curfolder)\n\nfeaturefolder &lt;- paste0(parentfolder, '/07_TS_featureExtraction/Datasets/')\ndemofolder    &lt;- paste0(parentfolder, '/00_RAWDATA/Demographics_all/')\ndatasets      &lt;- paste0(curfolder, '/datasets/')\nmodels        &lt;- paste0(curfolder, '/models/')\nplots         &lt;- paste0(curfolder, '/plots/')",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#data-wrangling",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#data-wrangling",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Data wrangling",
    "text": "Data wrangling\nTo be able to interpret the data, we will first need to do some data wrangling\nConvert columns to factors\n\nfinal_data$CommAtt &lt;- as.factor(final_data$CommAtt)\nfinal_data$Modality &lt;- as.factor(final_data$Modality)\nfinal_data$Participant &lt;- as.factor(final_data$Participant)\nfinal_data$Concept &lt;- as.factor(final_data$Concept)\n\nfinal_data$TrialNumber &lt;- as.numeric(final_data$TrialNumber)  # Ensure TrialNumber is numeric\n\nContrast-coding of categorical variables\n\ncontrasts(final_data$CommAtt) &lt;- MASS::contr.sdif(3)\ncontrasts(final_data$Modality) &lt;- contr.sum(3)/2\n\n\n\n\n\n\n\nNote\n\n\n\nThis is how CommAtt is contrast-coded now\n2-1 3-2\n1 -0.6666667 -0.3333333 2 0.3333333 -0.3333333 3 0.3333333 0.6666667\nThis is how modality is cc-ed\n[,1] [,2]\ncombined 0.5 0.0 gesture 0.0 0.5 vocal -0.5 -0.5\n\n\nCentering continuous variables\n\nfinal_data$TrialNumber_c &lt;- final_data$TrialNumber - median(range(final_data$TrialNumber))\nfinal_data$Familiarity &lt;- final_data$Familiarity - median(range(final_data$Familiarity))\nfinal_data$Big5 &lt;- final_data$Big5 - median(range(final_data$Big5))\n# For now, we will just center Familiarity and Big5 because we created them synthetically. But the real data have these two variables already z-scored\n\nZ-scoring expressibility within a modality\n\nfinal_data &lt;-\n  final_data |&gt;\n  group_by(Modality) |&gt;\n  mutate(Expressibility_z = (Expressibility - mean(Expressibility))/ sd(final_data$Expressibility, na.rm = T)) |&gt;\n  ungroup()",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-1---simple-reproduction-of-dag",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-1---simple-reproduction-of-dag",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 1 - Simple reproduction of DAG",
    "text": "Model 1 - Simple reproduction of DAG\nFirst we want to do a simple model that reproduce our DAG.\nOur main predictor is communicative attempt (CommAtt). To account for confounders - variables affecting both the predictor and the dependent variable - we need to adjust for them in the model by including them as covariates to isolate the effect of the predictor. Based on our DAG, confounders include:\n\nfamiliarity\nbig5\nexpressibility\ntrial number\nmodality\nconcept\nparticipant\n\nWe include 1.-5. as fixed factors. For 6.-7., we include varying intercepts as we expect that each participant and concept may have they own baseline level of effort and thus allow for individual variation. Partial pooling is also beneficial in that extreme values (or categories will fewer datapoints) will be shrunk toward the overal average.\nWe will now not include PrevAn (previous answer) variable because we will need to do some further data-wrangling when building model for H2. That is mainly because PrevAn has some NA values, concretely for first attempts. Models would automatically exclude NA data, and we would therefore loose all effort data for attempt 1. For H1, however, we want to keep it there.\nThis is our first model without setting any priors, leaving them to default values\n\nh1.m1 &lt;- brm(Eff ~ 1 + CommAtt + Familiarity + Big5 + Expressibility_z + TrialNumber_c + Modality + (1 | Participant) + (1 | Concept),\n                data = final_data,\n                iter = 4000,\n                cores = 4)\n\n# Add criterions for later diagnostics\nh1.m1 &lt;- add_criterion(h1.m1, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh1.m1_R2 &lt;- bayes_R2(h1.m1)\n\n# Save both as objects\nsaveRDS(h1.m1, here(\"09_Analysis_Modeling\", \"models\", \"h1.m1.rds\"))\nsaveRDS(h1.m1_R2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m1_R2.rds\"))\n\nbeep(5)\n\n\nh1.m1 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m1.rds\"))\nh1.m1_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m1_R2.rds\"))\n\n\n# Summary\nsummary(h1.m1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Eff ~ 1 + CommAtt + Familiarity + Big5 + Expressibility_z + TrialNumber_c + Modality + (1 | Participant) + (1 | Concept) \n   Data: final_data (Number of observations: 5076) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.04      0.02     0.00     0.09 1.00     2122     1863\n\n~Participant (Number of levels: 120) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.03      0.02     0.00     0.07 1.00     2320     3475\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            4.02      0.02     3.98     4.06 1.00     6694     5622\nCommAtt2M1           3.08      0.03     3.03     3.14 1.00     8963     6147\nCommAtt3M2          -4.39      0.04    -4.46    -4.32 1.00     8289     5880\nFamiliarity          1.26      0.02     1.21     1.30 1.00     8380     5226\nBig5                 1.29      0.02     1.25     1.34 1.00     9979     6018\nExpressibility_z     0.46      0.02     0.43     0.49 1.00     9888     5989\nTrialNumber_c       -0.00      0.00    -0.00     0.00 1.00     7595     5984\nModality1           -1.30      0.04    -1.38    -1.23 1.00     8559     5997\nModality2            0.64      0.04     0.57     0.71 1.00     7347     5787\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.91      0.01     0.89     0.93 1.00    11791     5667\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote on modality: Modality1 represents the baseline difference between combined and the other modalities, and Modality2 represents the difference between gesture and vocal compared to combined.\n\n\n\nTransforming coefficients\nTo be able to link these estimates back to the simulation coefficients, let’s create a function\n\ntransform_attempt &lt;- function(intercept, CommAtt2M1, CommAtt3M2) {\n  # Effort for the first attempt (base effort)\n  Eff_attempt_1 &lt;- intercept\n  \n  # Effort for the second attempt\n  Eff_attempt_2 &lt;- Eff_attempt_1 + CommAtt2M1\n  \n  # Effort for the third attempt\n  Eff_attempt_3 &lt;- Eff_attempt_1 + CommAtt2M1 + CommAtt3M2\n  \n  # Calculate ratios\n  b_attempt2 &lt;- Eff_attempt_2 / Eff_attempt_1\n  b_attempt3 &lt;- Eff_attempt_3 / Eff_attempt_2\n  \n  return(data.frame(b_attempt2, b_attempt3))\n}\n\nh1m1_coeff &lt;- transform_attempt(4.02, 3.08, -4.39)\nprint(h1m1_coeff)\n\n  b_attempt2 b_attempt3\n1   1.766169  0.3816901\n\n# For centered familiarity\nfam = (4.02 + 1.26) / 4.02\nprint(fam)\n\n[1] 1.313433\n\n# For centered BIF\nbif = (4.02 + 1.29) / 4.02\nprint(bif)\n\n[1] 1.320896\n\n\n\n\n\n\n\n\nNote\n\n\n\nExpressibility is z-scored so we will not get to the coefficient in the same way but we can check the conditinal effects for checking whether it looks good\n\n\nLet’s also check the visuals\n\nplot(h1.m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# all caterpillars look nice\n\nplot(conditional_effects(h1.m1), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the effects all go in good direction\n\npp_check(h1.m1, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# Looks good but not amazing - mostly because the posteriors seem to not know effort cannot be negative\n\npp_check(h1.m1, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# half blobby, half correlated, so still some room for improvement\n# positive correlation means that errors increase with predicted values. So the model does perform well for some range, but becomes less reliable with increase in the predicted values\n# also the blob is centered around 0 which is good\n# it could be we are ignoring some interaction terms or non-linearity (which we know we kind of do). Transformation could also help (e.g., log). Of course, we are also still not specifying any priors so let's not yet make it a disaster\n\nh1.m1_R2\n\n    Estimate   Est.Error      Q2.5     Q97.5\nR2 0.8381496 0.001662319 0.8348364 0.8412852\n\n# explained variance around 83%\n\nOverall, we see good directions of all predictors, mostly also in accordance with the expected coefficients. Of course, the synthetic data is quite complex so there might be other dependencies that moderate the causal relationships and that is why we do not see exactly the numbers we use to create the data.\nLet’s have another model for comparison.\nWe can assume that participants and concept have not only different baselines of effort (varying intercept). The effect of CommAtt on effort might vary across them too, hence we can try to add varying slopes for them and see whether the diagnostics improves. We will also add TrialNumber as a varying intercept, because we expect variation between earlier and later performances (because of learning, or opposite, fatigue).",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-2---varying-slopes-and-intercepts",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-2---varying-slopes-and-intercepts",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 2 - Varying slopes and intercepts",
    "text": "Model 2 - Varying slopes and intercepts\n\nh1.m2 &lt;- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), \n                data = final_data,\n                iter = 4000,\n                cores = 4)\n\n# Add criterions for later diagnostics\nh1.m2 &lt;- add_criterion(h1.m2, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh1.m2_R2 &lt;- bayes_R2(h1.m2)\n\n# Save both as objects\nsaveRDS(h1.m2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m2.rds\"))\nsaveRDS(h1.m2_R2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m2_R2.rds\"))\n\nbeep(5)\n\n\nh1.m2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m2.rds\"))\nh1.m2_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m2_R2.rds\"))\n\n# Summary\nsummary(h1.m2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z + (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c) \n   Data: final_data (Number of observations: 5076) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                  0.06      0.02     0.02     0.10 1.00     2791\nsd(CommAtt2M1)                 0.17      0.04     0.11     0.26 1.00     3239\nsd(CommAtt3M2)                 0.25      0.06     0.16     0.37 1.00     3386\ncor(Intercept,CommAtt2M1)      0.52      0.28    -0.14     0.93 1.00     1312\ncor(Intercept,CommAtt3M2)     -0.35      0.32    -0.90     0.32 1.00     1261\ncor(CommAtt2M1,CommAtt3M2)    -0.93      0.07    -1.00    -0.75 1.00     5691\n                           Tail_ESS\nsd(Intercept)                  2048\nsd(CommAtt2M1)                 4832\nsd(CommAtt3M2)                 5133\ncor(Intercept,CommAtt2M1)      1617\ncor(Intercept,CommAtt3M2)      1450\ncor(CommAtt2M1,CommAtt3M2)     6716\n\n~Participant (Number of levels: 120) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                  0.15      0.04     0.08     0.22 1.00      901\nsd(CommAtt2M1)                 0.78      0.06     0.68     0.90 1.00     2066\nsd(CommAtt3M2)                 1.10      0.08     0.95     1.27 1.00     2076\ncor(Intercept,CommAtt2M1)      0.96      0.03     0.87     1.00 1.01      352\ncor(Intercept,CommAtt3M2)     -0.96      0.04    -1.00    -0.85 1.01      353\ncor(CommAtt2M1,CommAtt3M2)    -1.00      0.00    -1.00    -0.99 1.00     7139\n                           Tail_ESS\nsd(Intercept)                  2511\nsd(CommAtt2M1)                 3409\nsd(CommAtt3M2)                 3623\ncor(Intercept,CommAtt2M1)       596\ncor(Intercept,CommAtt3M2)       690\ncor(CommAtt2M1,CommAtt3M2)     6841\n\n~TrialNumber_c (Number of levels: 50) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.02      0.02     0.00     0.06 1.00     3119     3942\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            4.03      0.02     3.99     4.08 1.00     2093     3494\nCommAtt2M1           3.07      0.08     2.90     3.23 1.00     1384     2570\nCommAtt3M2          -4.36      0.12    -4.59    -4.13 1.00     1411     2477\nModality1           -1.29      0.03    -1.35    -1.23 1.00     8986     6966\nModality2            0.64      0.03     0.58     0.71 1.00     9767     6458\nBig5                 1.08      0.05     0.99     1.16 1.01      854     2264\nFamiliarity          1.04      0.05     0.94     1.12 1.00      957     2173\nExpressibility_z     0.44      0.02     0.41     0.47 1.00     5445     5281\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.80      0.01     0.78     0.81 1.00    11536     6199\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Coefficients remain mostly unchanged but there were some divergent transitions\n\n\nplot(h1.m2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# some some of the caterpillars are not that pretty anymore\n\nplot(conditional_effects(h1.m2), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the effects all go in good direction\n\npp_check(h1.m2, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# Looks good but not amazing - mostly because the posteriors seem to not know effort cannot be negative\n\npp_check(h1.m2, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# This looks a bit more blobby than before but still lots of errors for higher values\n\nh1.m2_R2\n\n    Estimate  Est.Error      Q2.5     Q97.5\nR2 0.8760573 0.00125936 0.8734939 0.8784314\n\n# explained variance around 87%\n\nOne of the reasons why we might be getting the divergent transition is because of the correlation between slopes and intercepts in the previous model. Let’s now get rid of it",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-3---no-correlation-coefficient",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-3---no-correlation-coefficient",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 3 - No correlation coefficient",
    "text": "Model 3 - No correlation coefficient\n\nh1.m3 &lt;- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), \n                data = final_data,\n                iter = 4000,\n                cores = 4)\n                \n# Add criterions for later diagnostics\nh1.m3 &lt;- add_criterion(h1.m3, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh1.m3_R2 &lt;- bayes_R2(h1.m3)\n\n# Save both as objects\nsaveRDS(h1.m3, here(\"09_Analysis_Modeling\", \"models\", \"h1.m3.rds\"))\nsaveRDS(h1.m3_R2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m3_R2.rds\"))\n\nbeep(5)\n\n\nh1.m3 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m3.rds\"))\nh1.m3_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m3_R2.rds\"))\n\n# Summary\nsummary(h1.m3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z + (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c) \n   Data: final_data (Number of observations: 5076) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.05      0.02     0.01     0.10 1.00     2648     2360\nsd(CommAtt2M1)     0.09      0.05     0.01     0.19 1.01     1159     1540\nsd(CommAtt3M2)     0.17      0.06     0.04     0.30 1.00     1422     1517\n\n~Participant (Number of levels: 120) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.03      0.02     0.00     0.08 1.00     2442     4123\nsd(CommAtt2M1)     0.71      0.05     0.61     0.82 1.00     2733     4516\nsd(CommAtt3M2)     1.05      0.08     0.91     1.21 1.00     2231     3994\n\n~TrialNumber_c (Number of levels: 50) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.02      0.02     0.00     0.06 1.00     3301     3712\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            4.03      0.02     3.99     4.06 1.00     9303     6039\nCommAtt2M1           3.07      0.07     2.93     3.22 1.00     2152     3434\nCommAtt3M2          -4.36      0.11    -4.58    -4.15 1.00     2100     3982\nModality1           -1.30      0.03    -1.36    -1.24 1.00    11936     6326\nModality2            0.65      0.03     0.58     0.71 1.00    12551     6401\nBig5                 1.23      0.02     1.19     1.28 1.00    15911     6657\nFamiliarity          1.20      0.02     1.15     1.24 1.00    14065     6574\nExpressibility_z     0.45      0.01     0.42     0.48 1.00    14842     5619\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.81      0.01     0.80     0.83 1.00    11557     5519\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Coefficients remain mostly unchanged \n\n\nplot(h1.m3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# no correlation fixed the performance\n\nplot(conditional_effects(h1.m3), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the effects all go in good direction\n\npp_check(h1.m3, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# the problem with predicting negative values remains\n\npp_check(h1.m3, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# unchanged\n\nh1.m3_R2\n\n    Estimate   Est.Error      Q2.5    Q97.5\nR2 0.8714492 0.001445024 0.8685275 0.874215\n\n# explained variance remains around 87%",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-3.1---adding-priors",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-3.1---adding-priors",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 3.1 - adding priors",
    "text": "Model 3.1 - adding priors\nNow we will add some minimal priors, giving the model some reasonably informative, yet weak information.\n\npriors_h1m3p &lt;- c(\n  set_prior(\"normal(2.5, 0.5)\", class = \"Intercept\", lb=0),\n  set_prior(\"normal(0,0.50)\", class = \"b\", coef = \"CommAtt2M1\"),\n  set_prior(\"normal(0,0.50)\", class = \"b\", coef = \"CommAtt3M2\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Modality1\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Modality2\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Big5\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Familiarity\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Expressibility_z\"),\n  \n  set_prior(\"normal(0.5,0.1)\", class = \"sd\", group = \"TrialNumber_c\"),\n  set_prior(\"normal(0.5,0.1)\", class = \"sd\", group = \"Participant\"),\n  set_prior(\"normal(0.5,0.1)\", class = \"sd\", group = \"Concept\"),\n  set_prior(\"normal(1,0.1)\", class = \"sd\"),\n  \n  set_prior(\"normal(0.5,0.25)\", class = \"sigma\")\n  \n)\n\nh1.m3p &lt;- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), \n                data = final_data,\n                prior=priors_h1m3p,\n                family = gaussian,\n                iter = 4000,\n                cores = 4)\n\n\n# Add criterions for later diagnostics\nh1.m3p &lt;- add_criterion(h1.m3p, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh1.m3p_R2 &lt;- bayes_R2(h1.m3p)\n\n# Save both as objects\nsaveRDS(h1.m3p, here(\"09_Analysis_Modeling\", \"models\", \"h1.m3p.rds\"))\nsaveRDS(h1.m3p_R2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m3p_R2.rds\"))\n\nbeep(5)\n\n\nh1.m3p &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m3p.rds\"))\nh1.m3p_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m3p_R2.rds\"))\n\n\n# Summary\nsummary(h1.m3p)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z + (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c) \n   Data: final_data (Number of observations: 5076) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.08      0.03     0.03     0.14 1.00     2732     4188\nsd(CommAtt2M1)     0.22      0.06     0.12     0.36 1.00     2853     4652\nsd(CommAtt3M2)     0.34      0.08     0.20     0.51 1.00     2590     5063\n\n~Participant (Number of levels: 120) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.06      0.02     0.01     0.10 1.00     2220     2200\nsd(CommAtt2M1)     0.66      0.04     0.58     0.75 1.00     4131     5851\nsd(CommAtt3M2)     0.89      0.05     0.80     0.98 1.00     4638     6332\n\n~TrialNumber_c (Number of levels: 50) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.04      0.02     0.00     0.08 1.00     2687     3691\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            4.02      0.02     3.97     4.07 1.00     8469     6434\nCommAtt2M1           2.99      0.08     2.83     3.15 1.00     2795     4372\nCommAtt3M2          -4.12      0.12    -4.35    -3.87 1.00     2401     3679\nModality1           -1.27      0.03    -1.34    -1.21 1.00    13767     6673\nModality2            0.63      0.03     0.56     0.69 1.00    13131     6444\nBig5                 1.23      0.02     1.18     1.28 1.00    12639     6019\nFamiliarity          1.19      0.03     1.14     1.24 1.00    13068     6412\nExpressibility_z     0.45      0.02     0.42     0.48 1.00    15210     5985\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.81      0.01     0.80     0.83 1.00    14125     5732\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Coefficients remain mostly unchanged \n\n\nplot(h1.m3p)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# all good\n\nplot(conditional_effects(h1.m3p), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the effects all go in good direction\n\npp_check(h1.m3p, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# the problem with predicting negative values remains\n\npp_check(h1.m3p, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# unchanged\n\nh1.m3p_R2\n\n    Estimate   Est.Error      Q2.5    Q97.5\nR2 0.8705705 0.001478901 0.8676255 0.873398\n\n# explained variance remains around 87%",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-4---restricting-priors-with-exponential-distribution",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-4---restricting-priors-with-exponential-distribution",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 4 - Restricting priors with exponential distribution",
    "text": "Model 4 - Restricting priors with exponential distribution\nLet’s do one more test with the priors, restricting sd them with exponential distribution, i.e., negative values are not possible\n\npriors_h1m4 &lt;- c(\n  set_prior(\"normal(3, 0.3)\", class = \"Intercept\", lb = 0),\n  set_prior(\"normal(0, 0.25)\", class = \"b\", coef = \"CommAtt2M1\"),\n  set_prior(\"normal(0, 0.25)\", class = \"b\", coef = \"CommAtt3M2\"),\n  set_prior(\"normal(0, 0.15)\", class = \"b\", coef = \"Modality1\"),\n  set_prior(\"normal(0, 0.15)\", class = \"b\", coef = \"Modality2\"),\n  set_prior(\"normal(0, 0.15)\", class = \"b\", coef = \"Big5\"),\n  set_prior(\"normal(0, 0.15)\", class = \"b\", coef = \"Familiarity\"),\n  set_prior(\"normal(0, 0.15)\", class = \"b\", coef = \"Expressibility_z\"),\n\n  # Exponential priors for standard deviations\n  set_prior(\"exponential(3)\", class = \"sd\", group = \"TrialNumber_c\"), # exp(3) has a mean of 1/3 and concentrates most density around small values\n  set_prior(\"exponential(3)\", class = \"sd\", group = \"Participant\"),\n  set_prior(\"exponential(3)\", class = \"sd\", group = \"Concept\"),\n  set_prior(\"exponential(1)\", class = \"sd\"),  # Generic sd prior\n\n  # Residual standard deviation - keep it narrow\n  set_prior(\"normal(0.5, 0.1)\", class = \"sigma\")\n)\n\nh1.m4 &lt;- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), \n                data = final_data,\n                prior=priors_h1m4,\n                family = gaussian,\n                iter = 4000,\n                cores = 4)\n\n\n# Add criterions for later diagnostics\nh1.m4 &lt;- add_criterion(h1.m4, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh1.m4_R2 &lt;- bayes_R2(h1.m4)\n\n# Save both as objects\nsaveRDS(h1.m4, here(\"09_Analysis_Modeling\", \"models\", \"h1.m4.rds\"))\nsaveRDS(h1.m4_R2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m4_R2.rds\"))\n\nbeep(5)\n\n\nh1.m4 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m4.rds\"))\nh1.m4_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m4_R2.rds\"))\n\n\n# Summary\nsummary(h1.m4)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z + (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c) \n   Data: final_data (Number of observations: 5076) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.05      0.02     0.01     0.09 1.00     2376     2462\nsd(CommAtt2M1)     1.98      0.37     1.33     2.78 1.00     1362     2663\nsd(CommAtt3M2)     3.36      0.46     2.56     4.38 1.00      942     2031\n\n~Participant (Number of levels: 120) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.03      0.02     0.00     0.08 1.00     2217     4211\nsd(CommAtt2M1)     0.71      0.05     0.61     0.82 1.00     2746     4486\nsd(CommAtt3M2)     1.05      0.08     0.92     1.22 1.00     2446     4360\n\n~TrialNumber_c (Number of levels: 50) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.02      0.02     0.00     0.06 1.00     3383     3814\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            4.02      0.02     3.99     4.06 1.00     9553     6582\nCommAtt2M1           0.88      0.30     0.31     1.47 1.00     4070     5104\nCommAtt3M2          -0.36      0.26    -0.87     0.14 1.00    10008     6008\nModality1           -1.23      0.03    -1.29    -1.16 1.00    14208     6281\nModality2            0.59      0.03     0.53     0.65 1.00    15010     6334\nBig5                 1.21      0.02     1.16     1.25 1.00    13972     6575\nFamiliarity          1.17      0.02     1.12     1.21 1.00    13947     5839\nExpressibility_z     0.44      0.01     0.41     0.47 1.00    17170     6516\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.81      0.01     0.79     0.83 1.00    15722     5841\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# the coefficients for commatt now seem much lower\n\n\nh1m1_coeff &lt;- transform_attempt(4.02, 0.88, -0.36)\nprint(h1m1_coeff)\n\n  b_attempt2 b_attempt3\n1   1.218905  0.9265306\n\n# For centered familiarity\nfam = (4.02 + 1.17) / 4.02\nprint(fam)\n\n[1] 1.291045\n\n# For centered BIF\nbif = (4.02 + 1.21) / 4.02\nprint(bif)\n\n[1] 1.300995\n\n\nSo it seems that we are misleading with the priors and this will not be the way\n\nplot(h1.m4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# all good\n\nplot(conditional_effects(h1.m4), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the effect of main predictor is now very moderated \n\npp_check(h1.m4, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# the problem with predicting negative values remains\n\npp_check(h1.m4, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# unchanged\n\nh1.m4_R2\n\n    Estimate   Est.Error     Q2.5    Q97.5\nR2 0.8701813 0.001472199 0.867155 0.872996\n\n# explained variance remains around 87%",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-5---student-family",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-5---student-family",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 5 - student family",
    "text": "Model 5 - student family\nWe still see negative values in the posterior simulations, so let’s try Student’s t-distribution which is more robust to outliers and can potentially reduce the likelihood of negative values (if we reduce degrees of freedom)\n\npriors_h1m5 &lt;- c(\n  set_prior(\"normal(2.5, 0.5)\", class = \"Intercept\", lb=0),\n  set_prior(\"normal(0,0.50)\", class = \"b\", coef = \"CommAtt2M1\"),\n  set_prior(\"normal(0,0.50)\", class = \"b\", coef = \"CommAtt3M2\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Modality1\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Modality2\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Big5\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Familiarity\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Expressibility_z\"),\n  \n  set_prior(\"normal(0.5,0.05)\", class = \"sd\", group = \"TrialNumber_c\"),\n  set_prior(\"normal(0.5,0.05)\", class = \"sd\", group = \"Participant\"),\n  set_prior(\"normal(0.5,0.05)\", class = \"sd\", group = \"Concept\"),\n  set_prior(\"normal(1,0.05)\", class = \"sd\"),\n  \n  set_prior(\"normal(0.5,0.1)\", class = \"sigma\"),\n  set_prior(\"gamma(2, 0.1)\", class = \"nu\")  # Prior for degrees of freedom\n  \n)\n\nh1.m5 &lt;- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), \n                data = final_data,\n                prior=priors_h1m5,\n                family = student,\n                iter = 4000,\n                cores = 4)\n\n\n# Add criterions for later diagnostics\nh1.m5 &lt;- add_criterion(h1.m5, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh1.m5_R2 &lt;- bayes_R2(h1.m5)\n\n# Save both as objects\nsaveRDS(h1.m5, here(\"09_Analysis_Modeling\", \"models\", \"h1.m5.rds\"))\nsaveRDS(h1.m5_R2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m5_R2.rds\"))\n\nbeep(5)\n\n\nh1.m5 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m5.rds\"))\nh1.m5_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m5_R2.rds\"))\n\n\n# Summary\nsummary(h1.m5)\n\n Family: student \n  Links: mu = identity; sigma = identity; nu = identity \nFormula: Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z + (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c) \n   Data: final_data (Number of observations: 5076) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.38      0.06     0.26     0.49 1.00     5974     5242\nsd(CommAtt2M1)     0.42      0.05     0.32     0.53 1.00     8447     6363\nsd(CommAtt3M2)     0.46      0.05     0.36     0.56 1.00     8880     6408\n\n~Participant (Number of levels: 120) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.13      0.02     0.10     0.17 1.00     3617     5313\nsd(CommAtt2M1)     0.57      0.03     0.51     0.63 1.00     6105     6010\nsd(CommAtt3M2)     0.72      0.03     0.66     0.79 1.00     7296     6471\n\n~TrialNumber_c (Number of levels: 50) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.09      0.03     0.04     0.15 1.00     1649     2746\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            3.92      0.08     3.75     4.08 1.00     1736     3195\nCommAtt2M1           2.78      0.11     2.56     2.99 1.00     2927     3842\nCommAtt3M2          -3.95      0.13    -4.19    -3.70 1.00     2998     3937\nModality1           -1.11      0.03    -1.17    -1.06 1.00    13750     6634\nModality2            0.56      0.03     0.51     0.62 1.00    15623     6248\nBig5                 1.15      0.03     1.10     1.21 1.00    10082     6655\nFamiliarity          1.13      0.03     1.07     1.18 1.00     9422     6474\nExpressibility_z     0.39      0.01     0.36     0.41 1.00    16992     6093\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.52      0.01     0.50     0.55 1.00    11160     6372\nnu        2.96      0.16     2.67     3.28 1.00    13245     5813\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# the coefficients look ok again. Familiarity and Big5 even got closer to our simulated betas\n\n\nplot(h1.m5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# all good\n\nplot(conditional_effects(h1.m5), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the effects all go in good direction\n\npp_check(h1.m5, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# the fit seems somewhat better than with gaussian, but still negative values are there\n\npp_check(h1.m5, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# unchanged\n\nh1.m5_R2\n\n    Estimate   Est.Error    Q2.5     Q97.5\nR2 0.8541097 0.002443239 0.84921 0.8587499\n\n# explained variance remains around 85%",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-6---log-normal-distribution",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-6---log-normal-distribution",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 6 - log-normal distribution",
    "text": "Model 6 - log-normal distribution\nWe have already seen - when plotting - that effort probably tends towards lognormal distribution. We will keep the model constant, just exchange the distribution. Priors are kept default\n\nh1.m6 &lt;- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), \n                data = final_data,\n                family = lognormal(),\n                iter = 4000,\n                cores = 4)\n\n\n# Add criterions for later diagnostics\nh1.m6 &lt;- add_criterion(h1.m6, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh1.m6_R2 &lt;- bayes_R2(h1.m6)\n\n# Save both as objects\nsaveRDS(h1.m6, here(\"09_Analysis_Modeling\", \"models\", \"h1.m6.rds\"))\nsaveRDS(h1.m6_R2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m6_R2.rds\"))\n\nbeep(5)\n\n\nh1.m6 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m6.rds\"))\nh1.m6_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m6_R2.rds\"))\n\n\n# Summary\nsummary(h1.m6)\n\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z + (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c) \n   Data: final_data (Number of observations: 5076) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.01      0.00     0.00     0.02 1.00     2298     2414\nsd(CommAtt2M1)     0.00      0.00     0.00     0.01 1.00     4803     3682\nsd(CommAtt3M2)     0.01      0.01     0.00     0.02 1.00     3687     3650\n\n~Participant (Number of levels: 120) \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)      0.05      0.00     0.04     0.06 1.00     2844     4508\nsd(CommAtt2M1)     0.00      0.00     0.00     0.01 1.00     4417     3655\nsd(CommAtt3M2)     0.01      0.01     0.00     0.03 1.00     2618     2888\n\n~TrialNumber_c (Number of levels: 50) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.00      0.00     0.00     0.01 1.00     3411     3578\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            1.23      0.01     1.22     1.24 1.00     3120     4439\nCommAtt2M1           0.62      0.01     0.61     0.63 1.00     9634     6358\nCommAtt3M2          -1.10      0.01    -1.11    -1.08 1.00     9514     5588\nModality1           -0.31      0.01    -0.33    -0.30 1.00     9097     6088\nModality2            0.16      0.01     0.15     0.17 1.00     9223     6512\nBig5                 0.32      0.01     0.31     0.34 1.00     2924     4558\nFamiliarity          0.32      0.01     0.31     0.34 1.00     2297     3545\nExpressibility_z     0.12      0.00     0.12     0.13 1.00    10923     6710\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.17      0.00     0.17     0.18 1.00    11348     5998\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Coefficients remain mostly unchanged \n\n\nConverting to original scale\nOf course now we have all on lognormal scale so it is not so straightforwadly interpretable. This is how we can get to the values of an effort under certain value of a predictor\n\n# Compute estimated marginal means on log-scale\nem_h1.m6 &lt;- emmeans(h1.m6, ~ CommAtt) #~ CommAtt\n\n#Backtransform the post.beta values\nem_h1.m6@post.beta &lt;- exp(em_h1.m6@post.beta)\nprint(em_h1.m6)\n\n CommAtt emmean lower.HPD upper.HPD\n 1         3.33      3.29      3.37\n 2         6.21      6.13      6.29\n 3         2.08      2.05      2.11\n\nResults are averaged over the levels of: Modality \nPoint estimate displayed: median \nHPD interval probability: 0.95 \n\n\nSo we indeed see that effort in CommAtt2 increase but then decreases again for CommAtt3. We can also try to get our simulated betas\n\ncoeff1 &lt;- 3.33*1.5 \ncoeff2 &lt;- 3.33*0.5\n\nprint(coeff1)\n\n[1] 4.995\n\nprint(coeff2) # which is very close to the mean values we see from the model\n\n[1] 1.665\n\n\nWe see that the effect estimated by the model is now stronger than in our simulated data. However, note that these values are averaged over predictor modality\nWe can also try a different method to get the coefficients (code adapted from here)\n\n# Extract posterior samples\nalpha_samples &lt;- as_draws_df(h1.m6)$b_Intercept\nbeta_2_vs_1 &lt;- as_draws_df(h1.m6)$b_CommAtt2M1\nbeta_3_vs_2 &lt;- as_draws_df(h1.m6)$b_CommAtt3M2\n\n# Compute expected values on the log scale\nmu_1 &lt;- alpha_samples  # CommAtt 1\nmu_2 &lt;- alpha_samples + beta_2_vs_1  # CommAtt 2\nmu_3 &lt;- alpha_samples + beta_2_vs_1 + beta_3_vs_2  # CommAtt 3\n\n# Transform to original scale\neffect_1 &lt;- exp(mu_1)\neffect_2 &lt;- exp(mu_2)\neffect_3 &lt;- exp(mu_3)\n\n# Calculate contrasts on the original scale\neffect_diff_2_vs_1 &lt;- effect_2 - effect_1\neffect_diff_3_vs_2 &lt;- effect_3 - effect_2\neffect_diff_3_vs_1 &lt;- effect_3 - effect_1\n\n# Summarize the effects\nlist(\n  mean_intercept = mean(effect_1),\n  mean_diff_2_vs_1 = c(mean = mean(effect_diff_2_vs_1), quantile(effect_diff_2_vs_1, c(0.025, 0.975))),\n  mean_diff_3_vs_2 = c(mean = mean(effect_diff_3_vs_2), quantile(effect_diff_3_vs_2, c(0.025, 0.975))),\n  mean_diff_3_vs_1 = c(mean = mean(effect_diff_3_vs_1), quantile(effect_diff_3_vs_1, c(0.025, 0.975)))\n)\n\n$mean_intercept\n[1] 3.419549\n\n$mean_diff_2_vs_1\n    mean     2.5%    97.5% \n2.950688 2.872812 3.029410 \n\n$mean_diff_3_vs_2\n     mean      2.5%     97.5% \n-4.240333 -4.321665 -4.160104 \n\n$mean_diff_3_vs_1\n     mean      2.5%     97.5% \n-1.289646 -1.320918 -1.257698 \n\n\nNow we can use these coefficients to transform to our simulated betas\n\nh1m6_coeff &lt;- transform_attempt(3.419549, 2.950688, -4.240333)\nprint(h1m6_coeff)\n\n  b_attempt2 b_attempt3\n1   1.862888  0.3343524\n\n\nNow this looks closer again to our betas.\nThis code does the same for all predictors (except concept and participant)\n\n# Extract posterior samples\nposterior_samples &lt;- as_draws_df(h1.m6)\nalpha_samples &lt;- posterior_samples$b_Intercept\n\n# Create a list to store effects for each fixed factor\neffect_list &lt;- list()\n\n# Helper function to calculate summary statistics\nget_effect_summary &lt;- function(effect_samples) {\n  mean_effect &lt;- mean(effect_samples)\n  se_effect &lt;- sd(effect_samples)\n  ci_effect &lt;- quantile(effect_samples, c(0.025, 0.975))\n  post_prob &lt;- mean(effect_samples &gt; 0)\n  c(mean = mean_effect, \n    se = se_effect, \n    lower_ci = ci_effect[1], \n    upper_ci = ci_effect[2], \n    post_prob = post_prob)\n}\n\n# COMMATT (successive differences coding)\nif (\"b_CommAtt2M1\" %in% colnames(posterior_samples) & \"b_CommAtt3M2\" %in% colnames(posterior_samples)) {\n  beta_2_vs_1 &lt;- posterior_samples$b_CommAtt2M1\n  beta_3_vs_2 &lt;- posterior_samples$b_CommAtt3M2\n  \n  mu_1 &lt;- alpha_samples\n  mu_2 &lt;- alpha_samples + beta_2_vs_1\n  mu_3 &lt;- alpha_samples + beta_2_vs_1 + beta_3_vs_2\n  \n  effect_list$CommAtt &lt;- rbind(\n    \"commat 2 vs 1\" = get_effect_summary(exp(mu_2) - exp(mu_1)),\n    \"commat 3 vs 2\" = get_effect_summary(exp(mu_3) - exp(mu_2)),\n    \"commat 3 vs 1\" = get_effect_summary(exp(mu_3) - exp(mu_1))\n  )\n}\n\n# MODALITY (sum contrasts scaled by 0.5)\nif (\"b_Modality1\" %in% colnames(posterior_samples) & \"b_Modality2\" %in% colnames(posterior_samples)) {\n  beta_mod_1 &lt;- posterior_samples$b_Modality1\n  beta_mod_2 &lt;- posterior_samples$b_Modality2\n  \n  mu_mod_1 &lt;- alpha_samples + beta_mod_1\n  mu_mod_2 &lt;- alpha_samples + beta_mod_2\n  mu_mod_3 &lt;- alpha_samples - beta_mod_1 - beta_mod_2\n  \n  effect_list$Modality &lt;- rbind(\n    \"mod 1 vs 2\" = get_effect_summary(exp(mu_mod_1) - exp(mu_mod_2)),\n    \"mod 1 vs 3\" = get_effect_summary(exp(mu_mod_1) - exp(mu_mod_3)),\n    \"mod 2 vs 3\" = get_effect_summary(exp(mu_mod_2) - exp(mu_mod_3))\n  )\n}\n\n# BIG5 (continuous)\nif (\"b_Big5\" %in% colnames(posterior_samples)) {\n  beta_big5 &lt;- posterior_samples$b_Big5\n  effect_list$Big5 &lt;- get_effect_summary(exp(alpha_samples + beta_big5) - exp(alpha_samples))\n}\n\n# FAMILIARITY (continuous)\nif (\"b_Familiarity\" %in% colnames(posterior_samples)) {\n  beta_fam &lt;- posterior_samples$b_Familiarity\n  effect_list$Familiarity &lt;- get_effect_summary(exp(alpha_samples + beta_fam) - exp(alpha_samples))\n}\n\n# EXPRESSIBILITY_Z (continuous)\nif (\"b_Expressibility_z\" %in% colnames(posterior_samples)) {\n  beta_expr &lt;- posterior_samples$b_Expressibility_z\n  effect_list$Expressibility_z &lt;- get_effect_summary(exp(alpha_samples + beta_expr) - exp(alpha_samples))\n}\n\n# TRIAL NUMBER (centered continuous)\nif (\"b_TrialNumber_c\" %in% colnames(posterior_samples)) {\n  beta_trial &lt;- posterior_samples$b_TrialNumber_c\n  effect_list$TrialNumber_c &lt;- get_effect_summary(exp(alpha_samples + beta_trial) - exp(alpha_samples))\n}\n\n# Convert to a nicely formatted data frame\neffect_df &lt;- do.call(rbind, effect_list)\n\n# View effects\neffect_df\n\n                        mean         se lower_ci.2.5% upper_ci.97.5% post_prob\ncommat 2 vs 1     2.95068779 0.04052085    2.87281233      3.0294096   1.00000\ncommat 3 vs 2    -4.24033346 0.04140812   -4.32166524     -4.1601045   0.00000\ncommat 3 vs 1    -1.28964568 0.01620638   -1.32091801     -1.2576981   0.00000\nmod 1 vs 2       -1.52303929 0.04030161   -1.60299691     -1.4435554   0.00000\nmod 1 vs 3       -1.49131528 0.04098655   -1.57288685     -1.4107477   0.00000\nmod 2 vs 3        0.03172401 0.04858764   -0.06380552      0.1252410   0.74175\nBig5              1.31309529 0.04264827    1.23007768      1.3970338   1.00000\nFamiliarity       1.30972571 0.04408809    1.22420946      1.3983930   1.00000\nExpressibility_z  0.44448503 0.01197175    0.42135330      0.4684496   1.00000\n\n\nSo here we also see the negative effect of combined modality (mod1)\nNow we can also check some visual diagnostics.\n\nplot(h1.m6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# all good\n\nplot(conditional_effects(h1.m6), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the effects all go in good direction\n\npp_check(h1.m6, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# we got rid of negative predictions, and it looks very good\n\npp_check(h1.m6, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# very nice blob\n\nh1.m6_R2\n\n    Estimate  Est.Error      Q2.5     Q97.5\nR2 0.8889641 0.00137284 0.8861744 0.8915303\n\n# explained variance increases to 88%",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-6.1---with-correlation",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-6.1---with-correlation",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 6.1 - with correlation",
    "text": "Model 6.1 - with correlation\nSince we now significantly improved the model performance, let’s try once again the correlation between slope and intercept\n\nh1.m6c &lt;- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), \n                data = final_data,\n                family = lognormal(),\n                iter = 4000,\n                cores = 4)\n\n\n# Add criterions for later diagnostics\nh1.m6c &lt;- add_criterion(h1.m6c, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh1.m6c_R2 &lt;- bayes_R2(h1.m6c)\n\n# Save both as objects\nsaveRDS(h1.m6c, here(\"09_Analysis_Modeling\", \"models\", \"h1.m6c.rds\"))\nsaveRDS(h1.m6c_R2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m6c_R2.rds\"))\n\nbeep(5)\n\n\nh1.m6c &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m6c.rds\"))\nh1.m6c_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m6c_R2.rds\"))\n\n\n# Summary\nsummary(h1.m6c)\n\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z + (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c) \n   Data: final_data (Number of observations: 5076) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                  0.01      0.00     0.00     0.02 1.00     1744\nsd(CommAtt2M1)                 0.01      0.00     0.00     0.02 1.00     5613\nsd(CommAtt3M2)                 0.01      0.01     0.00     0.03 1.00     3784\ncor(Intercept,CommAtt2M1)      0.28      0.49    -0.78     0.95 1.00    11362\ncor(Intercept,CommAtt3M2)      0.33      0.47    -0.75     0.95 1.00     7628\ncor(CommAtt2M1,CommAtt3M2)    -0.05      0.50    -0.89     0.87 1.00     8530\n                           Tail_ESS\nsd(Intercept)                  1448\nsd(CommAtt2M1)                 5243\nsd(CommAtt3M2)                 4533\ncor(Intercept,CommAtt2M1)      5945\ncor(Intercept,CommAtt3M2)      5887\ncor(CommAtt2M1,CommAtt3M2)     6522\n\n~Participant (Number of levels: 120) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                  0.05      0.00     0.04     0.06 1.00     3640\nsd(CommAtt2M1)                 0.01      0.00     0.00     0.02 1.00     5290\nsd(CommAtt3M2)                 0.01      0.01     0.00     0.03 1.00     3474\ncor(Intercept,CommAtt2M1)      0.35      0.46    -0.73     0.95 1.00    10725\ncor(Intercept,CommAtt3M2)     -0.40      0.40    -0.94     0.59 1.00     9703\ncor(CommAtt2M1,CommAtt3M2)    -0.27      0.49    -0.94     0.78 1.00     5410\n                           Tail_ESS\nsd(Intercept)                  5384\nsd(CommAtt2M1)                 4470\nsd(CommAtt3M2)                 4069\ncor(Intercept,CommAtt2M1)      5934\ncor(Intercept,CommAtt3M2)      5524\ncor(CommAtt2M1,CommAtt3M2)     6511\n\n~TrialNumber_c (Number of levels: 50) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.00      0.00     0.00     0.01 1.00     4362     4575\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            1.23      0.01     1.22     1.24 1.00     5205     5979\nCommAtt2M1           0.62      0.01     0.61     0.63 1.00    14611     6124\nCommAtt3M2          -1.10      0.01    -1.11    -1.08 1.00    12219     6336\nModality1           -0.32      0.01    -0.33    -0.30 1.00    14134     6778\nModality2            0.16      0.01     0.15     0.18 1.00    15159     6553\nBig5                 0.32      0.01     0.31     0.34 1.00     4826     6410\nFamiliarity          0.32      0.01     0.30     0.34 1.00     4174     5246\nExpressibility_z     0.12      0.00     0.12     0.13 1.00    13884     6876\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.17      0.00     0.17     0.18 1.00    17858     5344\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Coefficients remain mostly unchanged \n\n\nplot(h1.m6c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# now correlation does not seem to generate problems\n\nplot(conditional_effects(h1.m6c), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the effects all go in good direction\n\npp_check(h1.m6c, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# ppcheck good\n\npp_check(h1.m6c, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# nice blob\n\nh1.m6c_R2\n\n    Estimate   Est.Error      Q2.5     Q97.5\nR2 0.8890392 0.001393478 0.8861745 0.8916361\n\n# explained variance remains around 88%",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#diagnostics-i",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#diagnostics-i",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Diagnostics I",
    "text": "Diagnostics I\nWe now have several models that we can test for predictive performance.\n\nRhat\nRhat tells us whether the model’s Markov chains have converged—values close to 1 mean the model has likely converged well.\n\n# Extract R-hat values for each model\nrhat_list &lt;- lapply(model_list, function(model) {\n  rhat_values &lt;- rhat(model)\n  data.frame(model = deparse(substitute(model)), \n             max_rhat = max(rhat_values), \n             min_rhat = min(rhat_values))\n})\n\n# Combine and inspect\ndo.call(rbind, rhat_list)\n\n   model max_rhat  min_rhat\n1 X[[i]] 1.002349 0.9996758\n2 X[[i]] 1.011516 0.9997215\n3 X[[i]] 1.005103 0.9997167\n4 X[[i]] 1.003503 0.9996573\n5 X[[i]] 1.004119 0.9996640\n6 X[[i]] 1.003712 0.9996386\n7 X[[i]] 1.002459 0.9995773\n8 X[[i]] 1.002605 0.9996426\n\n\nAll models seems actually ok in terms of Rhat values except model 2 (h1.m2)\n\n\nESS\nEffective sample size tells how many independent samples the model has effectively drawn from the PD. Low ESS suggests autocorrelation (i.e., sample explores one part of posterior), while high ESS means good mix\n\n# Extract n_eff values for each model\nneff_ratio_list &lt;- lapply(model_list, function(model) {\n  neff_values &lt;- neff_ratio(model)              # Here we calculate ratio (not the raw number of effective samples)\n  data.frame(model = deparse(substitute(model)), \n             min_neff = min(neff_values), \n             max_neff = max(neff_values),\n             mean_neff = mean(neff_values))\n               \n})\n\n# Combine and inspect\ndo.call(rbind, neff_ratio_list)\n\n   model   min_neff  max_neff mean_neff\n1 X[[i]] 0.23283181 0.8832823 0.6862927\n2 X[[i]] 0.04396011 0.9020581 0.6204512\n3 X[[i]] 0.14481431 0.8834718 0.7504325\n4 X[[i]] 0.22967606 0.9020381 0.7709286\n5 X[[i]] 0.11769241 0.9065446 0.7505601\n6 X[[i]] 0.20616401 0.9322803 0.7457049\n7 X[[i]] 0.28072060 0.8984722 0.7626900\n8 X[[i]] 0.18101711 0.9370772 0.7924409\n\n\nSo the highest ratio have model h1.m6c (lognormal with correlation) but in fact they are all quite comparable. Let’s loot at 3 highest\n\neffective_sample(h1.m6c) # this one indeed seems the best as it has all ESS around 10k\n\n           Parameter   ESS\n1        b_Intercept  5193\n2       b_CommAtt2M1 14438\n3       b_CommAtt3M2 12340\n4        b_Modality1 14518\n5        b_Modality2 14927\n6             b_Big5  4824\n7      b_Familiarity  4162\n8 b_Expressibility_z 14233\n\neffective_sample(h1.m6)\n\n           Parameter   ESS\n1        b_Intercept  3104\n2       b_CommAtt2M1  9618\n3       b_CommAtt3M2  9509\n4        b_Modality1  9057\n5        b_Modality2  9192\n6             b_Big5  2906\n7      b_Familiarity  2280\n8 b_Expressibility_z 10811\n\neffective_sample(h1.m3p) \n\n           Parameter   ESS\n1        b_Intercept  8533\n2       b_CommAtt2M1  2795\n3       b_CommAtt3M2  2385\n4        b_Modality1 13744\n5        b_Modality2 13144\n6             b_Big5 12483\n7      b_Familiarity 13139\n8 b_Expressibility_z 15614\n\n\nSo there are some considerable differences for different coefficients\n\n\nLOO & WAIC\nLeave-One-Out (LOO) validation is a technique where, for each iteration, one data point is left out as the test set, and the model is trained on the remaining data points; this process is repeated for each data point, and the model’s overall performance is averaged over all iterations.\n\nl &lt;- loo_compare(h1.m1, h1.m2, h1.m3, h1.m3p, h1.m4, h1.m5, h1.m6, h1.m6c, criterion = \"loo\")\n\nprint(l, simplify = F)\n\n       elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic   se_looic\nh1.m6c     0.0       0.0 -5070.5     69.3       126.5     3.5  10141.0   138.5 \nh1.m6     -0.2       1.5 -5070.7     69.2       120.4     3.3  10141.4   138.5 \nh1.m5   -904.2      59.2 -5974.6     74.1       418.5     5.3  11949.3   148.1 \nh1.m2  -1143.5      67.5 -6214.0     78.4       285.6    10.7  12428.0   156.8 \nh1.m3  -1221.1      66.5 -6291.6     75.5       261.3     7.7  12583.2   150.9 \nh1.m3p -1222.5      66.9 -6293.0     76.3       281.6     8.3  12586.0   152.5 \nh1.m4  -1224.7      67.4 -6295.2     77.0       281.3     8.4  12590.3   154.0 \nh1.m1  -1681.0      72.2 -6751.5     74.9        25.3     0.8  13503.0   149.9 \n\n\n\n\n\n\n\n\nNote\n\n\n\nelpd_loo: This is the expected log pointwise predictive density for LOO. Higher values indicate a better fit to the data.\nse_elpd_loo: The standard error of the elpd_loo, representing uncertainty in the model’s predictive fit according to LOO.\nlooic: The LOO Information Criterion, which is similar to waic but based on leave-one-out cross-validation. Lower values are better.\np_loo: The effective number of parameters according to LOO, indicating the model’s complexity.\nse_p_loo: The standard error of p_loo, representing uncertainty around the effective number of parameters.\n\n\nModel with lognormal distribution seems the best as assessed by LOO.\n\nw &lt;- loo_compare(h1.m1, h1.m2, h1.m3, h1.m3p, h1.m4, h1.m5, h1.m6, h1.m6c, criterion = \"waic\")\n\nprint(w, simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nh1.m6c     0.0       0.0 -5070.3      69.3        126.3     3.5   10140.5\nh1.m6     -0.3       1.5 -5070.5      69.2        120.2     3.3   10141.0\nh1.m5   -903.7      59.2 -5974.0      74.1        417.8     5.3   11948.0\nh1.m2  -1142.2      67.5 -6212.5      78.3        284.1    10.6   12424.9\nh1.m3  -1220.4      66.5 -6290.7      75.4        260.3     7.6   12581.3\nh1.m3p -1221.6      66.9 -6291.8      76.2        280.4     8.2   12583.7\nh1.m4  -1223.8      67.3 -6294.1      77.0        280.3     8.4   12588.2\nh1.m1  -1681.2      72.2 -6751.5      74.9         25.3     0.8   13502.9\n       se_waic\nh1.m6c   138.5\nh1.m6    138.5\nh1.m5    148.1\nh1.m2    156.6\nh1.m3    150.9\nh1.m3p   152.5\nh1.m4    153.9\nh1.m1    149.9\n\n# see Solomon Kurz\ncbind(waic_diff = w[,1] * -2,\n      se = w[,2] * 2)\n\n          waic_diff         se\nh1.m6c    0.0000000   0.000000\nh1.m6     0.5289111   2.917133\nh1.m5  1807.4580148 118.471557\nh1.m2  2284.3843630 134.987550\nh1.m3  2440.8062251 132.903242\nh1.m3p 2443.1522534 133.864984\nh1.m4  2447.6767321 134.678147\nh1.m1  3362.3866702 144.475730\n\n\n\n\n\n\n\n\nNote\n\n\n\nelpd_waic (expected log pointwise predictive density for WAIC): This represents the model’s predictive fit to the data. Higher values indicate a better fit.\nse_elpd_waic (standard error of elpd_waic): Measures uncertainty around the elpd_waic estimate.\nwaic: The Widely Applicable Information Criterion, a measure of model fit where lower values indicate a better fit.\nse_waic (standard error of WAIC): Uncertainty around the WAIC estimate.\nelpd_diff: The difference in the elpd_waic between the model in question and the baseline model (fit_eff_2, which has elpd_diff of 0). A negative value indicates that the model fits worse than fit_eff_2.\nse_diff: The standard error of the elpd_diff, indicating how much uncertainty there is in the difference in predictive performance.\np_waic: The number of effective parameters in the model (related to model complexity). Lower values indicate simpler models, and higher values suggest more complexity.\n\n\nPlot the comparison\n\n\n\n\n\n\n\n\n\n\nmodel_weights(h1.m1, h1.m2, h1.m3, h1.m3p, h1.m4, h1.m5, h1.m6, h1.m6c, weights = \"waic\") %&gt;% \n  round(digits = 2)\n\n h1.m1  h1.m2  h1.m3 h1.m3p  h1.m4  h1.m5  h1.m6 h1.m6c \n  0.00   0.00   0.00   0.00   0.00   0.00   0.43   0.57 \n\n\nSo as ppcheck already suggested, lognormal model indeed seem to have the most predictive power. For this particular (synthetic) data, we will now proceed with model h1.m6c\nWe will first add some mildly informative priors, and then we also try to add some interaction terms and do a comparison once again.",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-7---lognormal-with-priors",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-7---lognormal-with-priors",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 7 - lognormal with priors",
    "text": "Model 7 - lognormal with priors\nLet’s first check what priors have been selected as default for h1.m6c\n\n# Print priors\nprior_summary(h1.m6c)\n\n                  prior     class             coef         group resp dpar\n                 (flat)         b                                         \n                 (flat)         b             Big5                        \n                 (flat)         b       CommAtt2M1                        \n                 (flat)         b       CommAtt3M2                        \n                 (flat)         b Expressibility_z                        \n                 (flat)         b      Familiarity                        \n                 (flat)         b        Modality1                        \n                 (flat)         b        Modality2                        \n student_t(3, 1.3, 2.5) Intercept                                         \n   lkj_corr_cholesky(1)         L                                         \n   lkj_corr_cholesky(1)         L                        Concept          \n   lkj_corr_cholesky(1)         L                    Participant          \n   student_t(3, 0, 2.5)        sd                                         \n   student_t(3, 0, 2.5)        sd                        Concept          \n   student_t(3, 0, 2.5)        sd       CommAtt2M1       Concept          \n   student_t(3, 0, 2.5)        sd       CommAtt3M2       Concept          \n   student_t(3, 0, 2.5)        sd        Intercept       Concept          \n   student_t(3, 0, 2.5)        sd                    Participant          \n   student_t(3, 0, 2.5)        sd       CommAtt2M1   Participant          \n   student_t(3, 0, 2.5)        sd       CommAtt3M2   Participant          \n   student_t(3, 0, 2.5)        sd        Intercept   Participant          \n   student_t(3, 0, 2.5)        sd                  TrialNumber_c          \n   student_t(3, 0, 2.5)        sd        Intercept TrialNumber_c          \n   student_t(3, 0, 2.5)     sigma                                         \n nlpar lb ub       source\n                  default\n             (vectorized)\n             (vectorized)\n             (vectorized)\n             (vectorized)\n             (vectorized)\n             (vectorized)\n             (vectorized)\n                  default\n                  default\n             (vectorized)\n             (vectorized)\n        0         default\n        0    (vectorized)\n        0    (vectorized)\n        0    (vectorized)\n        0    (vectorized)\n        0    (vectorized)\n        0    (vectorized)\n        0    (vectorized)\n        0    (vectorized)\n        0    (vectorized)\n        0    (vectorized)\n        0         default\n\n\nWee can keep all defaulted ones, but we do not need to leave flat priors for the beta coefficients as we do have some assumptions/expectations\nLet’s reuse priors we have already used for h1.m3p\n\npriors_h1m7 &lt;- c(\n  set_prior(\"normal(2.5, 0.5)\", class = \"Intercept\", lb=0),\n  set_prior(\"normal(0,0.50)\", class = \"b\", coef = \"CommAtt2M1\"),\n  set_prior(\"normal(0,0.50)\", class = \"b\", coef = \"CommAtt3M2\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Modality1\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Modality2\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Big5\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Familiarity\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Expressibility_z\")\n)\n\n# The rest we will leave default (and check afterwards)\nh1.m7 &lt;- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), \n                data = final_data,\n                family = lognormal(),\n                prior = priors_h1m7,\n                iter = 4000,\n                cores = 4)\n\n\n# Add criterions for later diagnostics\nh1.m7 &lt;- add_criterion(h1.m7, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh1.m7_R2 &lt;- bayes_R2(h1.m7)\n\n# Save both as objects\nsaveRDS(h1.m7, here(\"09_Analysis_Modeling\", \"models\", \"h1.m7.rds\"))\nsaveRDS(h1.m7_R2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m7_R2.rds\"))\n\nbeep(5)\n\n\nh1.m7 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m7.rds\"))\nh1.m7_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m7_R2.rds\"))\n\n# Summary\nsummary(h1.m7)\n\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z + (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c) \n   Data: final_data (Number of observations: 5076) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                  0.01      0.00     0.00     0.02 1.00     2406\nsd(CommAtt2M1)                 0.01      0.00     0.00     0.02 1.00     5336\nsd(CommAtt3M2)                 0.01      0.01     0.00     0.03 1.00     3663\ncor(Intercept,CommAtt2M1)      0.27      0.49    -0.79     0.95 1.00     9049\ncor(Intercept,CommAtt3M2)      0.34      0.47    -0.74     0.95 1.00     7153\ncor(CommAtt2M1,CommAtt3M2)    -0.05      0.50    -0.89     0.88 1.00     8030\n                           Tail_ESS\nsd(Intercept)                  2582\nsd(CommAtt2M1)                 4860\nsd(CommAtt3M2)                 4212\ncor(Intercept,CommAtt2M1)      5794\ncor(Intercept,CommAtt3M2)      6108\ncor(CommAtt2M1,CommAtt3M2)     5937\n\n~Participant (Number of levels: 120) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                  0.05      0.00     0.04     0.06 1.00     3562\nsd(CommAtt2M1)                 0.01      0.00     0.00     0.02 1.00     5277\nsd(CommAtt3M2)                 0.01      0.01     0.00     0.03 1.00     3118\ncor(Intercept,CommAtt2M1)      0.34      0.46    -0.72     0.95 1.00    11493\ncor(Intercept,CommAtt3M2)     -0.40      0.40    -0.94     0.57 1.00     7854\ncor(CommAtt2M1,CommAtt3M2)    -0.26      0.48    -0.94     0.77 1.00     5771\n                           Tail_ESS\nsd(Intercept)                  5189\nsd(CommAtt2M1)                 4743\nsd(CommAtt3M2)                 3237\ncor(Intercept,CommAtt2M1)      6204\ncor(Intercept,CommAtt3M2)      5447\ncor(CommAtt2M1,CommAtt3M2)     6837\n\n~TrialNumber_c (Number of levels: 50) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.00      0.00     0.00     0.01 1.00     4640     4109\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            1.23      0.01     1.22     1.24 1.00     5166     5351\nCommAtt2M1           0.62      0.01     0.61     0.63 1.00    12440     6165\nCommAtt3M2          -1.10      0.01    -1.11    -1.08 1.00    10543     6212\nModality1           -0.31      0.01    -0.33    -0.30 1.00    13106     5622\nModality2            0.16      0.01     0.15     0.17 1.00    13551     6493\nBig5                 0.32      0.01     0.31     0.34 1.00     4012     4552\nFamiliarity          0.32      0.01     0.30     0.34 1.00     4337     5552\nExpressibility_z     0.12      0.00     0.12     0.13 1.00    11948     6505\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.17      0.00     0.17     0.18 1.00    15087     5361\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Coefficients remain mostly unchanged \n\n\nplot(h1.m7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# all good\n\nplot(conditional_effects(h1.m7), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the effects all go in good direction\n\npp_check(h1.m7, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# nice\n\npp_check(h1.m7, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# unchanged\n\nh1.m7_R2\n\n    Estimate   Est.Error      Q2.5     Q97.5\nR2 0.8889672 0.001399296 0.8861732 0.8916425\n\n# explained variance remains around 88%\n\nLet’s now also check whether the priors make sense",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-8---adding-interactions",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-8---adding-interactions",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 8 - adding interactions",
    "text": "Model 8 - adding interactions\nPossible interactions include:\n\nCommAtt x Modality - Effort could increase differently across modalities, depending on whether concept is guessed on the first, second or third attempt. E.g., gesture might require more effort on initial attempt, but vocal require more effort in repeated attempt. This is interesting question and it would help disentangle the benfit of multimodality over unimodality (i.e., multimodal trials might be more effortful, but overal have less attempts). However, we already model effect of modality on effort, so maybe this is not top priority.\nCommAtt x Expressibility - Higher expressibility should moderate the effect of repeated attempts, such that the increase in effort with each additional attempt is smaller (or bigger?) for more expressible concepts.This is not a priority for our analysis.\nModality × Expressibility_z - The influence of expressibility on effort could be modality-specific — perhaps effort increases less with expressibility in the combined modality. For our analysis, this is also not a priority (especially since expressibiliy has already modality encoded)\nFamiliarity x CommAtt - More familiar partners may guess faster (fewer attempts) and require less effort, but this effect could diminish over multiple attempts. For our analysis, this is not a priority,\nBig5 × Modality or Big5 × CommAtt - More open/extraverted participants might maintain higher effort over attempts, or adjust more dynamically depending on the communicative channel. While not priority, it is an interesting question if we want to tap more into the inter-individual variability.\n\nLet’s try CommAtt x Modality and Big5 x CommAtt\n\nh1.m8 &lt;- brm(Eff ~ 1 + CommAtt * Modality + Big5 * CommAtt + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), \n                data = final_data,\n                family = lognormal(),\n                prior = priors_h1m7, # we keep the priors from previous model\n                iter = 4000,\n                cores = 4)\n\n\n# Add criterions for later diagnostics\nh1.m8 &lt;- add_criterion(h1.m8, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh1.m8_R2 &lt;- bayes_R2(h1.m8)\n\n# Save both as objects\nsaveRDS(h1.m8, here(\"09_Analysis_Modeling\", \"models\", \"h1.m8.rds\"))\nsaveRDS(h1.m8_R2, here(\"09_Analysis_Modeling\", \"models\", \"h1.m8_R2.rds\"))\n\nbeep(5)\n\n\nh1.m8 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m8.rds\"))\nh1.m8_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h1.m8_R2.rds\"))\n\n\n# Summary\nsummary(h1.m8)\n\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: Eff ~ 1 + CommAtt * Modality + Big5 * CommAtt + Familiarity + Expressibility_z + (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c) \n   Data: final_data (Number of observations: 5076) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                  0.01      0.00     0.00     0.02 1.00     2549\nsd(CommAtt2M1)                 0.01      0.00     0.00     0.02 1.00     5836\nsd(CommAtt3M2)                 0.01      0.01     0.00     0.03 1.00     4322\ncor(Intercept,CommAtt2M1)      0.29      0.48    -0.76     0.94 1.00    10754\ncor(Intercept,CommAtt3M2)      0.36      0.46    -0.70     0.95 1.00     8535\ncor(CommAtt2M1,CommAtt3M2)    -0.04      0.50    -0.89     0.86 1.00     7971\n                           Tail_ESS\nsd(Intercept)                  2905\nsd(CommAtt2M1)                 4937\nsd(CommAtt3M2)                 4495\ncor(Intercept,CommAtt2M1)      5912\ncor(Intercept,CommAtt3M2)      6615\ncor(CommAtt2M1,CommAtt3M2)     6944\n\n~Participant (Number of levels: 120) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)                  0.05      0.00     0.04     0.06 1.00     3682\nsd(CommAtt2M1)                 0.01      0.00     0.00     0.02 1.00     5068\nsd(CommAtt3M2)                 0.01      0.01     0.00     0.03 1.00     3392\ncor(Intercept,CommAtt2M1)      0.34      0.45    -0.71     0.94 1.00    12364\ncor(Intercept,CommAtt3M2)     -0.41      0.39    -0.94     0.57 1.00     9996\ncor(CommAtt2M1,CommAtt3M2)    -0.26      0.49    -0.95     0.78 1.00     5862\n                           Tail_ESS\nsd(Intercept)                  5433\nsd(CommAtt2M1)                 5479\nsd(CommAtt3M2)                 3982\ncor(Intercept,CommAtt2M1)      5945\ncor(Intercept,CommAtt3M2)      6038\ncor(CommAtt2M1,CommAtt3M2)     6673\n\n~TrialNumber_c (Number of levels: 50) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.00      0.00     0.00     0.01 1.00     4900     4106\n\nRegression Coefficients:\n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nIntercept                1.23      0.01     1.22     1.24 1.00     5217\nCommAtt2M1               0.62      0.01     0.61     0.63 1.00    14494\nCommAtt3M2              -1.10      0.01    -1.11    -1.08 1.00    13959\nModality1               -0.31      0.01    -0.33    -0.30 1.00    12978\nModality2                0.16      0.01     0.15     0.18 1.00    13925\nBig5                     0.32      0.01     0.30     0.34 1.00     4506\nFamiliarity              0.32      0.01     0.30     0.34 1.00     3885\nExpressibility_z         0.12      0.00     0.12     0.13 1.00    12340\nCommAtt2M1:Modality1     0.01      0.02    -0.02     0.04 1.00    13065\nCommAtt3M2:Modality1     0.01      0.02    -0.03     0.05 1.00    12851\nCommAtt2M1:Modality2    -0.01      0.02    -0.04     0.02 1.00    13667\nCommAtt3M2:Modality2     0.02      0.02    -0.02     0.06 1.00    12109\nCommAtt2M1:Big5         -0.00      0.01    -0.02     0.02 1.00    14725\nCommAtt3M2:Big5         -0.01      0.01    -0.04     0.01 1.00    14109\n                     Tail_ESS\nIntercept                6099\nCommAtt2M1               7032\nCommAtt3M2               6248\nModality1                6662\nModality2                6627\nBig5                     5386\nFamiliarity              5579\nExpressibility_z         6403\nCommAtt2M1:Modality1     6974\nCommAtt3M2:Modality1     6884\nCommAtt2M1:Modality2     6960\nCommAtt3M2:Modality2     6404\nCommAtt2M1:Big5          6229\nCommAtt3M2:Big5          6504\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.17      0.00     0.17     0.18 1.00    17574     5831\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Coefficients remain mostly unchanged \n# Since we did not really focused on the interactions during the simulation, we also don't have strong expectations here. But for the real data, there is a good reason to expect some meaningful values\n\n\nplot(h1.m8)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# all good\n\nplot(conditional_effects(h1.m8), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the effects all go in good direction\n# we can see here that combined modality remains moderated across all commatts\n# and also big5 seems to matter the most in the second attempt\n\npp_check(h1.m8, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# the problem with predicting negative values remains\n\npp_check(h1.m8, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# unchanged\n\nh1.m8_R2\n\n    Estimate   Est.Error      Q2.5     Q97.5\nR2 0.8889759 0.001459646 0.8859485 0.8916926\n\n# explained variance remains around 87%",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#diagnostics-ii",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#diagnostics-ii",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Diagnostics II",
    "text": "Diagnostics II\n\nESS\n\neffective_sample(h1.m6) \n\n           Parameter   ESS\n1        b_Intercept  3104\n2       b_CommAtt2M1  9618\n3       b_CommAtt3M2  9509\n4        b_Modality1  9057\n5        b_Modality2  9192\n6             b_Big5  2906\n7      b_Familiarity  2280\n8 b_Expressibility_z 10811\n\neffective_sample(h1.m6c)\n\n           Parameter   ESS\n1        b_Intercept  5193\n2       b_CommAtt2M1 14438\n3       b_CommAtt3M2 12340\n4        b_Modality1 14518\n5        b_Modality2 14927\n6             b_Big5  4824\n7      b_Familiarity  4162\n8 b_Expressibility_z 14233\n\neffective_sample(h1.m7) \n\n           Parameter   ESS\n1        b_Intercept  5149\n2       b_CommAtt2M1 12363\n3       b_CommAtt3M2 10483\n4        b_Modality1 13095\n5        b_Modality2 13808\n6             b_Big5  3976\n7      b_Familiarity  4298\n8 b_Expressibility_z 11917\n\neffective_sample(h1.m8) \n\n                Parameter   ESS\n1             b_Intercept  5221\n2            b_CommAtt2M1 14165\n3            b_CommAtt3M2 13961\n4             b_Modality1 12872\n5             b_Modality2 13799\n6                  b_Big5  4484\n7           b_Familiarity  3878\n8      b_Expressibility_z 12296\n9  b_CommAtt2M1:Modality1 13103\n10 b_CommAtt3M2:Modality1 12880\n11 b_CommAtt2M1:Modality2 13651\n12 b_CommAtt3M2:Modality2 12303\n13      b_CommAtt2M1:Big5 15040\n14      b_CommAtt3M2:Big5 13963\n\n# Now h1m6 looks as the weakest, while h1m8 looks much better - but ESS for Intercept is for some reason still quite low\n\n\n\nLOO & WAIC\n\nl &lt;- loo_compare(h1.m6, h1.m6c, h1.m7, h1.m8, criterion = \"loo\")\n\nprint(l, simplify = F)\n\n       elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic   se_looic\nh1.m6c     0.0       0.0 -5070.5     69.3       126.5     3.5  10141.0   138.5 \nh1.m6     -0.2       1.5 -5070.7     69.2       120.4     3.3  10141.4   138.5 \nh1.m7     -0.3       0.2 -5070.8     69.3       126.8     3.5  10141.5   138.6 \nh1.m8     -4.4       1.9 -5074.8     69.2       132.9     3.7  10149.7   138.5 \n\n\n\nw &lt;- loo_compare(h1.m6, h1.m6c, h1.m7, h1.m8, criterion = \"waic\")\n\nprint(w, simplify = F)\n\n       elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nh1.m6c     0.0       0.0 -5070.3      69.3        126.3     3.5   10140.5\nh1.m6     -0.3       1.5 -5070.5      69.2        120.2     3.3   10141.0\nh1.m7     -0.3       0.2 -5070.5      69.3        126.5     3.5   10141.1\nh1.m8     -4.4       1.9 -5074.6      69.2        132.7     3.7   10149.2\n       se_waic\nh1.m6c   138.5\nh1.m6    138.5\nh1.m7    138.6\nh1.m8    138.5\n\n# see Solomon Kurz\ncbind(waic_diff = w[,1] * -2,\n      se = w[,2] * 2)\n\n       waic_diff        se\nh1.m6c 0.0000000 0.0000000\nh1.m6  0.5289111 2.9171328\nh1.m7  0.5582963 0.3803033\nh1.m8  8.7265395 3.7869214\n\n\nSo in terms of WAIC & LOO, interactions do not really add predictive power. This might be specific for the synthetic data, as we did not explicitly focused on the interaction coefficients there. At the same time, the difference of h1.m8 from h1.m6c is not so significant. For now, we stop here. With the full data, we will proceed with the same evaluation.",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#data-wrangling-i",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#data-wrangling-i",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Data wrangling I",
    "text": "Data wrangling I\n\n# Calculate Effort Change (Difference)\nfinal_data_2 &lt;- final_data %&gt;%\n  group_by(Participant, Concept) %&gt;%\n  mutate(\n    Effort_1 = Eff[CommAtt == 1][1],  # Effort for attempt 1\n    Effort_2 = Eff[CommAtt == 2][1],  # Effort for attempt 2\n    Effort_3 = Eff[CommAtt == 3][1],  # Effort for attempt 3\n    Effort_Change_1_to_2 = case_when(\n      CommAtt == 2 & !is.na(Effort_1) ~ Eff - Effort_1,  # Change from 1st to 2nd attempt\n      TRUE ~ NA_real_\n    ),\n    Effort_Change_2_to_3 = case_when(\n      CommAtt == 3 & !is.na(Effort_2) ~ Eff - Effort_2,  # Change from 2nd to 3rd attempt\n      TRUE ~ NA_real_\n    )\n  ) %&gt;%\n  ungroup()\n\n# Collide changes into a single column\nfinal_data_2 &lt;- final_data_2 %&gt;%\n  mutate(\n    Effort_Change = coalesce(Effort_Change_1_to_2, Effort_Change_2_to_3)\n  ) \n\n# Remove unnecessary columns\nfinal_data_2 &lt;- subset(final_data_2, select = -c(Effort_1, Effort_2, Effort_3, Effort_Change_1_to_2, Effort_Change_2_to_3)) \n\n# View the result\nhead(final_data_2, n=15)\n\n# A tibble: 15 × 11\n   Participant Concept Modality  Big5 Familiarity Expressibility CommAtt   Eff\n         &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1           1       1 combined  1.92        1.84        0.576         1  4.34\n 2           1       1 combined  1.92        1.84        0.576         2  6.57\n 3           1       1 combined  1.92        1.84        0.576         3  2.37\n 4           1       2 gesture   1.92        1.84        0.349         1  5.50\n 5           1       2 gesture   1.92        1.84        0.349         2 10.4 \n 6           1       2 gesture   1.92        1.84        0.349         3  4.00\n 7           1       3 vocal     1.92        1.84        0.340         1  6.15\n 8           1       3 vocal     1.92        1.84        0.340         2 12.9 \n 9           1       3 vocal     1.92        1.84        0.340         3  3.59\n10           1       4 combined  1.92        1.84        0.719         1  4.09\n11           1       4 combined  1.92        1.84        0.719         2  6.41\n12           1       5 gesture   1.92        1.84        0.0781        1  5.28\n13           1       6 gesture   1.92        1.84        0.00419       1  5.61\n14           1       6 gesture   1.92        1.84        0.00419       2 12.0 \n15           1       6 gesture   1.92        1.84        0.00419       3  3.44\n# ℹ 3 more variables: TrialNumber &lt;dbl&gt;, PrevAn &lt;dbl&gt;, Effort_Change &lt;dbl&gt;",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#exploring-structure",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#exploring-structure",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Exploring structure",
    "text": "Exploring structure\nThis is the relationship between effort and answer similarity (H2) as seen in the raw (synthetic) data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is how effort change is distributed\n\n\n\n\n\n\n\n\n\nSo we will have to work with bimodal distribution, as we either have a decrease in effort or increase",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#model-1---dag",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#model-1---dag",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Model 1 - DAG",
    "text": "Model 1 - DAG\nSimilarly to H1, we will start with reproducing the DAG for H2\n\nh2.m1 &lt;- brm(Effort_Change ~ 1 + PrevAn_z + CommAtt + Familiarity + Big5 + Expressibility_z + TrialNumber_c + Modality + (1 | Participant) + (1 | Concept),\n                data = filtered_data,\n                iter = 4000,\n                cores = 4)\n\n# Add criterions for later diagnostics\nh2.m1 &lt;- add_criterion(h2.m1, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh2.m1_R2 &lt;- bayes_R2(h2.m1)\n\n# Save both as objects\nsaveRDS(h2.m1, here(\"09_Analysis_Modeling\", \"models\", \"h2.m1.rds\"))\nsaveRDS(h2.m1_R2, here(\"09_Analysis_Modeling\", \"models\", \"h2.m1_R2.rds\"))\n\nbeep(5)\n\n\nh2.m1 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h2.m1.rds\"))\nh2.m1_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h2.m1_R2.rds\"))\n\n\n# Summary\nsummary(h2.m1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Effort_Change ~ 1 + PrevAn_z + CommAtt + Familiarity + Big5 + Expressibility_z + TrialNumber_c + Modality + (1 | Participant) + (1 | Concept) \n   Data: filtered_data (Number of observations: 2556) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.03      0.02     0.00     0.09 1.00     5564     4490\n\n~Participant (Number of levels: 120) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.02      0.02     0.00     0.07 1.00     6328     4353\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           -0.66      0.03    -0.71    -0.60 1.00    13909     5911\nPrevAn_z            -0.62      0.02    -0.66    -0.57 1.00    17850     5508\nCommAtt1             3.73      0.03     3.68     3.79 1.00    19981     5855\nFamiliarity          0.15      0.05     0.06     0.24 1.00    16619     5695\nBig5                 0.14      0.04     0.06     0.23 1.00    16358     5795\nExpressibility_z     0.08      0.03     0.02     0.13 1.00    17823     5427\nTrialNumber_c        0.00      0.00    -0.00     0.01 1.00    11346     5008\nModality1           -0.14      0.07    -0.28     0.00 1.00    13812     6660\nModality2            0.01      0.07    -0.12     0.15 1.00    11997     6265\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.25      0.02     1.21     1.29 1.00    19597     4922\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# coefficients seem quite conservative, even for PrevAn (but remember, it's z-scored)\n\n\nplot(h2.m1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# all looks ok\n\nplot(conditional_effects(h2.m1), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# the main effect seems ok, the rest of the predictors does not show nothing (maybe because of the transformation we lost the relations between them and the effort)\n\npp_check(h2.m1, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# this looks quite okay-ish\n\npp_check(h2.m1, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# so there seems to be quite high residual error\n\nh2.m1_R2\n\n    Estimate   Est.Error      Q2.5     Q97.5\nR2 0.8921087 0.001381664 0.8892535 0.8946349\n\n# explained variance around 89%\n\nWe see two modes in the distribution of the predictor. While Gaussian family seems to deal with it well, we could also consider model with mixture of two Gaussian distributions. This will allow each mode to have their own mean and variance. With effort change being distributed over positive as well as negative values, lognormal distribution is not applicable.\nImportantly, there are some diagnostics we should be cautious about - the relationship between predicted values and residual error is quite correlated.\nAlso notice the high explained variance indicated by R^2. In real case, this would be almost pointing to over-fitting issues. However, we created synthetic data that does not contain as much noise as we can expect from the real data, so it is quite likely that if we simply used all predictors we know to cause a variance here, we might be explaining most of the variation. It is quite likely this will not be the case for real data.",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#splines-1---fitting-b-splines",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#splines-1---fitting-b-splines",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Splines 1 - fitting b-splines",
    "text": "Splines 1 - fitting b-splines\nIn the previous model for H1, we were able to spot possible non-linear trends by contrast coding of the communicative attempts whereby each level was defined relative to the two remaining. Now, we are dealing with situation where both outcome variable and the main predictor are continuous variable. Also here we might expect nonlinear patterns in their relationship. For instance, it might be the case that for answers that are completely off, people do not feel motivated to put in more effort, and some kind of sensible ‘threshold’ needs to be reached for the effort to increase to resolve misunderstanding, and then again decrease when the answer is very similar.\nWe will use a combination of B-splines and Bayesian GAMs to prepare some models and try them fit to the synthetic data. However, note that we currently do not expect any non-linearity as we have not coded any within the simulation, as seen in the following plot fitted with loess method.\n\n\n\n\n\n\n\n\n\nB-splines allow to build up wiggly functions from more simple components which are called ‘basis functions’ (B). They divide a full range of predictor into parts and assign a parameter to each part. The parameters are gradually turned on and off in a way that makes their sum into a curve. Unlike polynomial regression, b-splines do not transform predictor, but they invent a series of new synthetic predictor variables. Each of them then exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of these synthetic variables is called a basis function B. See more in Richard McElreath’s Statistical Rethinking (McElreath (2018)).\nWe are using the code from Solomon Kurz’s adaptation of this book.\n\n# Get rid of NAs in the predictor\nd &lt;-\n  final_data_2 %&gt;% \n  drop_na(PrevAn)\n\n# And convert all that is necessary to factor/numerical\nd$CommAtt &lt;- as.factor(d$CommAtt)\nd$Modality &lt;- as.factor(d$Modality)\nd$Participant &lt;- as.factor(d$Participant)\nd$Concept &lt;- as.factor(d$Concept)\nd$TrialNumber &lt;- as.numeric(d$TrialNumber) \n\nHere we can see summary of each predictor\n\nd %&gt;% \n  select_if(is.numeric) %&gt;%  # Select only numeric columns\n  pivot_longer(cols = everything(), names_to = \"key\", values_to = \"value\") %&gt;%\n  group_by(key) %&gt;%\n  summarise(\n    mean = mean(value, na.rm = TRUE),\n    sd   = sd(value, na.rm = TRUE),\n    ll   = quantile(value, probs = 0.055, na.rm = TRUE),\n    ul   = quantile(value, probs = 0.945, na.rm = TRUE)\n  ) %&gt;%\n  mutate(across(where(is.double), round, digits = 2))\n\n# A tibble: 7 × 5\n  key             mean    sd    ll    ul\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Big5            0.98  0.57  0.09  1.93\n2 Eff             5.1   2.79  1.43  9.79\n3 Effort_Change   0.54  3.8  -5.92  4.99\n4 Expressibility  0.47  0.38  0.07  1.32\n5 Familiarity     1.13  0.55  0.15  1.9 \n6 PrevAn          0.5   0.29  0.05  0.94\n7 TrialNumber    22.6  12.4   3    42   \n\nhead(d, n=10)\n\n# A tibble: 10 × 11\n   Participant Concept Modality  Big5 Familiarity Expressibility CommAtt   Eff\n   &lt;fct&gt;       &lt;fct&gt;   &lt;fct&gt;    &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;\n 1 1           1       combined  1.92        1.84        0.576   2        6.57\n 2 1           1       combined  1.92        1.84        0.576   3        2.37\n 3 1           2       gesture   1.92        1.84        0.349   2       10.4 \n 4 1           2       gesture   1.92        1.84        0.349   3        4.00\n 5 1           3       vocal     1.92        1.84        0.340   2       12.9 \n 6 1           3       vocal     1.92        1.84        0.340   3        3.59\n 7 1           4       combined  1.92        1.84        0.719   2        6.41\n 8 1           6       gesture   1.92        1.84        0.00419 2       12.0 \n 9 1           6       gesture   1.92        1.84        0.00419 3        3.44\n10 1           8       vocal     1.92        1.84        0.591   2        9.11\n# ℹ 3 more variables: TrialNumber &lt;dbl&gt;, PrevAn &lt;dbl&gt;, Effort_Change &lt;dbl&gt;\n\n\nNow we need to specify knots that function as pivots for number of different basis functions. The B variable then tells you which knot you are close to.\n\nnum_knots &lt;- 7\nknot_list &lt;- quantile(d$PrevAn, probs = seq(from = 0, to = 1, length.out = num_knots))\nknot_list\n\n          0%    16.66667%    33.33333%          50%    66.66667%    83.33333% \n0.0001020494 0.1540585973 0.3319149296 0.5038721841 0.6719870094 0.8383128390 \n        100% \n0.9989464423 \n\n\nNow we can see how we chopped the data by the knots\n\n\n\n\n\n\n\n\n\nNow we need to specify the polynomial degree which determines how parameters interact to produce the spline.\nFor degree 1, two basis functions combine at each point. For degree 2, three functions combine at each point. For degree 3, four combine.\n\nB &lt;- bs(d$PrevAn,\n        knots = knot_list[-c(1, num_knots)], \n        degree = 3, # cubic spline\n        intercept = TRUE)\n\nThis is how cubic spline with 7 knots look like\n\n\n\n\n\n\n\n\n\nWe will now add this B matrix into the data to be able to further use it in the models\n\nd2 &lt;-\n  d %&gt;% \n  mutate(B = B) \n\n# take a look at the structure of `d3\nd2 %&gt;% glimpse()\n\nRows: 2,556\nColumns: 12\n$ Participant    &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Concept        &lt;fct&gt; 1, 1, 2, 2, 3, 3, 4, 6, 6, 8, 8, 9, 9, 14, 15, 15, 17, …\n$ Modality       &lt;fct&gt; combined, combined, gesture, gesture, vocal, vocal, com…\n$ Big5           &lt;dbl&gt; 1.919899, 1.919899, 1.919899, 1.919899, 1.919899, 1.919…\n$ Familiarity    &lt;dbl&gt; 1.8439059, 1.8439059, 1.8439059, 1.8439059, 1.8439059, …\n$ Expressibility &lt;dbl&gt; 0.576172309, 0.576172309, 0.349378655, 0.349378655, 0.3…\n$ CommAtt        &lt;fct&gt; 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 3, 2…\n$ Eff            &lt;dbl&gt; 6.566250, 2.367010, 10.367605, 3.998448, 12.934986, 3.5…\n$ TrialNumber    &lt;dbl&gt; 2, 3, 5, 6, 8, 9, 11, 14, 15, 18, 19, 21, 22, 28, 30, 3…\n$ PrevAn         &lt;dbl&gt; 0.98450637, 0.82035722, 0.48565959, 0.09090184, 0.19707…\n$ Effort_Change  &lt;dbl&gt; 2.222401, -4.199240, 4.869752, -6.369157, 6.781893, -9.…\n$ B              &lt;bs[,9]&gt; &lt;bs[26 x 9]&gt;\n\n\nThe B matrix is now a matrix column which contains the same number of rows as the others, but has also 9 columns within that columns. Each of them correspond to one synthetic B variable.\nWe can now use this matrix to fit our model\n\nh2.s1 &lt;- \n  brm(data = d2,\n      family = gaussian,\n      Effort_Change ~ 1 + B,\n      prior = c(prior(normal(100, 10), class = Intercept),\n                prior(normal(0, 10), class = b),\n                prior(exponential(1), class = sigma)),\n      iter = 4000, warmup = 2000, chains = 4, cores = 4,\n      seed = 4)\n\n\n# Add criterions for later diagnostics\nh2.s1 &lt;- add_criterion(h2.s1, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh2.s1_R2 &lt;- bayes_R2(h2.s1)\n\n# Save both as objects\nsaveRDS(h2.s1, here(\"09_Analysis_Modeling\", \"models\", \"h2.s1.rds\"))\nsaveRDS(h2.s1_R2, here(\"09_Analysis_Modeling\", \"models\", \"h2.s1_R2.rds\"))\n\nbeep(5)\n\n\nh2.s1 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h2.s1.rds\"))\nh2.s1_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h2.s1_R2.rds\"))\n\n# Summary\nprint(h2.s1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Effort_Change ~ 1 + B \n   Data: d2 (Number of observations: 2556) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.60      3.29    -5.87     6.72 1.00     1022     1772\nB1            0.54      3.32    -5.66     7.08 1.00     1043     1951\nB2            0.15      3.34    -6.13     6.70 1.00     1044     1856\nB3            1.83      3.34    -4.47     8.44 1.00     1057     1901\nB4           -0.33      3.32    -6.66     6.19 1.00     1042     1844\nB5            0.42      3.31    -5.80     6.88 1.00     1041     1814\nB6           -0.81      3.31    -7.00     5.71 1.00     1036     1880\nB7           -0.57      3.33    -6.88     5.95 1.00     1049     1780\nB8           -0.89      3.34    -7.14     5.67 1.00     1067     2043\nB9           -1.21      3.32    -7.50     5.35 1.00     1039     1822\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     3.75      0.05     3.65     3.85 1.00     4696     4264\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIt’s a bit difficult to see what is going on so let’s just plot it\n\n\n\n\n\n\n\n\n\nNow with the predictor\n\n\n\n\n\n\n\n\n\nSo as expected, we see quite linear decrease similar to our previous models\nNow let’s check some diagnostics\n\nplot(h2.s1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# looks good\n\npp_check(h2.s1, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# because we used only main predictor, we can see quite a bad result of the posterior predictive distribution, but we will work on that\n\npp_check(h2.s1, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# high residual error for both low and high values\n\nh2.s1_R2\n\n    Estimate   Est.Error       Q2.5      Q97.5\nR2 0.0299754 0.006354033 0.01827862 0.04305631\n\n# explained variance 2%",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#splines-2---bayesian-gams",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#splines-2---bayesian-gams",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Splines 2 - Bayesian GAMs",
    "text": "Splines 2 - Bayesian GAMs\nNow we will use a different workflow, using smooth functions with brms package. Brms allow for non-linear models by borrowing functions from mgcv package (Wood (2017))\nThis is how priors should look like for one of these functions\n\nget_prior(data = d2,\n          family = gaussian,\n          Effort_Change ~ 1 + s(PrevAn))\n\n                  prior     class      coef group resp dpar nlpar lb ub\n                 (flat)         b                                      \n                 (flat)         b sPrevAn_1                            \n student_t(3, 2.1, 2.7) Intercept                                      \n   student_t(3, 0, 2.7)       sds                                  0   \n   student_t(3, 0, 2.7)       sds s(PrevAn)                        0   \n   student_t(3, 0, 2.7)     sigma                                  0   \n       source\n      default\n (vectorized)\n      default\n      default\n (vectorized)\n      default\n\n\nAt this point, I will just keep priors at default\n\nh2.s2 &lt;-\n  brm(data = d2,\n      family = gaussian,\n      Effort_Change ~ 1 + s(PrevAn, bs = \"bs\", k = 19),\n      iter = 4000, \n      warmup = 2000, \n      chains = 4, \n      cores = 4,\n      seed = 4,\n      control = list(adapt_delta = .99)\n      )\n\n# Add criterions for later diagnostics\nh2.s2 &lt;- add_criterion(h2.s2, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh2.s2_R2 &lt;- bayes_R2(h2.s2)\n\n# Save both as objects\nsaveRDS(h2.s2, here(\"09_Analysis_Modeling\", \"models\", \"h2.s2.rds\"))\nsaveRDS(h2.s2_R2, here(\"09_Analysis_Modeling\", \"models\", \"h2.s2_R2.rds\"))\n\n\nbeep(5)\n\nNow we can proceed as we usually do\n\nh2.s2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h2.s2.rds\"))\nh2.s2_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h2.s2_R2.rds\"))\n\n\n# Summary\nsummary(h2.s2)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Effort_Change ~ 1 + s(PrevAn, bs = \"bs\", k = 19) \n   Data: d2 (Number of observations: 2556) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nSmoothing Spline Hyperparameters:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(sPrevAn_1)     0.04      0.04     0.00     0.15 1.00     2227     4150\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.55      0.07     0.40     0.69 1.00    10619     5777\nsPrevAn_1    -0.30      0.05    -0.39    -0.21 1.00     6524     5066\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     3.75      0.05     3.65     3.86 1.00    10144     5898\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nplot(h2.s2)\n\n\n\n\n\n\n\n# looks good\n\nplot(conditional_effects(h2.s2), points = TRUE)\n\n\n\n\n\n\n\n# again, we see similar (non-linear) trend \n\npp_check(h2.s2, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# same as before, ignoring bimodality\n\npp_check(h2.s2, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# still weird\n\nh2.s2_R2\n\n     Estimate   Est.Error       Q2.5      Q97.5\nR2 0.02581913 0.006114862 0.01489172 0.03874999\n\n# explained variance 2%",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#splines-3---gams-with-fr-effects",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#splines-3---gams-with-fr-effects",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Splines 3 - GAMs with f+r effects",
    "text": "Splines 3 - GAMs with f+r effects\nNow let’s use the same model, but adding our other predictors\n\nh2.s3 &lt;- \n  brm(\n    data = filtered_data,\n    family = gaussian,\n    Effort_Change ~ 1 +\n      s(PrevAn_z, bs = \"bs\", k = 19) +  # Smooth for Previous Answer similarity\n      + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  # Fixed effects\n      (1 | Participant) + (1 | Concept),  # Random effects\n    iter = 4000, \n    warmup = 2000, \n    chains = 4, \n    cores = 4,\n    seed = 4,\n    control = list(adapt_delta = .99)\n  )\n\n\n# Add criterions for later diagnostics\nh2.s3 &lt;- add_criterion(h2.s3, criterion = c(\"loo\", \"waic\"))\n\n# Calculate also variance explained (R^2)\nh2.s3_R2 &lt;- bayes_R2(h2.s3)\n\n# Save both as objects\nsaveRDS(h2.s3, here(\"09_Analysis_Modeling\", \"models\", \"h2.s3.rds\"))\nsaveRDS(h2.s3_R2, here(\"09_Analysis_Modeling\", \"models\", \"h2.s3_R2.rds\"))\n\nbeep(5)\n\n\nh2.s3 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h2.s3.rds\"))\nh2.s3_R2 &lt;- readRDS(here(\"09_Analysis_Modeling\", \"models\", \"h2.s3_R2.rds\"))\n\n\n# Summary\nsummary(h2.s3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Effort_Change ~ 1 + s(PrevAn_z, bs = \"bs\", k = 19) + +CommAtt + Modality + Big5 + Familiarity + Expressibility_z + (1 | Participant) + (1 | Concept) \n   Data: filtered_data (Number of observations: 2556) \n  Draws: 4 chains, each with iter = 4000; warmup = 2000; thin = 1;\n         total post-warmup draws = 8000\n\nSmoothing Spline Hyperparameters:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(sPrevAn_z_1)     0.01      0.01     0.00     0.03 1.00     3075     4614\n\nMultilevel Hyperparameters:\n~Concept (Number of levels: 21) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.03      0.02     0.00     0.08 1.00     5219     4819\n\n~Participant (Number of levels: 120) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.02      0.02     0.00     0.07 1.00     6622     4876\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           -0.66      0.03    -0.72    -0.61 1.00    15652     5457\nCommAtt1             3.73      0.03     3.68     3.78 1.00    20938     5112\nModality1           -0.14      0.07    -0.27    -0.00 1.00    14396     6300\nModality2            0.01      0.07    -0.12     0.15 1.00    15298     7228\nBig5                 0.14      0.04     0.05     0.23 1.00    18750     5647\nFamiliarity          0.15      0.05     0.06     0.24 1.00    22307     5918\nExpressibility_z     0.08      0.03     0.02     0.14 1.00    21298     5485\nsPrevAn_z_1         -0.32      0.01    -0.35    -0.30 1.00    17636     6556\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.25      0.02     1.21     1.28 1.00    22077     5247\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nplot(h2.s3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# looks ok\n\nplot(conditional_effects(h2.s3), points = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# now the ffect of PrevAn looks a bit different because we are back to using z-scored version\n\npp_check(h2.s3, type = \"dens_overlay\")\n\n\n\n\n\n\n\n# looks ok but the ppd undershoots the low values and overshoot the high values, similar to the linear regression\n\npp_check(h2.s3, type = \"error_scatter_avg\")\n\n\n\n\n\n\n\n# still not what we would like\n\nh2.s3_R2\n\n   Estimate  Est.Error      Q2.5     Q97.5\nR2 0.892084 0.00136303 0.8893236 0.8945848\n\n# 89% variance",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#diagnostics-i-1",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#diagnostics-i-1",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Diagnostics I",
    "text": "Diagnostics I\nBefore we proceed further, let’s do first round of diagnostics\n\nRhat\n\n# Extract R-hat values for each model\nrhat_list &lt;- lapply(model_list, function(model) {\n  rhat_values &lt;- rhat(model)\n  data.frame(model = deparse(substitute(model)), \n             max_rhat = max(rhat_values), \n             min_rhat = min(rhat_values))\n})\n\n# Combine and inspect\ndo.call(rbind, rhat_list)\n\n   model max_rhat  min_rhat\n1 X[[i]] 1.003546 0.9996569\n2 X[[i]] 1.003278 1.0003198\n3 X[[i]] 1.001547 1.0000413\n4 X[[i]] 1.002112 0.9996489\n\n\nAll Rhat values look good\n\n\nESS\nEffective sample size tells how many independent samples the model has effectively drawn from the PD. Low ESS suggests autocorrelation (i.e., sample explores one part of posterior), while high ESS means good mix\n\n# Extract n_eff values for each model\nneff_ratio_list &lt;- lapply(model_list, function(model) {\n  neff_values &lt;- neff_ratio(model)              # Here we calculate ratio (not the raw number of effective samples)\n  data.frame(model = deparse(substitute(model)), \n             min_neff = min(neff_values), \n             max_neff = max(neff_values),\n             mean_neff = mean(neff_values))\n               \n})\n\n# Combine and inspect\ndo.call(rbind, neff_ratio_list)\n\n   model  min_neff  max_neff mean_neff\n1 X[[i]] 0.3290114 0.9137058 0.8042730\n2 X[[i]] 0.1277981 0.5329452 0.2049973\n3 X[[i]] 0.2784304 0.7904525 0.6233618\n4 X[[i]] 0.3166404 1.0163927 0.8098956\n\n\nThey all look good expect the very first non-linear model h2.s1 which is quite expectable since we used only main predictor. Linear regression model and GAMs model have highest ESS.\n\neffective_sample(h2.m1) \n\n           Parameter   ESS\n1        b_Intercept 13719\n2         b_PrevAn_z 17596\n3         b_CommAtt1 19690\n4      b_Familiarity 16236\n5             b_Big5 16183\n6 b_Expressibility_z 18846\n7    b_TrialNumber_c 11127\n8        b_Modality1 13781\n9        b_Modality2 11755\n\neffective_sample(h2.s3) \n\n           Parameter   ESS\n1        b_Intercept 15467\n2         b_CommAtt1 20347\n3        b_Modality1 14550\n4        b_Modality2 15207\n5             b_Big5 18402\n6      b_Familiarity 22096\n7 b_Expressibility_z 20936\n8     bs_sPrevAn_z_1 17735\n\n\n\n\nLOO & WAIC\n\nl &lt;- loo_compare(h2.m1, h2.s1, h2.s2, h2.s3, criterion = \"loo\")\n\nprint(l, simplify = F)\n\n      elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic   se_looic\nh2.m1     0.0       0.0 -4201.7     48.1        14.8     0.7   8403.4    96.2 \nh2.s3    -0.1       0.7 -4201.8     48.1        15.2     0.7   8403.6    96.3 \nh2.s2 -2808.1      38.3 -7009.8     26.4         4.8     0.1  14019.6    52.7 \nh2.s1 -2810.8      38.3 -7012.5     26.6         9.6     0.3  14025.0    53.3 \n\n\nHere again we see that GAM and LR model have best performance assessed by LOO.\n\nw &lt;- loo_compare(h2.m1, h2.s1, h2.s2, h2.s3, criterion = \"waic\")\n\nprint(w, simplify = F)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nh2.m1     0.0       0.0 -4201.7      48.1         14.8     0.7    8403.4\nh2.s3    -0.1       0.7 -4201.8      48.1         15.2     0.7    8403.5\nh2.s2 -2808.1      38.3 -7009.8      26.4          4.8     0.1   14019.6\nh2.s1 -2810.8      38.3 -7012.5      26.6          9.6     0.3   14025.0\n      se_waic\nh2.m1    96.2\nh2.s3    96.3\nh2.s2    52.7\nh2.s1    53.3\n\n# see Solomon Kurz\ncbind(waic_diff = w[,1] * -2,\n      se = w[,2] * 2)\n\n         waic_diff        se\nh2.m1    0.0000000  0.000000\nh2.s3    0.1437139  1.412563\nh2.s2 5616.2358401 76.619424\nh2.s1 5621.6500755 76.529817\n\n\nPlot the comparison\n\n\n\n\n\n\n\n\n\nHere we see identical results\n\nmodel_weights(h2.m1, h2.s1, h2.s2, h2.s3, weights = \"waic\") %&gt;% \n  round(digits = 2)\n\nh2.m1 h2.s1 h2.s2 h2.s3 \n 0.52  0.00  0.00  0.48 \n\n\nFor the synthetic data, GAMs are probably adding unnecessary complexity as we are not gaining much more explanatory power. However, we leave it open whether linear or non-linear models will be better suited for the real data, following this pipeline.",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#adding-priors",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#adding-priors",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Adding priors",
    "text": "Adding priors\nSimilarly to H1, we will also set mildly informative priors. Let’s check what priors have been selected as default for h2.s3\n\n# Print priors\nprior_summary(h2.s3)\n\n                  prior     class                           coef       group\n                 (flat)         b                                           \n                 (flat)         b                           Big5            \n                 (flat)         b                       CommAtt1            \n                 (flat)         b               Expressibility_z            \n                 (flat)         b                    Familiarity            \n                 (flat)         b                      Modality1            \n                 (flat)         b                      Modality2            \n                 (flat)         b                    sPrevAn_z_1            \n student_t(3, 2.1, 2.7) Intercept                                           \n   student_t(3, 0, 2.7)        sd                                           \n   student_t(3, 0, 2.7)        sd                                    Concept\n   student_t(3, 0, 2.7)        sd                      Intercept     Concept\n   student_t(3, 0, 2.7)        sd                                Participant\n   student_t(3, 0, 2.7)        sd                      Intercept Participant\n   student_t(3, 0, 2.7)       sds                                           \n   student_t(3, 0, 2.7)       sds s(PrevAn_z, bs = \"bs\", k = 19)            \n   student_t(3, 0, 2.7)     sigma                                           \n resp dpar nlpar lb ub       source\n                            default\n                       (vectorized)\n                       (vectorized)\n                       (vectorized)\n                       (vectorized)\n                       (vectorized)\n                       (vectorized)\n                       (vectorized)\n                            default\n                  0         default\n                  0    (vectorized)\n                  0    (vectorized)\n                  0    (vectorized)\n                  0    (vectorized)\n                  0         default\n                  0    (vectorized)\n                  0         default\n\n\nFor the flat priors that have been selected as default, we can re-use our previous priors from H1. The priors could therefore look somewhat like this\n\npriors_h2s4 &lt;- c(\n  set_prior(\"normal(0,0.50)\", class = \"b\", coef = \"sPrevAn_z_1\"),\n  set_prior(\"normal(0,0.50)\", class = \"b\", coef = \"CommAtt2M1\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Modality1\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Modality2\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Big5\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Familiarity\"),\n  set_prior(\"normal(0,0.25)\", class = \"b\", coef = \"Expressibility_z\")\n)",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "09_Analysis_Modeling/Modelling_syntheticData.html#adding-interactions",
    "href": "09_Analysis_Modeling/Modelling_syntheticData.html#adding-interactions",
    "title": "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort",
    "section": "Adding Interactions",
    "text": "Adding Interactions\nBecause we are currently already reaching the ceiling of R^2, we are not going to add more interactions. However, we are not excluding the option of adding few interactions when using the real data. Possible interactions include:\n\nPrevAn x Modality - similarity affects the change in effort differently (e.g., vocal might still require more effort)\nPrevAn x Expressibility - similarity in relation to effort might matter only for highly expressible concepts, and low expressible concepts are difficult to express, so also difficult to exaggerate\nFamiliarity x PrevAn - if the guess is really bad, only very familiar people might be motivated enough to put more effort\nBig x PrevAn - same like with familiarity\n\nSimilar to the workflow adopted in H1 modelling, we would fit two new models, one with priors and one with interactions, and perform another round of diagnostics to see which model seems to have the most predictive power.",
    "crumbs": [
      "Analysis",
      "Statistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort"
    ]
  },
  {
    "objectID": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html",
    "href": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html",
    "title": "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution",
    "section": "",
    "text": "This script is an adaption of pipeline by Ćwiek et al. (n.d.).\nIn this script, we are going to use eXtreme Gradient Boosting (XGBoost, Chen and Guestrin (2016)) to identify effort-related features beyond those investigated within our confirmatory analysis, i.e., torque change, amplitude envelope, and change in center of pressure.\nXGBoost is a machine learning algorithm that builds an ensemble of decision trees to predict an outcome based on input features. For us, outcome refers to communicative attempt (baseline, first correction, second correction) and input features refer to all features of effort collected in Feature extraction script.\nWe are going to use separate models for each modality, as we expect differences in how (significantly) features contribute to resolving misunderstanding in first and second correction. Finally, when having list of features ordered by their importance in predicting communicative attempt, we will combine the XGBoost-computed importance with PCA analysis we have performed in this script. This is to ensure we pick features from uncorrelated dimensions and thus cover more explanatory planes of effort.\nNote that the current version of the script is used with data only from dyad 0. Since this is not sufficient amount of data for any meaningful conclusions, this script serves for building the workflow. We will use identical pipeline with the full dataset, and any deviations from this script will be reported.\n\n\nCode to prepare the environment\nparentfolder &lt;- dirname(getwd())\n\ndatasets      &lt;- paste0(parentfolder, '/08_Analysis_XGBoost/datasets/')\nmodels        &lt;- paste0(parentfolder, '/08_Analysis_XGBoost/models/')\nplots         &lt;- paste0(parentfolder, '/08_Analysis_XGBoost/plots/')\n\n# Packages \nlibrary(tibble) # Data Manipulation\nlibrary(stringr)\nlibrary(tidyverse) # includes readr, tidyr, dplyr, ggplot2\nlibrary(data.table)\n \nlibrary(ggforce) # Plotting\nlibrary(ggpubr)\nlibrary(gridExtra)\n\nlibrary(rpart) # Random Forests and XGBoost\nlibrary(rpart.plot)\nlibrary(ranger)\nlibrary(tuneRanger)\nlibrary(caret)\nlibrary(xgboost)\nlibrary(parallel)\nlibrary(mice)\nlibrary(doParallel)\n\n# Use all available cores for parallel computing\noptions(mc.cores = parallel::detectCores())\n\ncolorBlindBlack8  &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n                       \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution"
    ]
  },
  {
    "objectID": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#random-forests",
    "href": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#random-forests",
    "title": "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution",
    "section": "Random forests",
    "text": "Random forests\nWe will build a random forest first.\n\n# prepare predictors\npredictors &lt;- setdiff(names(data_ges), \"correction_info\")\n\nformula_str &lt;- paste(\"correction_info ~\", paste(predictors, collapse = \" + \"))\n\n# Convert the formula string to a formula object\ngesTree_formula &lt;- as.formula(formula_str)\n\n# Now use the formula in rpart\ngesTree &lt;- rpart(formula = gesTree_formula, data = data_ges, \n                method='class', # Specify that it's a classification tree\n                control = rpart.control(maxdepth = 5)  # Control parameters for the 'rpart' function\n)\n\nprp(\n  gesTree,         # The decision tree object to be visualized\n  extra = 1,      # Show extra information (like node statistics) in the plot\n  varlen = 0,     # Length of variable names (0 means auto-determined)\n  faclen = 0     # Length of factor levels displayed on the plot (increase as needed)\n)\n\n\n\n\n\n\n\n\nSplit the data\n\n# This method should ensure that all levels of our dependent variable are present in both sets\n# Ensure each level is present in both sets\ntrain_data &lt;- data_ges %&gt;%\n  group_by(correction_info) %&gt;%\n  sample_frac(0.8, replace = FALSE) %&gt;%\n  ungroup()\n\n# Assign the remaining samples to the test set\ntest_data &lt;- anti_join(data_ges, train_data)\n\nBuilding the untuned model.\n\n# Untuned Model with importance (permutation) option set\ngesUntuned &lt;- ranger(\n  y = train_data$correction_info,\n  x = train_data[,0:324],\n  num.trees = 500,\n  importance = \"permutation\"\n)\n\npredictions &lt;- predict(gesUntuned, data = test_data)$predictions\n\n# Create a confusion matrix\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$correction_info)\n\n# Print the confusion matrix\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction c0 c0_only c1 c2\n   c0       0       2  1  0\n   c0_only  1       0  0  0\n   c1       0       0  0  1\n   c2       0       0  0  0\n\nOverall Statistics\n                                     \n               Accuracy : 0          \n                 95% CI : (0, 0.5218)\n    No Information Rate : 0.4        \n    P-Value [Acc &gt; NIR] : 1          \n                                     \n                  Kappa : -0.3158    \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: c0 Class: c0_only Class: c1 Class: c2\nSensitivity              0.000         0.0000     0.000       0.0\nSpecificity              0.250         0.6667     0.750       1.0\nPos Pred Value           0.000         0.0000     0.000       NaN\nNeg Pred Value           0.500         0.5000     0.750       0.8\nPrevalence               0.200         0.4000     0.200       0.2\nDetection Rate           0.000         0.0000     0.000       0.0\nDetection Prevalence     0.600         0.2000     0.200       0.0\nBalanced Accuracy        0.125         0.3333     0.375       0.5\n\n# Calculate feature importance\nfeature_importance &lt;- importance(gesUntuned, num.threads = 1, type = 1) \n\n# Convert to data frame\nfeature_importance &lt;- as.data.frame(feature_importance, stringsAsFactors = FALSE)\nfeature_importance$Feature &lt;- rownames(feature_importance)\ncolnames(feature_importance) &lt;- c(\"Importance\", \"Feature\")\n\n# Sort by importance\nsorted_feature_importance &lt;- feature_importance[order(-feature_importance$Importance), ]\n\n# Print sorted feature importance\nhead(sorted_feature_importance, n=10)\n\n                              Importance                      Feature\nbody_slope_95                0.005932540                body_slope_95\nhead_power_range             0.005242857             head_power_range\narm_speedKin_sum_pospeak_std 0.003863492 arm_speedKin_sum_pospeak_std\narm_speedKin_sum_pospeak_n   0.003602381   arm_speedKin_sum_pospeak_n\narm_inter_Kin                0.003116667                arm_inter_Kin\narm_accKin_sum_integral      0.002766667      arm_accKin_sum_integral\nduration_mov                 0.002723810                 duration_mov\nbody_nComp_95                0.002709524                body_nComp_95\nhead_angSpeed_sum_range      0.002538095      head_angSpeed_sum_range\narm_accKin_sum_pospeak_std   0.002257143   arm_accKin_sum_pospeak_std\n\n\nSet the parameters for the random forest.\n\n# Define the number of CPU cores to use\nnum_cores &lt;- detectCores()\n\n# Create a cluster with specified number of cores\ncl &lt;- makeCluster(num_cores)\n\nTuning the random forest.\n\ntuneGes &lt;- makeClassifTask(data = data_ges[,0:325],\n                           target = \"correction_info\")\n\ntuneGes &lt;- tuneRanger(tuneGes,\n                      measure = list(multiclass.brier),\n                      num.trees = 500)\n\n# Return hyperparameter values\n#tuneGes\n\n# Recommended parameter settings: \n#   mtry min.node.size sample.fraction\n# 1   57             4       0.2279307\n# Results: \n#   multiclass.brier exec.time\n# 1         0.790973     0.164\n\ngesTuned &lt;- ranger(\n  y = train_data$correction_info,\n  x = train_data[,0:324], \n  num.trees = 5000, \n  mtry = 57, # Set the recommended mtry value (number of features).\n  min.node.size = 4, # Set the recommended min.node.size value (number of samples before a node terminates).\n  sample.fraction = 0.2279307, # Set the recommended sample fraction value.(% of data for bagging).\n  importance = \"permutation\" # Permutation is a computationally intensive test.\n)\n\npredictions &lt;- predict(gesTuned, data = test_data)$predictions\n\n# Create a confusion matrix\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$correction_info)\n\n# Print the confusion matrix\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction c0 c0_only c1 c2\n   c0       0       0  0  0\n   c0_only  1       2  1  1\n   c1       0       0  0  0\n   c2       0       0  0  0\n\nOverall Statistics\n                                          \n               Accuracy : 0.4             \n                 95% CI : (0.0527, 0.8534)\n    No Information Rate : 0.4             \n    P-Value [Acc &gt; NIR] : 0.663           \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: c0 Class: c0_only Class: c1 Class: c2\nSensitivity                0.0            1.0       0.0       0.0\nSpecificity                1.0            0.0       1.0       1.0\nPos Pred Value             NaN            0.4       NaN       NaN\nNeg Pred Value             0.8            NaN       0.8       0.8\nPrevalence                 0.2            0.4       0.2       0.2\nDetection Rate             0.0            0.4       0.0       0.0\nDetection Prevalence       0.0            1.0       0.0       0.0\nBalanced Accuracy          0.5            0.5       0.5       0.5\n\n# Calculate feature importance\nfeature_importance &lt;- importance(gesTuned, num.threads = 1, type = 1) \n\n# Convert to data frame\nfeature_importance &lt;- as.data.frame(feature_importance, stringsAsFactors = FALSE)\nfeature_importance$Feature &lt;- rownames(feature_importance)\ncolnames(feature_importance) &lt;- c(\"Importance\", \"Feature\")\n\n# Sort by importance\nsorted_feature_importance &lt;- feature_importance[order(-feature_importance$Importance), ]\n\n# Print sorted feature importance\nhead(sorted_feature_importance, n=10)\n\n                    Importance             Feature\narm_duration                 0        arm_duration\narm_inter_Kin                0       arm_inter_Kin\narm_inter_IK                 0        arm_inter_IK\narm_bbmv                     0            arm_bbmv\nlowerbody_duration           0  lowerbody_duration\nlowerbody_inter_Kin          0 lowerbody_inter_Kin\nlowerbody_inter_IK           0  lowerbody_inter_IK\nlowerbody_bbmv               0      lowerbody_bbmv\nleg_duration                 0        leg_duration\nleg_inter_Kin                0       leg_inter_Kin\n\n# Close the cluster when you're done with your parallel tasks\n#stopCluster(cl)\n\nCreate a tuned model only.\n\n# Create a classification task for tuning\ntuneGes &lt;- makeClassifTask(data = train_data[, 0:325], target = \"correction_info\")\n\n# Tune the model\ntuneGes &lt;- tuneRanger(tuneGes, measure = list(multiclass.brier), num.trees = 500)\n\n# Return hyperparameter values\n#tuneGes\n\n# Recommended parameter settings: \n#   mtry min.node.size sample.fraction\n# 1  221             3       0.2056429\n# Results: \n#   multiclass.brier exec.time\n# 1        0.8124712     0.168\n\n# Fit the tuned model on the training data\ngesTuned &lt;- ranger(\n  y = train_data$correction_info,\n  x = train_data[, 0:324],\n  num.trees = 5000,\n  mtry = 221,\n  min.node.size = 3,\n  sample.fraction = 0.2056429,\n  importance = \"permutation\"\n)\n\n# Predict on the test data\npredictions &lt;- predict(gesTuned, data = test_data)$predictions\n\n# Create a confusion matrix\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$correction_info)\n\n# Print the confusion matrix\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction c0 c0_only c1 c2\n   c0       0       0  0  0\n   c0_only  1       2  1  1\n   c1       0       0  0  0\n   c2       0       0  0  0\n\nOverall Statistics\n                                          \n               Accuracy : 0.4             \n                 95% CI : (0.0527, 0.8534)\n    No Information Rate : 0.4             \n    P-Value [Acc &gt; NIR] : 0.663           \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: c0 Class: c0_only Class: c1 Class: c2\nSensitivity                0.0            1.0       0.0       0.0\nSpecificity                1.0            0.0       1.0       1.0\nPos Pred Value             NaN            0.4       NaN       NaN\nNeg Pred Value             0.8            NaN       0.8       0.8\nPrevalence                 0.2            0.4       0.2       0.2\nDetection Rate             0.0            0.4       0.0       0.0\nDetection Prevalence       0.0            1.0       0.0       0.0\nBalanced Accuracy          0.5            0.5       0.5       0.5\n\n# Calculate feature importance\nfeature_importance &lt;- importance(gesTuned, num.threads = 1, type = 1)\n\n# Convert to data frame\nfeature_importance_df &lt;- as.data.frame(feature_importance, stringsAsFactors = FALSE)\nfeature_importance_df$Feature &lt;- rownames(feature_importance_df)\ncolnames(feature_importance_df) &lt;- c(\"Importance\", \"Feature\")\n\n# Sort by importance\nsorted_feature_importance &lt;- feature_importance_df[order(-feature_importance_df$Importance), ]\n\n# Print sorted feature importance\nhead(sorted_feature_importance, n=10)\n\n                    Importance             Feature\narm_duration                 0        arm_duration\narm_inter_Kin                0       arm_inter_Kin\narm_inter_IK                 0        arm_inter_IK\narm_bbmv                     0            arm_bbmv\nlowerbody_duration           0  lowerbody_duration\nlowerbody_inter_Kin          0 lowerbody_inter_Kin\nlowerbody_inter_IK           0  lowerbody_inter_IK\nlowerbody_bbmv               0      lowerbody_bbmv\nleg_duration                 0        leg_duration\nleg_inter_Kin                0       leg_inter_Kin\n\n# Close the cluster when you're done with your parallel tasks\n#stopCluster(cl)\n\nSave data frame.\n\nwrite.csv(data_ges, file = paste0(datasets, \"gesDataXGB.csv\"), row.names = FALSE)",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution"
    ]
  },
  {
    "objectID": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#xgboost",
    "href": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#xgboost",
    "title": "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution",
    "section": "XGBoost",
    "text": "XGBoost\nEnsure parallel processing.\n\n# Detect the number of available cores\ncores &lt;- detectCores() #- 1  # Leave one core free\n\n# Create a cluster with the detected number of cores\ncl &lt;- makeCluster(cores)\n\n# Register the parallel backend\nregisterDoParallel(cl)\n\nDefine the grid and estimate runtime.\n\ngrid_tune &lt;- expand.grid(\n  nrounds = c(5000, 10000), \n  max_depth = c(3, 6), \n  eta = c(0.05, 0.1), \n  gamma = c(0.1), \n  colsample_bytree = c(0.6, 0.8), \n  min_child_weight = c(1), \n  subsample = c(0.75, 1.0)\n)\n\n# Calculate total combinations\ntotal_combinations &lt;- nrow(grid_tune)\n\n# Estimate single model run time (assume 1 minute per run)\nsingle_model_time &lt;- 10 # minute\n\n# Total runs for cross-validation\nfolds &lt;- 5\ntotal_runs &lt;- total_combinations * folds\n\n# Total time estimation without parallel processing\ntotal_time &lt;- total_runs * single_model_time # in minutes\n\n# Convert to hours\ntotal_time_hours &lt;- total_time / 60\n\n# Output estimated time without parallel processing\nprint(paste(\"Estimated time for grid search without parallel processing:\", total_time_hours, \"hours\"))\n\n[1] \"Estimated time for grid search without parallel processing: 26.6666666666667 hours\"\n\n# Parallel processing with 4 cores\ncores &lt;- 24\ntotal_time_parallel &lt;- total_time / cores # in minutes\n\n# Convert to hours\ntotal_time_parallel_hours &lt;- total_time_parallel / 60\n\n# Output estimated time with parallel processing\nprint(paste(\"Estimated time for grid search with\", cores, \"cores:\", total_time_parallel_hours, \"hours\"))\n\n[1] \"Estimated time for grid search with 24 cores: 1.11111111111111 hours\"\n\nrm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)\n\n\nK-fold cross-validation\nCreate subsets to train and test data (80/20).\n\n# Set seed for reproducibility\nset.seed(998)\n\n# Set up train control\ntrain_control &lt;- trainControl(\n  method = \"cv\",        # Cross-validation\n  number = 5,           # 5-fold cross-validation\n  allowParallel = TRUE  # Enable parallel processing\n)\n\n# Define the number of subsets\nnumSubsets &lt;- 5\n\n# Load data if needed\ngesDataXGB &lt;- read_csv(paste0(datasets, \"gesDataXGB.csv\"))\n\n# Ensure 'correction_info' is a factor\ngesDataXGB$correction_info &lt;- as.factor(gesDataXGB$correction_info)\n\n# Remove rows with only NA values\ngesDataXGB &lt;- gesDataXGB[rowSums(is.na(gesDataXGB)) &lt; ncol(gesDataXGB), ]\n\n# Split data by levels of 'correction_info'\ncorrection_levels &lt;- levels(gesDataXGB$correction_info)\nsplit_data &lt;- split(gesDataXGB, gesDataXGB$correction_info)\n\n# Initialize a list to store subsets\ngesSubsets &lt;- vector(\"list\", length = numSubsets)\n\n# Distribute rows for each level equally across subsets\nfor (level in correction_levels) {\n  level_data &lt;- split_data[[level]]\n  subset_sizes &lt;- rep(floor(nrow(level_data) / numSubsets), numSubsets)\n  remainder &lt;- nrow(level_data) %% numSubsets\n  \n  # Distribute remainder rows randomly\n  if (remainder &gt; 0) {\n    subset_sizes[seq_len(remainder)] &lt;- subset_sizes[seq_len(remainder)] + 1\n  }\n  \n  # Shuffle rows of the level and assign to subsets\n  shuffled_data &lt;- level_data[sample(nrow(level_data)), ]\n  indices &lt;- cumsum(c(0, subset_sizes))\n  \n  for (i in 1:numSubsets) {\n    if (is.null(gesSubsets[[i]])) {\n      gesSubsets[[i]] &lt;- shuffled_data[(indices[i] + 1):indices[i + 1], ]\n    } else {\n      gesSubsets[[i]] &lt;- rbind(gesSubsets[[i]], shuffled_data[(indices[i] + 1):indices[i + 1], ])\n    }\n  }\n}\n\n# Naming the subsets\nnames(gesSubsets) &lt;- paste0(\"gesData\", 1:numSubsets)\n\n# Verify balance in subsets\nfor (i in 1:numSubsets) {\n  cat(\"Subset\", i, \"contains rows:\", nrow(gesSubsets[[i]]), \"and levels:\\n\")\n  print(table(gesSubsets[[i]]$correction_info))\n}\n\nSubset 1 contains rows: 5 and levels:\n\n     c0 c0_only      c1      c2 \n      1       2       1       1 \nSubset 2 contains rows: 5 and levels:\n\n     c0 c0_only      c1      c2 \n      1       2       1       1 \nSubset 3 contains rows: 5 and levels:\n\n     c0 c0_only      c1      c2 \n      1       2       1       1 \nSubset 4 contains rows: 5 and levels:\n\n     c0 c0_only      c1      c2 \n      1       1       1       1 \nSubset 5 contains rows: 5 and levels:\n\n     c0 c0_only      c1      c2 \n      1       1       1       1 \n\n# Remove any rows with only NAs from subsets just to ensure cleanliness\ngesSubsets &lt;- lapply(gesSubsets, function(subset) {\n  subset[rowSums(is.na(subset)) &lt; ncol(subset), ]\n})\n\n# Access the subsets\ngesData1 &lt;- gesSubsets$gesData1\ngesData2 &lt;- gesSubsets$gesData2\ngesData3 &lt;- gesSubsets$gesData3\ngesData4 &lt;- gesSubsets$gesData4\ngesData5 &lt;- gesSubsets$gesData5\n\n# Combine subsets into 80% groups\ngesData1234 &lt;- rbind(gesData1, gesData2, gesData3, gesData4)\ngesData1235 &lt;- rbind(gesData1, gesData2, gesData3, gesData5)\ngesData1245 &lt;- rbind(gesData1, gesData2, gesData4, gesData5)\ngesData1345 &lt;- rbind(gesData1, gesData3, gesData4, gesData5)\ngesData2345 &lt;- rbind(gesData2, gesData3, gesData4, gesData5)\n\n# Final verification of all levels in the combined datasets\ncombined_sets &lt;- list(gesData1234, gesData1235, gesData1245, gesData1345, gesData2345)\nnames(combined_sets) &lt;- c(\"gesData1234\", \"gesData1235\", \"gesData1245\", \"gesData1345\", \"gesData2345\")\n\nfor (set_name in names(combined_sets)) {\n  cat(\"Dataset\", set_name, \"contains rows:\", nrow(combined_sets[[set_name]]), \"and levels:\\n\")\n  print(table(combined_sets[[set_name]]$correction_info))\n}\n\nDataset gesData1234 contains rows: 19 and levels:\n\n     c0 c0_only      c1      c2 \n      4       7       4       4 \nDataset gesData1235 contains rows: 19 and levels:\n\n     c0 c0_only      c1      c2 \n      4       7       4       4 \nDataset gesData1245 contains rows: 18 and levels:\n\n     c0 c0_only      c1      c2 \n      4       6       4       4 \nDataset gesData1345 contains rows: 18 and levels:\n\n     c0 c0_only      c1      c2 \n      4       6       4       4 \nDataset gesData2345 contains rows: 18 and levels:\n\n     c0 c0_only      c1      c2 \n      4       6       4       4 \n\n\n\n\nModels\nOnly run the models one time and then readRDS.\n\nModel 1\n\ngesModel1 &lt;- caret::train(\n  correction_info ~ .,              \n  data = gesData1234,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(gesModel1, file = paste0(models, \"gesModel1.rds\"), compress = TRUE)\n\n\n\nModel 2\n\ngesModel2 &lt;- caret::train(\n  correction_info ~ .,              \n  data = gesData1235,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(gesModel2, file = paste0(models, \"gesModel2.rds\"), compress = TRUE)\n\n\n\nModel 3\n\ngesModel3 &lt;- caret::train(\n  correction_info ~ .,              \n  data = gesData1245,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(gesModel3, file = paste0(models, \"gesModel3.rds\"), compress = TRUE)\n\n\n\nModel 4\n\ngesModel4 &lt;- caret::train(\n  correction_info ~ .,              \n  data = gesData1345,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\nsaveRDS(gesModel4, file = paste0(models, \"gesModel4.rds\"), compress = TRUE)\n\n\n\nModel 5\n\ngesModel5 &lt;- caret::train(\n  correction_info ~ .,              \n  data = gesData2345,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(gesModel5, file = paste0(models, \"gesModel5.rds\"), compress = TRUE)\n\n\n\nLoad models\nLoad all models after running, if necessary.\n\ngesModel1 &lt;- readRDS(paste0(models, \"gesModel1.rds\"))\ngesModel2 &lt;- readRDS(paste0(models, \"gesModel2.rds\"))\ngesModel3 &lt;- readRDS(paste0(models, \"gesModel3.rds\"))\ngesModel4 &lt;- readRDS(paste0(models, \"gesModel4.rds\"))\ngesModel5 &lt;- readRDS(paste0(models, \"gesModel5.rds\"))\n\n\n\nTest models\nGenerate predictions and confusion matrices\n\n# Generate predictions\ngesPredictions1 &lt;- predict(gesModel1, newdata = gesData5)\n\n[14:19:08] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\ngesPredictions2 &lt;- predict(gesModel2, newdata = gesData4)\n\n[14:19:08] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\ngesPredictions3 &lt;- predict(gesModel3, newdata = gesData3)\n\n[14:19:09] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\ngesPredictions4 &lt;- predict(gesModel4, newdata = gesData2)\n\n[14:19:09] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\ngesPredictions5 &lt;- predict(gesModel5, newdata = gesData1)\n\n[14:19:09] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\n# Compute confusion matrices\ngesCm1 &lt;- confusionMatrix(gesPredictions1, gesData5$correction_info)\ngesCm2 &lt;- confusionMatrix(gesPredictions2, gesData4$correction_info)\ngesCm3 &lt;- confusionMatrix(gesPredictions3, gesData3$correction_info)\ngesCm4 &lt;- confusionMatrix(gesPredictions4, gesData2$correction_info)\ngesCm5 &lt;- confusionMatrix(gesPredictions5, gesData1$correction_info)\n\n# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)\ngesPValues &lt;- c(gesCm1$overall['AccuracyPValue'], \n              gesCm2$overall['AccuracyPValue'], \n              gesCm3$overall['AccuracyPValue'], \n              gesCm4$overall['AccuracyPValue'], \n              gesCm5$overall['AccuracyPValue'])\n\nCombine p-values using Fisher’s method\n\n# Fisher's method\ngesFisher_combined &lt;- -2 * sum(log(gesPValues))\ndf &lt;- 2 * length(gesPValues)\ngesPCcombined_fisher &lt;- 1 - pchisq(gesFisher_combined, df)\nprint(gesPCcombined_fisher)\n\n[1] 0.630725\n\n# Stouffer's method\ngesZ_scores &lt;- qnorm(1 - gesPValues/2)\ngesCombined_z &lt;- sum(gesZ_scores) / sqrt(length(gesPValues))\ngesP_combined_stouffer &lt;- 2 * (1 - pnorm(abs(gesCombined_z)))\nprint(gesP_combined_stouffer)\n\n[1] 0.1239873\n\n\nThe p-values should sum up to 0. Currently we do not have enough data.\n\n\nFeature importance\n\nModel 1\n\nXGBgesModel1 &lt;- gesModel1$finalModel\nimportanceXGBgesModel1 &lt;- xgb.importance(model = XGBgesModel1)\nhead(importanceXGBgesModel1, n=10)\n\n                        Feature       Gain      Cover  Frequency\n                         &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1: arm_moment_sum_change_range 0.10621775 0.06140352 0.05448718\n 2:            head_power_range 0.08514769 0.07967481 0.08012821\n 3:             head_power_Gstd 0.06856142 0.05235004 0.04807692\n 4:  arm_moment_sum_change_Gstd 0.05890059 0.05588046 0.05769231\n 5:               body_slope_95 0.04080824 0.04771831 0.04487179\n 6:        lowerbody_power_Gstd 0.03443709 0.02881254 0.02884615\n 7:  spine_moment_sum_pospeak_n 0.03367215 0.02497264 0.01923077\n 8:                arm_duration 0.03270931 0.02571379 0.02564103\n 9:        arm_moment_sum_Gmean 0.02745091 0.02582984 0.02564103\n10:  arm_angSpeed_sum_pospeak_n 0.02553955 0.01598632 0.01282051\n\nxgb.plot.importance(importanceXGBgesModel1)\n\n\n\n\n\n\n\n\n\n\nModel 2\n\nXGBgesModel2 &lt;- gesModel2$finalModel\nimportanceXGBgesModel2 &lt;- xgb.importance(model = XGBgesModel2)\nhead(importanceXGBgesModel2, n=10)\n\n                              Feature       Gain      Cover Frequency\n                               &lt;char&gt;      &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n 1:        arm_moment_sum_change_Gstd 0.15656328 0.10177312  0.093750\n 2:                  head_power_range 0.08191146 0.09313640  0.087500\n 3:             head_moment_sum_Gmean 0.05615654 0.04697675  0.037500\n 4: lowerbody_jerkKin_sum_pospeak_std 0.04383749 0.02855344  0.025000\n 5:                     body_slope_95 0.04140191 0.04229001  0.037500\n 6:       arm_moment_sum_change_range 0.04046465 0.03485711  0.034375\n 7:              head_angAcc_sum_Gstd 0.02922143 0.02022442  0.018750\n 8:                    head_inter_Kin 0.02911760 0.02318121  0.025000\n 9:     pelvis_moment_sum_pospeak_std 0.02902350 0.02526397  0.025000\n10:                      arm_nComp_80 0.02862824 0.02214754  0.018750\n\nxgb.plot.importance(importanceXGBgesModel2)\n\n\n\n\n\n\n\n\n\n\nModel 3\n\nXGBgesModel3 &lt;- gesModel3$finalModel\nimportanceXGBgesModel3 &lt;- xgb.importance(model = XGBgesModel3)\nhead(importanceXGBgesModel3, n=10)\n\n                               Feature       Gain      Cover  Frequency\n                                &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:               arm_moment_sum_Gmean 0.08511288 0.07536878 0.07468124\n 2:        arm_moment_sum_change_range 0.06728393 0.05329511 0.04918033\n 3:                   head_power_range 0.06628795 0.04044630 0.03278689\n 4:               head_angAcc_sum_Gstd 0.06543792 0.08684720 0.09836066\n 5:         arm_moment_sum_change_Gstd 0.06096995 0.05092504 0.04918033\n 6: head_moment_sum_change_pospeak_std 0.06096008 0.04739633 0.03825137\n 7: leg_moment_sum_change_pospeak_mean 0.05280459 0.04304653 0.03825137\n 8:                       arm_duration 0.04014164 0.03501963 0.03460838\n 9:               head_moment_sum_Gstd 0.03976587 0.03494228 0.03096539\n10:           leg_moment_sum_pospeak_n 0.03297082 0.02711725 0.02367942\n\nxgb.plot.importance(importanceXGBgesModel3)\n\n\n\n\n\n\n\n\n\n\nModel 4\n\nXGBgesModel4 &lt;- gesModel4$finalModel\nimportanceXGBgesModel4 &lt;- xgb.importance(model = XGBgesModel4)\nhead(importanceXGBgesModel4, n=10)\n\n                        Feature       Gain      Cover  Frequency\n                         &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:                arm_inter_IK 0.16009713 0.10406014 0.09335727\n 2:              head_inter_Kin 0.11882778 0.09862493 0.10053860\n 3:        arm_moment_sum_Gmean 0.06599796 0.06139296 0.05206463\n 4:                arm_duration 0.05548316 0.04347284 0.03949731\n 5:               arm_inter_Kin 0.05482641 0.04860660 0.04308797\n 6:       head_moment_sum_Gmean 0.05183538 0.05242433 0.04667864\n 7:      leg_power_pospeak_mean 0.05120850 0.04967961 0.05026930\n 8: arm_moment_sum_pospeak_mean 0.02549236 0.02261922 0.02333932\n 9:            head_power_Gmean 0.02352650 0.01920458 0.01436266\n10:    head_moment_sum_integral 0.02241479 0.02772374 0.02692998\n\nxgb.plot.importance(importanceXGBgesModel4)\n\n\n\n\n\n\n\n\n\n\nModel 5\n\nXGBgesModel5 &lt;- gesModel5$finalModel\nimportanceXGBgesModel5 &lt;- xgb.importance(model = XGBgesModel5)\nhead(importanceXGBgesModel5, n=10)\n\n                          Feature       Gain      Cover  Frequency\n                           &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:           arm_moment_sum_Gstd 0.13850676 0.10096313 0.08521739\n 2:   arm_moment_sum_change_range 0.06521056 0.05179674 0.05217391\n 3:          arm_moment_sum_range 0.05274530 0.04044015 0.03478261\n 4:                head_inter_Kin 0.04433975 0.05737692 0.06434783\n 5:                  arm_coupling 0.04294857 0.03466822 0.03130435\n 6:              head_power_range 0.03782498 0.04249504 0.04347826\n 7: pelvis_moment_sum_pospeak_std 0.03779493 0.03732779 0.03826087\n 8:                  arm_slope_95 0.03164522 0.02732536 0.02782609\n 9:                leg_power_Gstd 0.02771742 0.02463017 0.02434783\n10:           head_power_integral 0.02668085 0.02211122 0.01739130\n\nxgb.plot.importance(importanceXGBgesModel5)\n\n\n\n\n\n\n\n\n\n\n\nCumulative feature importance\n\n# Function to extract and normalize importance\nget_normalized_importance &lt;- function(model) {\n  importance &lt;- xgb.importance(model = model)\n  importance$Gain &lt;- importance$Gain / sum(importance$Gain)\n  return(importance)\n}\n\n# Extract normalized importance for each model\ngesImportance1 &lt;- get_normalized_importance(gesModel1$finalModel)\ngesImportance2 &lt;- get_normalized_importance(gesModel2$finalModel)\ngesImportance3 &lt;- get_normalized_importance(gesModel3$finalModel)\ngesImportance4 &lt;- get_normalized_importance(gesModel4$finalModel)\ngesImportance5 &lt;- get_normalized_importance(gesModel5$finalModel)\n\n# Combine importances\ngesAllImportances &lt;- list(gesImportance1, gesImportance2, gesImportance3, gesImportance4, gesImportance5)\n\n# Function to merge importances\nmerge_importances &lt;- function(importances) {\n  for (i in 2:length(importances)) {\n    names(importances[[i]])[2:4] &lt;- paste0(names(importances[[i]])[2:4], \"_\", i)\n  }\n  merged &lt;- Reduce(function(x, y) merge(x, y, by = \"Feature\", all = TRUE), importances)\n  merged[is.na(merged)] &lt;- 0  # Replace NAs with 0\n  gain_cols &lt;- grep(\"Gain\", colnames(merged), value = TRUE)\n  merged$Cumulative &lt;- rowSums(merged[, ..gain_cols])\n  return(merged[, .(Feature, Cumulative)])\n}\n\n# Merge and sort importances\ngesCumulativeImportance &lt;- merge_importances(gesAllImportances)\ngesCumulativeImportance &lt;- gesCumulativeImportance[order(-gesCumulativeImportance$Cumulative), ]\n\n# Print cumulative feature importance\nhead(gesCumulativeImportance, n=10)\n\n                        Feature Cumulative\n                         &lt;char&gt;      &lt;num&gt;\n 1:  arm_moment_sum_change_Gstd  0.3030472\n 2:            head_power_range  0.2885670\n 3: arm_moment_sum_change_range  0.2857463\n 4:              head_inter_Kin  0.2163460\n 5:        arm_moment_sum_Gmean  0.2162907\n 6:                arm_inter_IK  0.2116572\n 7:         arm_moment_sum_Gstd  0.1714339\n 8:                arm_duration  0.1674509\n 9:       head_moment_sum_Gmean  0.1308098\n10:        head_angAcc_sum_Gstd  0.1161849\n\n\n\n\nPCA\nNow, to select features along different (uncorrelated) dimensions, we want to connect these results with the results of PCA we performed in this script.\n\n\nCustom function\n# Function to select top 3 features per component\nselect_top_features &lt;- function(pc_column, xgb_importance, top_n = 3) {\n\n  # Find common features ranked by XGBoost importance\n  common_features &lt;- intersect(pc_column, xgb_importance$Feature)\n  common_features &lt;- xgb_importance %&gt;%\n    filter(Feature %in% common_features) %&gt;%\n    arrange(Rank) %&gt;%\n    pull(Feature)\n  return(head(common_features, top_n))\n}\n\n\nFirst, we collect 10 features per component that have highest combined ranking from PCA and XGBoost. This means that for each feature we sum up the ranking it obtained in cumulative importance (XGBoost) and loading on a principal component (PCA).\n\n# Rank the features based on XGBoost importance (cumulative)\ngesCumulativeImportance$XGB_Rank &lt;- rank(-gesCumulativeImportance$Cumulative)\n\n# Load in PCA for gesture\nges_pca &lt;- read_csv(paste0(datasets, \"PCA_top_contributors_ges.csv\"))\n\n# For each PC (PC1, PC2, PC3), rank the features based on their loadings\ncombined_ranks_per_pc &lt;- list()\n\nfor (pc in c(\"PC1\", \"PC2\", \"PC3\")) {\n  # Extract the features and loadings for the current PC\n  pca_pc_loadings &lt;- ges_pca[, c(pc, paste0(pc, \"_Loading\"))]\n  colnames(pca_pc_loadings) &lt;- c(\"Feature\", \"Loading\")\n  \n  # Rank the features based on the absolute loading values (higher loadings should get lower rank)\n  pca_pc_loadings$PCA_Rank &lt;- rank(-abs(pca_pc_loadings$Loading))\n  \n  # Merge PCA loadings with XGBoost importance ranks\n  merged_data &lt;- merge(pca_pc_loadings, gesCumulativeImportance[, c(\"Feature\", \"XGB_Rank\")], by = \"Feature\")\n  \n  # Calculate combined rank by summing XGBoost rank and PCA rank\n  merged_data$Combined_Rank &lt;- merged_data$XGB_Rank + merged_data$PCA_Rank\n  \n  # Sort by the combined rank (lower rank is better)\n  sorted_data &lt;- merged_data[order(merged_data$Combined_Rank), ]\n  \n  # Select the top n features based on the combined rank for the current PC\n  top_n_features &lt;- 10  # Adjust the number of top features as needed\n  combined_ranks_per_pc[[pc]] &lt;- head(sorted_data, top_n_features)\n}\n\n# Output the top features per PC based on combined ranking\nhead(combined_ranks_per_pc, n=10)\n\n$PC1\n                      Feature    Loading PCA_Rank XGB_Rank Combined_Rank\n28               arm_inter_IK 0.06696878       97        6           103\n25                   arm_bbmv 0.08606171        5       99           104\n68       head_accKin_sum_Gstd 0.08200432       18       86           104\n136           leg_power_Gmean 0.07496099       54       56           110\n138        leg_power_integral 0.07328730       62       52           114\n88                  head_bbmv 0.08376976       11      107           118\n15  arm_angJerk_sum_pospeak_n 0.06625538      105       28           133\n51     arm_power_pospeak_mean 0.07210599       66       76           142\n92     head_jerkKin_sum_Gmean 0.07629644       42      105           147\n139    leg_power_pospeak_mean 0.06311577      126       22           148\n\n$PC2\n                               Feature     Loading PCA_Rank XGB_Rank\n113                   head_power_range  0.08564644       41        2\n41         arm_moment_sum_change_range -0.08534023       43        3\n109                head_power_integral  0.08958606       33       14\n37          arm_moment_sum_change_Gstd -0.08359439       48        1\n128 leg_moment_sum_change_pospeak_mean  0.08776522       37       17\n112             head_power_pospeak_std  0.09732699       23       36\n205             spine_moment_sum_Gmean -0.10247717       14       49\n108                    head_power_Gstd  0.07966316       59       12\n172          lowerbody_moment_sum_Gstd  0.10128711       16       58\n102               head_moment_sum_Gstd  0.08005577       58       19\n    Combined_Rank\n113            43\n41             46\n109            47\n37             49\n128            54\n112            59\n205            63\n108            71\n172            74\n102            77\n\n$PC3\n                          Feature     Loading PCA_Rank XGB_Rank Combined_Rank\n177          lowerbody_power_Gstd -0.10452139        7       30            37\n45    arm_moment_sum_pospeak_mean  0.09070628       34       20            54\n208    spine_moment_sum_pospeak_n -0.09428843       28       26            54\n182         lowerbody_power_range -0.10525663        5       51            56\n205        spine_moment_sum_Gmean -0.10301385       11       49            60\n42           arm_moment_sum_Gmean  0.07878132       60        5            65\n133      leg_moment_sum_pospeak_n -0.08279836       52       23            75\n154   lowerbody_angSpeed_sum_Gstd -0.10225587       12       70            82\n207     spine_moment_sum_integral -0.10149747       14       71            85\n186 pelvis_moment_sum_change_Gstd -0.09860998       19       73            92\n\n\nFor modelling, we want to pick only three features per component. Which would it be in this case?\n\n# Number of top features to display\ntop_n_features &lt;- 3\n\n# Print the top 3 features per component\nfor (pc in c(\"PC1\", \"PC2\", \"PC3\")) {\n  cat(\"\\nTop 3 Features for\", pc, \":\\n\")\n  \n  # Get the top 3 features based on combined rank for the current PC\n  top_features &lt;- head(combined_ranks_per_pc[[pc]], top_n_features)\n  \n  # Print the results\n  print(top_features[, c(\"Feature\", \"XGB_Rank\", \"PCA_Rank\", \"Combined_Rank\")])\n}\n\n\nTop 3 Features for PC1 :\n                Feature XGB_Rank PCA_Rank Combined_Rank\n28         arm_inter_IK        6       97           103\n25             arm_bbmv       99        5           104\n68 head_accKin_sum_Gstd       86       18           104\n\nTop 3 Features for PC2 :\n                        Feature XGB_Rank PCA_Rank Combined_Rank\n113            head_power_range        2       41            43\n41  arm_moment_sum_change_range        3       43            46\n109         head_power_integral       14       33            47\n\nTop 3 Features for PC3 :\n                        Feature XGB_Rank PCA_Rank Combined_Rank\n177        lowerbody_power_Gstd       30        7            37\n45  arm_moment_sum_pospeak_mean       20       34            54\n208  spine_moment_sum_pospeak_n       26       28            54",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution"
    ]
  },
  {
    "objectID": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#random-forests-1",
    "href": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#random-forests-1",
    "title": "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution",
    "section": "Random forests",
    "text": "Random forests\nWe first run random forests.\n\n# prepare predictors\npredictors &lt;- setdiff(names(data_voc), \"correction_info\")\n\nformula_str &lt;- paste(\"correction_info ~\", paste(predictors, collapse = \" + \"))\n\n# Convert the formula string to a formula object\nvocTree_formula &lt;- as.formula(formula_str)\n\n# Now use the formula in rpart\nvocTree &lt;- rpart(formula = vocTree_formula, data = data_voc, \n                method='class', # Specify that it's a classification tree\n                control = rpart.control(maxdepth = 5)  # Control parameters for the 'rpart' function\n)\n\nprp(\n  vocTree,         # The decision tree object to be visualized\n  extra = 1,      # Show extra information (like node statistics) in the plot\n  varlen = 0,     # Length of variable names (0 means auto-determined)\n  faclen = 0     # Length of factor levels displayed on the plot (increase as needed)\n)\n\n\n\n\n\n\n\n\nSplit the data\nBuilding the untuned model.\n\n# Untuned Model with importance (permutation) option set\nvocUntuned &lt;- ranger(\n  y = train_data$correction_info,\n  x = train_data[,0:70], # without outcome var\n  num.trees = 500,\n  importance = \"permutation\"\n)\n\npredictions &lt;- predict(vocUntuned, data = test_data)$predictions\n\n# Create a confusion matrix\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$correction_info)\n\n# Print the confusion matrix\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction c0 c0_only c1 c2\n   c0       0       0  0  1\n   c0_only  0       0  0  0\n   c1       1       0  1  0\n   c2       0       0  0  0\n\nOverall Statistics\n                                          \n               Accuracy : 0.3333          \n                 95% CI : (0.0084, 0.9057)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 0.7037          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: c0 Class: c0_only Class: c1 Class: c2\nSensitivity             0.0000             NA    1.0000    0.0000\nSpecificity             0.5000              1    0.5000    1.0000\nPos Pred Value          0.0000             NA    0.5000       NaN\nNeg Pred Value          0.5000             NA    1.0000    0.6667\nPrevalence              0.3333              0    0.3333    0.3333\nDetection Rate          0.0000              0    0.3333    0.0000\nDetection Prevalence    0.3333              0    0.6667    0.0000\nBalanced Accuracy       0.2500             NA    0.7500    0.5000\n\n# Calculate feature importance\nfeature_importance &lt;- importance(vocUntuned, num.threads = 1, type = 1) \n\n# Convert to data frame\nfeature_importance &lt;- as.data.frame(feature_importance, stringsAsFactors = FALSE)\nfeature_importance$Feature &lt;- rownames(feature_importance)\ncolnames(feature_importance) &lt;- c(\"Importance\", \"Feature\")\n\n# Sort by importance\nsorted_feature_importance &lt;- feature_importance[order(-feature_importance$Importance), ]\n\n# Print sorted feature importance\nhead(sorted_feature_importance, n=10)\n\n                           Importance                   Feature\nVSA_f1f2                  0.009076190                  VSA_f1f2\nf3_clean_Gstd             0.008980952             f3_clean_Gstd\nf2_clean_vel_range        0.008032540        f2_clean_vel_range\nf1_clean_vel_range        0.006071429        f1_clean_vel_range\nenvelope_integral         0.005361905         envelope_integral\nf3_clean_vel_Gmean        0.004771429        f3_clean_vel_Gmean\nf1_clean_range            0.004598413            f1_clean_range\nf1_clean_Gstd             0.004238095             f1_clean_Gstd\nenvelope_Gmean            0.003252381            envelope_Gmean\nf1_clean_vel_pospeak_mean 0.003186508 f1_clean_vel_pospeak_mean\n\n\nSet the parameters for the random forest.\n\n# Define the number of CPU cores to use\nnum_cores &lt;- detectCores()\n\n# Create a cluster with specified number of cores\ncl &lt;- makeCluster(num_cores)\n\nTuning the random forest.\n\ntuneVoc &lt;- makeClassifTask(data = data_voc[,0:71], # with OV\n                           target = \"correction_info\")\n\ntuneVoc &lt;- tuneRanger(tuneVoc,\n                      measure = list(multiclass.brier),\n                      num.trees = 500)\n\n#Return hyperparameter values\n#tuneVoc\n\n# Recommended parameter settings: \n# mtry min.node.size sample.fraction\n# 1    6             4        0.522745\n# Results: \n#   multiclass.brier exec.time\n# 1        0.7256575     0.166\n\nvocTuned &lt;- ranger(\n  y = train_data$correction_info,\n  x = train_data[,0:70],  #without OV\n  num.trees = 5000, \n  mtry = 6, # Set the recommended mtry value (number of features).\n  min.node.size = 4, # Set the recommended min.node.size value (number of samples before a node terminates).\n  sample.fraction = 0.522745, # Set the recommended sample fraction value.(% of data for bagging).\n  importance = \"permutation\" # Permutation is a computationally intensive test.\n)\n\npredictions &lt;- predict(vocTuned, data = test_data)$predictions\n\n# Create a confusion matrix\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$correction_info)\n\n# Print the confusion matrix\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction c0 c0_only c1 c2\n   c0       0       0  0  1\n   c0_only  0       0  0  0\n   c1       1       0  1  0\n   c2       0       0  0  0\n\nOverall Statistics\n                                          \n               Accuracy : 0.3333          \n                 95% CI : (0.0084, 0.9057)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 0.7037          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: c0 Class: c0_only Class: c1 Class: c2\nSensitivity             0.0000             NA    1.0000    0.0000\nSpecificity             0.5000              1    0.5000    1.0000\nPos Pred Value          0.0000             NA    0.5000       NaN\nNeg Pred Value          0.5000             NA    1.0000    0.6667\nPrevalence              0.3333              0    0.3333    0.3333\nDetection Rate          0.0000              0    0.3333    0.0000\nDetection Prevalence    0.3333              0    0.6667    0.0000\nBalanced Accuracy       0.2500             NA    0.7500    0.5000\n\n# Calculate feature importance\nfeature_importance &lt;- importance(vocTuned, num.threads = 1, type = 1) \n\n# Convert to data frame\nfeature_importance &lt;- as.data.frame(feature_importance, stringsAsFactors = FALSE)\nfeature_importance$Feature &lt;- rownames(feature_importance)\ncolnames(feature_importance) &lt;- c(\"Importance\", \"Feature\")\n\n# Sort by importance\nsorted_feature_importance &lt;- feature_importance[order(-feature_importance$Importance), ]\n\n# Print sorted feature importance\nhead(sorted_feature_importance, n=10)\n\n                           Importance                   Feature\nVSA_f1f2                  0.005848687                  VSA_f1f2\nf1_clean_range            0.004574312            f1_clean_range\nf3_clean_Gstd             0.003591010             f3_clean_Gstd\nf3_clean_vel_Gmean        0.003268267        f3_clean_vel_Gmean\nf2_clean_vel_range        0.002858143        f2_clean_vel_range\nenvelope_integral         0.002575750         envelope_integral\nf1_clean_Gstd             0.002248725             f1_clean_Gstd\nf2_clean_vel_Gmean        0.001997623        f2_clean_vel_Gmean\nf1_clean_vel_pospeak_mean 0.001879969 f1_clean_vel_pospeak_mean\nf3_clean_pospeak_mean     0.001799207     f3_clean_pospeak_mean\n\n# Close the cluster when you're done with your parallel tasks\n#stopCluster(cl)\n\nCreate a tuned model only.\n\n# Create a classification task for tuning\ntuneVoc &lt;- makeClassifTask(data = train_data[, 0:71], target = \"correction_info\") #with OV\n\n# Tune the model\ntuneVoc &lt;- tuneRanger(tuneVoc, measure = list(multiclass.brier), num.trees = 500)\n\n# Return hyperparameter values\n#tuneVoc\n# Recommended parameter settings: \n#   mtry min.node.size sample.fraction\n# 1    3             3       0.5835222\n# Results: \n#   multiclass.brier exec.time\n# 1        0.7457527      0.16\n\n# Fit the tuned model on the training data\nvocTuned &lt;- ranger(\n  y = train_data$correction_info,\n  x = train_data[, 0:70], # without OV\n  num.trees = 5000,\n  mtry = 3,\n  min.node.size = 3,\n  sample.fraction = 0.5835222,\n  importance = \"permutation\"\n)\n\n# Predict on the test data\npredictions &lt;- predict(vocTuned, data = test_data)$predictions\n\n# Create a confusion matrix\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$correction_info)\n\n# Print the confusion matrix\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction c0 c0_only c1 c2\n   c0       0       0  0  1\n   c0_only  0       0  0  0\n   c1       1       0  1  0\n   c2       0       0  0  0\n\nOverall Statistics\n                                          \n               Accuracy : 0.3333          \n                 95% CI : (0.0084, 0.9057)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 0.7037          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: c0 Class: c0_only Class: c1 Class: c2\nSensitivity             0.0000             NA    1.0000    0.0000\nSpecificity             0.5000              1    0.5000    1.0000\nPos Pred Value          0.0000             NA    0.5000       NaN\nNeg Pred Value          0.5000             NA    1.0000    0.6667\nPrevalence              0.3333              0    0.3333    0.3333\nDetection Rate          0.0000              0    0.3333    0.0000\nDetection Prevalence    0.3333              0    0.6667    0.0000\nBalanced Accuracy       0.2500             NA    0.7500    0.5000\n\n# Calculate feature importance\nfeature_importance &lt;- importance(vocTuned, num.threads = 1, type = 1)\n\n# Convert to data frame\nfeature_importance_df &lt;- as.data.frame(feature_importance, stringsAsFactors = FALSE)\nfeature_importance_df$Feature &lt;- rownames(feature_importance_df)\ncolnames(feature_importance_df) &lt;- c(\"Importance\", \"Feature\")\n\n# Sort by importance\nsorted_feature_importance &lt;- feature_importance_df[order(-feature_importance_df$Importance), ]\n\n# Print sorted feature importance\nhead(sorted_feature_importance, n=10)\n\n                       Importance               Feature\nVSA_f1f2              0.005612179              VSA_f1f2\nf1_clean_range        0.004155960        f1_clean_range\nf3_clean_Gstd         0.003867630         f3_clean_Gstd\nf2_clean_vel_range    0.003017071    f2_clean_vel_range\nf3_clean_vel_Gmean    0.002908297    f3_clean_vel_Gmean\nf3_clean_pospeak_mean 0.002557821 f3_clean_pospeak_mean\nenvelope_integral     0.002401515     envelope_integral\nenvelope_Gmean        0.002082677        envelope_Gmean\nf1_clean_vel_range    0.002057444    f1_clean_vel_range\nf1_clean_Gstd         0.001727735         f1_clean_Gstd\n\n# Close the cluster when you're done with your parallel tasks\n#stopCluster(cl)\n\nSave data frame.\n\nwrite.csv(data_voc, file = paste0(datasets, \"vocDataXGB.csv\"), row.names = FALSE)",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution"
    ]
  },
  {
    "objectID": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#xgboost-1",
    "href": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#xgboost-1",
    "title": "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution",
    "section": "XGBoost",
    "text": "XGBoost\nEnsure parallel processing.\n\n# Detect the number of available cores\ncores &lt;- detectCores() #- 1  # Leave one core free\n\n# Create a cluster with the detected number of cores\ncl &lt;- makeCluster(cores)\n\n# Register the parallel backend\nregisterDoParallel(cl)\n\nDefine the grid and estimate runtime.\n\ngrid_tune &lt;- expand.grid(\n  nrounds = c(5000, 10000), \n  max_depth = c(3, 6), \n  eta = c(0.05, 0.1), \n  gamma = c(0.1), \n  colsample_bytree = c(0.6, 0.8), \n  min_child_weight = c(1), \n  subsample = c(0.75, 1.0)\n)\n\n# Calculate total combinations\ntotal_combinations &lt;- nrow(grid_tune)\n\n# Estimate single model run time (assume 1 minute per run)\nsingle_model_time &lt;- 10 # minute\n\n# Total runs for cross-validation\nfolds &lt;- 5\ntotal_runs &lt;- total_combinations * folds\n\n# Total time estimation without parallel processing\ntotal_time &lt;- total_runs * single_model_time # in minutes\n\n# Convert to hours\ntotal_time_hours &lt;- total_time / 60\n\n# Output estimated time without parallel processing\nprint(paste(\"Estimated time for grid search without parallel processing:\", total_time_hours, \"hours\"))\n\n[1] \"Estimated time for grid search without parallel processing: 26.6666666666667 hours\"\n\n# Parallel processing with 4 cores\ncores &lt;- 24\ntotal_time_parallel &lt;- total_time / cores # in minutes\n\n# Convert to hours\ntotal_time_parallel_hours &lt;- total_time_parallel / 60\n\n# Output estimated time with parallel processing\nprint(paste(\"Estimated time for grid search with\", cores, \"cores:\", total_time_parallel_hours, \"hours\"))\n\n[1] \"Estimated time for grid search with 24 cores: 1.11111111111111 hours\"\n\nrm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)\n\n\nK-fold cross-validation\nCreate subsets to train and test data (80/20).\n\n# Set seed for reproducibility\nset.seed(998)\n\n# Set up train control\ntrain_control &lt;- trainControl(\n  method = \"cv\",        # Cross-validation\n  number = 5,           # 5-fold cross-validation\n  allowParallel = TRUE  # Enable parallel processing\n)\n\n\n# Define the number of subsets\nnumSubsets &lt;- 5\n\n# Load MICE-imputed data (using placeholder 'data_ges' as the input dataset)\nvocDataXGB &lt;- read_csv(paste0(datasets, \"vocDataXGB.csv\"))\n\n# Ensure 'correction_info' is a factor\nvocDataXGB$correction_info &lt;- as.factor(vocDataXGB$correction_info)\n\n# Remove rows with only NA values\nvocDataXGB &lt;- vocDataXGB[rowSums(is.na(vocDataXGB)) &lt; ncol(vocDataXGB), ]\n\n# Split data by levels of 'correction_info'\ncorrection_levels &lt;- levels(vocDataXGB$correction_info)\nsplit_data &lt;- split(vocDataXGB, vocDataXGB$correction_info)\n\n# Initialize a list to store subsets\nvocSubsets &lt;- vector(\"list\", length = numSubsets)\n\n# Distribute rows for each level equally across subsets\nfor (level in correction_levels) {\n  level_data &lt;- split_data[[level]]\n  subset_sizes &lt;- rep(floor(nrow(level_data) / numSubsets), numSubsets)\n  remainder &lt;- nrow(level_data) %% numSubsets\n  \n  # Distribute remainder rows randomly\n  if (remainder &gt; 0) {\n    subset_sizes[seq_len(remainder)] &lt;- subset_sizes[seq_len(remainder)] + 1\n  }\n  \n  # Shuffle rows of the level and assign to subsets\n  shuffled_data &lt;- level_data[sample(nrow(level_data)), ]\n  indices &lt;- cumsum(c(0, subset_sizes))\n  \n  for (i in 1:numSubsets) {\n    if (is.null(vocSubsets[[i]])) {\n      vocSubsets[[i]] &lt;- shuffled_data[(indices[i] + 1):indices[i + 1], ]\n    } else {\n      vocSubsets[[i]] &lt;- rbind(vocSubsets[[i]], shuffled_data[(indices[i] + 1):indices[i + 1], ])\n    }\n  }\n}\n\n# Naming the subsets\nnames(vocSubsets) &lt;- paste0(\"vocData\", 1:numSubsets)\n\n# Verify balance in subsets\nfor (i in 1:numSubsets) {\n  cat(\"Subset\", i, \"contains rows:\", nrow(vocSubsets[[i]]), \"and levels:\\n\")\n  print(table(vocSubsets[[i]]$correction_info))\n}\n\nSubset 1 contains rows: 7 and levels:\n\n     c0 c0_only      c1      c2 \n      2       1       2       2 \nSubset 2 contains rows: 7 and levels:\n\n     c0 c0_only      c1      c2 \n      2       1       2       1 \nSubset 3 contains rows: 5 and levels:\n\n     c0 c0_only      c1      c2 \n      1       1       1       1 \nSubset 4 contains rows: 5 and levels:\n\n     c0 c0_only      c1      c2 \n      1       1       1       1 \nSubset 5 contains rows: 5 and levels:\n\n     c0 c0_only      c1      c2 \n      1       1       1       1 \n\n# Remove any rows with only NAs from subsets just to ensure cleanliness\nvocSubsets &lt;- lapply(vocSubsets, function(subset) {\n  subset[rowSums(is.na(subset)) &lt; ncol(subset), ]\n})\n\n# Access the subsets\nvocData1 &lt;- vocSubsets$vocData1\nvocData2 &lt;- vocSubsets$vocData2\nvocData3 &lt;- vocSubsets$vocData3\nvocData4 &lt;- vocSubsets$vocData4\nvocData5 &lt;- vocSubsets$vocData5\n\n# Combine subsets into 80% groups\nvocData1234 &lt;- rbind(vocData1, vocData2, vocData3, vocData4)\nvocData1235 &lt;- rbind(vocData1, vocData2, vocData3, vocData5)\nvocData1245 &lt;- rbind(vocData1, vocData2, vocData4, vocData5)\nvocData1345 &lt;- rbind(vocData1, vocData3, vocData4, vocData5)\nvocData2345 &lt;- rbind(vocData2, vocData3, vocData4, vocData5)\n\n# Final verification of all levels in the combined datasets\ncombined_sets &lt;- list(vocData1234, vocData1235, vocData1245, vocData1345, vocData2345)\nnames(combined_sets) &lt;- c(\"vocData1234\", \"vocData1235\", \"vocData1245\", \"vocData1345\", \"vocData2345\")\n\nfor (set_name in names(combined_sets)) {\n  cat(\"Dataset\", set_name, \"contains rows:\", nrow(combined_sets[[set_name]]), \"and levels:\\n\")\n  print(table(combined_sets[[set_name]]$correction_info))\n}\n\nDataset vocData1234 contains rows: 21 and levels:\n\n     c0 c0_only      c1      c2 \n      6       4       6       5 \nDataset vocData1235 contains rows: 21 and levels:\n\n     c0 c0_only      c1      c2 \n      6       4       6       5 \nDataset vocData1245 contains rows: 21 and levels:\n\n     c0 c0_only      c1      c2 \n      6       4       6       5 \nDataset vocData1345 contains rows: 19 and levels:\n\n     c0 c0_only      c1      c2 \n      5       4       5       5 \nDataset vocData2345 contains rows: 18 and levels:\n\n     c0 c0_only      c1      c2 \n      5       4       5       4 \n\n\n\n\nModels\n\nModel 1\n\nvocModel1 &lt;- caret::train(\n  correction_info ~ .,              \n  data = vocData1234,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(vocModel1, file = paste0(models, \"vocModel1.rds\"), compress = TRUE)\n\n\n\nModel 2\n\nvocModel2 &lt;- caret::train(\n  correction_info ~ .,              \n  data = vocData1235,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(vocModel2, file = paste0(models, \"vocModel2.rds\"), compress = TRUE)\n\n\n\nModel 3\n\nvocModel3 &lt;- caret::train(\n  correction_info ~ .,              \n  data = vocData1245,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(vocModel3, file = paste0(models, \"vocModel3.rds\"), compress = TRUE)\n\n\n\nModel 4\n\nvocModel4 &lt;- caret::train(\n  correction_info ~ .,              \n  data = vocData1345,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\nsaveRDS(vocModel4, file = paste0(models, \"vocModel4.rds\"), compress = TRUE)\n\n\n\nModel 5\n\nvocModel5 &lt;- caret::train(\n  correction_info ~ .,              \n  data = vocData2345,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(vocModel5, file = paste0(models, \"vocModel5.rds\"), compress = TRUE)\n\n\n\nLoad models\nLoad all models after running, if necessary.\n\nvocModel1 &lt;- readRDS(paste0(models, \"vocModel1.rds\"))\nvocModel2 &lt;- readRDS(paste0(models, \"vocModel2.rds\"))\nvocModel3 &lt;- readRDS(paste0(models, \"vocModel3.rds\"))\nvocModel4 &lt;- readRDS(paste0(models, \"vocModel4.rds\"))\nvocModel5 &lt;- readRDS(paste0(models, \"vocModel5.rds\"))\n\n\n\nTest models\nGenerate predictions and confusion matrices\n\n# Generate predictions\nvocPredictions1 &lt;- predict(vocModel1, newdata = vocData5)\n\n[14:20:05] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\nvocPredictions2 &lt;- predict(vocModel2, newdata = vocData4)\n\n[14:20:05] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\nvocPredictions3 &lt;- predict(vocModel3, newdata = vocData3)\n\n[14:20:05] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\nvocPredictions4 &lt;- predict(vocModel4, newdata = vocData2)\n\n[14:20:05] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\nvocPredictions5 &lt;- predict(vocModel5, newdata = vocData1)\n\n[14:20:06] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\n# Compute confusion matrices\nvocCm1 &lt;- confusionMatrix(vocPredictions1, vocData5$correction_info)\nvocCm2 &lt;- confusionMatrix(vocPredictions2, vocData4$correction_info)\nvocCm3 &lt;- confusionMatrix(vocPredictions3, vocData3$correction_info)\nvocCm4 &lt;- confusionMatrix(vocPredictions4, vocData2$correction_info)\nvocCm5 &lt;- confusionMatrix(vocPredictions5, vocData1$correction_info)\n\n# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)\nvocPValues &lt;- c(vocCm1$overall['AccuracyPValue'], \n              vocCm2$overall['AccuracyPValue'], \n              vocCm3$overall['AccuracyPValue'], \n              vocCm4$overall['AccuracyPValue'], \n              vocCm5$overall['AccuracyPValue'])\n\nCombine p-values using Fisher’s method\n\n# Fisher's method\nvocFisher_combined &lt;- -2 * sum(log(vocPValues))\ndf &lt;- 2 * length(vocPValues)\nvocPCcombined_fisher &lt;- 1 - pchisq(vocFisher_combined, df)\nprint(vocPCcombined_fisher)\n\n[1] 0.6893521\n\n# Stouffer's method\nvocZ_scores &lt;- qnorm(1 - vocPValues/2)\nvocCombined_z &lt;- sum(vocZ_scores) / sqrt(length(vocPValues))\nvocP_combined_stouffer &lt;- 2 * (1 - pnorm(abs(vocCombined_z)))\nprint(vocP_combined_stouffer)\n\n[1] 0.1282563\n\n\nThe p-values should sum up to 0.\n\n\nFeature importance\n\nModel 1\n\nXGBvocModel1 &lt;- vocModel1$finalModel\nimportanceXGBvocModel1 &lt;- xgb.importance(model = XGBvocModel1)\nhead(importanceXGBvocModel1, n=10)\n\n                      Feature       Gain      Cover  Frequency\n                       &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:            envelope_Gmean 0.13602726 0.08502319 0.07502800\n 2:        f3_clean_vel_Gmean 0.09818061 0.09322070 0.09070549\n 3:             envelope_Gstd 0.07400464 0.05561961 0.04927212\n 4:            f1_clean_range 0.07164337 0.07482928 0.07502800\n 5:                  VSA_f1f2 0.05515123 0.04918245 0.04143337\n 6:         envelope_integral 0.04682987 0.06292669 0.06830907\n 7: f1_clean_vel_pospeak_mean 0.04011472 0.03522299 0.03135498\n 8:            f3_clean_Gmean 0.03869450 0.03362272 0.03695409\n 9:      envelope_change_Gstd 0.02847876 0.03867110 0.03919373\n10:            envelope_range 0.02722598 0.02976886 0.03247480\n\nxgb.plot.importance(importanceXGBvocModel1)\n\n\n\n\n\n\n\n\n\n\nModel 2\n\nXGBvocModel2 &lt;- vocModel2$finalModel\nimportanceXGBvocModel2 &lt;- xgb.importance(model = XGBvocModel2)\nhead(importanceXGBvocModel2, n=10)\n\n                      Feature       Gain      Cover  Frequency\n                       &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:        f2_clean_pospeak_n 0.13483087 0.13544889 0.12921348\n 2:            f1_clean_range 0.12070709 0.16690803 0.14325843\n 3:            envelope_Gmean 0.09668145 0.07235325 0.07584270\n 4:             f3_clean_Gstd 0.08237970 0.05602321 0.04494382\n 5:             envelope_Gstd 0.07577008 0.03880060 0.03370787\n 6: envelope_change_pospeak_n 0.06873527 0.03725576 0.04213483\n 7:         envelope_integral 0.04674692 0.06279656 0.07865169\n 8:            envelope_range 0.04544508 0.01653634 0.01685393\n 9:         f2_clean_vel_Gstd 0.03037540 0.01712546 0.01966292\n10:                  VSA_f1f2 0.02954035 0.02412326 0.01404494\n\nxgb.plot.importance(importanceXGBvocModel2)\n\n\n\n\n\n\n\n\n\n\nModel 3\n\nXGBvocModel3 &lt;- vocModel3$finalModel\nimportanceXGBvocModel3 &lt;- xgb.importance(model = XGBvocModel3)\nhead(importanceXGBvocModel3, n=10)\n\n                  Feature       Gain      Cover  Frequency\n                   &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:        envelope_Gmean 0.19678849 0.13101717 0.11990950\n 2:        f1_clean_Gmean 0.06518270 0.06577408 0.05656109\n 3:         f1_clean_Gstd 0.06139091 0.05900369 0.04638009\n 4:    f2_clean_vel_range 0.05294853 0.04460489 0.04411765\n 5:  envelope_change_Gstd 0.05211902 0.05584529 0.04977376\n 6:        f1_clean_range 0.04669633 0.05152589 0.05656109\n 7:        envelope_range 0.04665922 0.03300070 0.03393665\n 8:         envelope_Gstd 0.04528333 0.04547200 0.04751131\n 9: envelope_pospeak_mean 0.03633826 0.04397990 0.04751131\n10:     f3_clean_vel_Gstd 0.03632154 0.03074968 0.03054299\n\nxgb.plot.importance(importanceXGBvocModel3)\n\n\n\n\n\n\n\n\n\n\nModel 4\n\nXGBvocModel4 &lt;- vocModel4$finalModel\nimportanceXGBvocModel4 &lt;- xgb.importance(model = XGBvocModel4)\nhead(importanceXGBvocModel4, n=10)\n\n                         Feature       Gain      Cover  Frequency\n                          &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:               envelope_Gmean 0.16819530 0.09570472 0.08753316\n 2:  envelope_change_pospeak_std 0.08807969 0.07223289 0.06896552\n 3:         envelope_change_Gstd 0.08052363 0.06335792 0.05835544\n 4:                     VSA_f1f2 0.07818772 0.08241809 0.07161804\n 5: envelope_change_pospeak_mean 0.06608976 0.05146865 0.04509284\n 6:    f1_clean_vel_pospeak_mean 0.05408322 0.05589466 0.04509284\n 7:                envelope_Gstd 0.05269826 0.03871238 0.03183024\n 8:               f1_clean_Gmean 0.04965680 0.04615765 0.03713528\n 9:                f1_clean_Gstd 0.03815808 0.05655439 0.06631300\n10:               f1_clean_range 0.03635432 0.04276747 0.04244032\n\nxgb.plot.importance(importanceXGBvocModel4)\n\n\n\n\n\n\n\n\n\n\nModel 5\n\nXGBvocModel5 &lt;- vocModel5$finalModel\nimportanceXGBvocModel5 &lt;- xgb.importance(model = XGBvocModel5)\nhead(importanceXGBvocModel5, n=10)\n\n                        Feature       Gain      Cover  Frequency\n                         &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:              envelope_Gmean 0.15496456 0.13085882 0.13613445\n 2:              f1_clean_range 0.12172902 0.09508698 0.08235294\n 3:           f1_clean_vel_Gstd 0.09589133 0.09944857 0.08739496\n 4:               envelope_Gstd 0.07687082 0.05204549 0.05042017\n 5:       f3_clean_pospeak_mean 0.06338331 0.04400205 0.03865546\n 6:          envelope_pospeak_n 0.04257925 0.03277477 0.03361345\n 7:              f2_clean_range 0.03612444 0.01860086 0.01680672\n 8:               f1_clean_Gstd 0.03560007 0.03146792 0.03025210\n 9: envelope_change_pospeak_std 0.03102815 0.02610947 0.01848739\n10:           envelope_integral 0.02690926 0.03638461 0.04369748\n\nxgb.plot.importance(importanceXGBvocModel5)\n\n\n\n\n\n\n\n\n\n\nCumulative feature importance\n\n# Function to extract and normalize importance\nget_normalized_importance &lt;- function(model) {\n  importance &lt;- xgb.importance(model = model)\n  importance$Gain &lt;- importance$Gain / sum(importance$Gain)\n  return(importance)\n}\n\n# Extract normalized importance for each model\nvocImportance1 &lt;- get_normalized_importance(vocModel1$finalModel)\nvocImportance2 &lt;- get_normalized_importance(vocModel2$finalModel)\nvocImportance3 &lt;- get_normalized_importance(vocModel3$finalModel)\nvocImportance4 &lt;- get_normalized_importance(vocModel4$finalModel)\nvocImportance5 &lt;- get_normalized_importance(vocModel5$finalModel)\n\n# Combine importances\nvocAllImportances &lt;- list(vocImportance1, vocImportance2, vocImportance3, vocImportance4, vocImportance5)\n\n# Function to merge importances\nmerge_importances &lt;- function(importances) {\n  for (i in 2:length(importances)) {\n    names(importances[[i]])[2:4] &lt;- paste0(names(importances[[i]])[2:4], \"_\", i)\n  }\n  merged &lt;- Reduce(function(x, y) merge(x, y, by = \"Feature\", all = TRUE), importances)\n  merged[is.na(merged)] &lt;- 0  # Replace NAs with 0\n  gain_cols &lt;- grep(\"Gain\", colnames(merged), value = TRUE)\n  merged$Cumulative &lt;- rowSums(merged[, ..gain_cols])\n  return(merged[, .(Feature, Cumulative)])\n}\n\n# Merge and sort importances\nvocCumulativeImportance &lt;- merge_importances(vocAllImportances)\nvocCumulativeImportance &lt;- vocCumulativeImportance[order(-vocCumulativeImportance$Cumulative), ]\n\n# Print cumulative feature importance\nhead(vocCumulativeImportance, n=10)\n\n                 Feature Cumulative\n                  &lt;char&gt;      &lt;num&gt;\n 1:       envelope_Gmean  0.7526571\n 2:       f1_clean_range  0.3971301\n 3:        envelope_Gstd  0.3246271\n 4: envelope_change_Gstd  0.2136246\n 5:   f2_clean_pospeak_n  0.1929605\n 6:             VSA_f1f2  0.1870122\n 7:       f1_clean_Gmean  0.1636278\n 8:        f1_clean_Gstd  0.1584567\n 9:    envelope_integral  0.1578377\n10:   f3_clean_vel_Gmean  0.1536120\n\n\n\n\n\nPCA\nNow let’s collect 10 features per component that have highest combined ranking from PCA and XGBoost. This means that for each feature we sum up the ranking it obtained in cumulative importance (XGBoost) and loading on a principal component (PCA).\n\n# Rank the features based on XGBoost importance (cumulative)\nvocCumulativeImportance$XGB_Rank &lt;- rank(-vocCumulativeImportance$Cumulative)\n\n# Load in PCA for gesture\nvoc_pca &lt;- read_csv(paste0(datasets, \"PCA_top_contributors_voc.csv\"))\n\n# For each PC (PC1, PC2, PC3), rank the features based on their loadings\ncombined_ranks_per_pc &lt;- list()\n\nfor (pc in c(\"PC1\", \"PC2\", \"PC3\")) {\n  # Extract the features and loadings for the current PC\n  pca_pc_loadings &lt;- voc_pca[, c(pc, paste0(pc, \"_Loading\"))]\n  colnames(pca_pc_loadings) &lt;- c(\"Feature\", \"Loading\")\n  \n  # Rank the features based on the absolute loading values (higher loadings should get lower rank)\n  pca_pc_loadings$PCA_Rank &lt;- rank(-abs(pca_pc_loadings$Loading))\n  \n  # Merge PCA loadings with XGBoost importance ranks\n  merged_data &lt;- merge(pca_pc_loadings, vocCumulativeImportance[, c(\"Feature\", \"XGB_Rank\")], by = \"Feature\")\n  \n  # Calculate combined rank by summing XGBoost rank and PCA rank\n  merged_data$Combined_Rank &lt;- merged_data$XGB_Rank + merged_data$PCA_Rank\n  \n  # Sort by the combined rank (lower rank is better)\n  sorted_data &lt;- merged_data[order(merged_data$Combined_Rank), ]\n  \n  # Select the top n features based on the combined rank for the current PC\n  top_n_features &lt;- 10  # Adjust the number of top features as needed\n  combined_ranks_per_pc[[pc]] &lt;- head(sorted_data, top_n_features)\n}\n\n# Output the top features per PC based on combined ranking\nhead(combined_ranks_per_pc, n=10)\n\n$PC1\n              Feature    Loading PCA_Rank XGB_Rank Combined_Rank\n21     f1_clean_range -0.2038667        7        2             9\n31 f2_clean_pospeak_n -0.1992125        9        5            14\n50           VSA_f1f2 -0.1981162       10        6            16\n19      f1_clean_Gstd -0.1609309       16        8            24\n38 f2_clean_vel_range -0.2093378        4       20            24\n27 f1_clean_vel_range -0.2044053        6       24            30\n32     f2_clean_range -0.2012178        8       23            31\n13  envelope_integral -0.1301649       23        9            32\n40      f3_clean_Gstd -0.1385837       19       13            32\n29      f2_clean_Gstd -0.2075814        5       29            34\n\n$PC2\n                     Feature     Loading PCA_Rank XGB_Rank Combined_Rank\n13         envelope_integral -0.17231302        7        9            16\n11            envelope_Gmean -0.14800731       20        1            21\n12             envelope_Gstd -0.14806488       19        3            22\n41     f3_clean_pospeak_mean  0.20209426        6       22            28\n44        f3_clean_vel_Gmean -0.14362905       21       10            31\n5       envelope_change_Gstd  0.11631278       29        4            33\n17            envelope_range -0.14099569       22       12            34\n18            f1_clean_Gmean  0.11723539       28        7            35\n8  envelope_change_pospeak_n  0.13615851       25       17            42\n19             f1_clean_Gstd -0.09502265       35        8            43\n\n$PC3\n                     Feature     Loading PCA_Rank XGB_Rank Combined_Rank\n5       envelope_change_Gstd  0.20397194       11        4            15\n12             envelope_Gstd  0.17081447       12        3            15\n11            envelope_Gmean  0.14034453       15        1            16\n19             f1_clean_Gstd -0.12711091       18        8            26\n13         envelope_integral  0.11466983       21        9            30\n18            f1_clean_Gmean -0.10469280       23        7            30\n8  envelope_change_pospeak_n  0.13146985       16       17            33\n4      envelope_change_Gmean  0.25484373        4       30            34\n17            envelope_range  0.06834337       27       12            39\n41     f3_clean_pospeak_mean -0.12623294       19       22            41\n\n\nFor modelling, we want to pick three features per component. Which would it be in this case?\n\n# Number of top features to display\ntop_n_features &lt;- 3\n\n# Print the top 3 features per component\nfor (pc in c(\"PC1\", \"PC2\", \"PC3\")) {\n  cat(\"\\nTop 3 Features for\", pc, \":\\n\")\n  \n  # Get the top 3 features based on combined rank for the current PC\n  top_features &lt;- head(combined_ranks_per_pc[[pc]], top_n_features)\n  \n  # Print the results\n  print(top_features[, c(\"Feature\", \"XGB_Rank\", \"PCA_Rank\", \"Combined_Rank\")])\n}\n\n\nTop 3 Features for PC1 :\n              Feature XGB_Rank PCA_Rank Combined_Rank\n21     f1_clean_range        2        7             9\n31 f2_clean_pospeak_n        5        9            14\n50           VSA_f1f2        6       10            16\n\nTop 3 Features for PC2 :\n             Feature XGB_Rank PCA_Rank Combined_Rank\n13 envelope_integral        9        7            16\n11    envelope_Gmean        1       20            21\n12     envelope_Gstd        3       19            22\n\nTop 3 Features for PC3 :\n                Feature XGB_Rank PCA_Rank Combined_Rank\n5  envelope_change_Gstd        4       11            15\n12        envelope_Gstd        3       12            15\n11       envelope_Gmean        1       15            16",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution"
    ]
  },
  {
    "objectID": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#random-forests-2",
    "href": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#random-forests-2",
    "title": "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution",
    "section": "Random forests",
    "text": "Random forests\nWe will build a random forest first.\n\n# prepare predictors\npredictors &lt;- setdiff(names(data_mult), \"correction_info\")\n\nformula_str &lt;- paste(\"correction_info ~\", paste(predictors, collapse = \" + \"))\n\n# Convert the formula string to a formula object\nmultTree_formula &lt;- as.formula(formula_str)\n\n# Now use the formula in rpart\nmultTree &lt;- rpart(formula = multTree_formula, data = data_mult, \n                method='class', # Specify that it's a classification tree\n                control = rpart.control(maxdepth = 5)  # Control parameters for the 'rpart' function\n)\n\nprp(\n  multTree,         # The decision tree object to be visualized\n  extra = 1,      # Show extra information (like node statistics) in the plot\n  varlen = 0,     # Length of variable names (0 means auto-determined)\n  faclen = 0     # Length of factor levels displayed on the plot (increase as needed)\n)\n\n\n\n\n\n\n\n\nSplit the data\nBuilding the untuned model.\n\n# Untuned Model with importance (permutation) option set\nmultUntuned &lt;- ranger(\n  y = train_data$correction_info,\n  x = train_data[,0:394], # without OV\n  num.trees = 500,\n  importance = \"permutation\"\n)\n\npredictions &lt;- predict(multUntuned, data = test_data)$predictions\n\n# Create a confusion matrix\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$correction_info)\n\n# Print the confusion matrix\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction c0 c0_only c1 c2\n   c0       0       0  0  0\n   c0_only  1       0  0  0\n   c1       0       1  0  1\n   c2       0       0  1  0\n\nOverall Statistics\n                                     \n               Accuracy : 0          \n                 95% CI : (0, 0.6024)\n    No Information Rate : 0.25       \n    P-Value [Acc &gt; NIR] : 1          \n                                     \n                  Kappa : -0.3333    \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: c0 Class: c0_only Class: c1 Class: c2\nSensitivity               0.00         0.0000    0.0000    0.0000\nSpecificity               1.00         0.6667    0.3333    0.6667\nPos Pred Value             NaN         0.0000    0.0000    0.0000\nNeg Pred Value            0.75         0.6667    0.5000    0.6667\nPrevalence                0.25         0.2500    0.2500    0.2500\nDetection Rate            0.00         0.0000    0.0000    0.0000\nDetection Prevalence      0.00         0.2500    0.5000    0.2500\nBalanced Accuracy         0.50         0.3333    0.1667    0.3333\n\n# Calculate feature importance\nfeature_importance &lt;- importance(multUntuned, num.threads = 1, type = 1) \n\n# Convert to data frame\nfeature_importance &lt;- as.data.frame(feature_importance, stringsAsFactors = FALSE)\nfeature_importance$Feature &lt;- rownames(feature_importance)\ncolnames(feature_importance) &lt;- c(\"Importance\", \"Feature\")\n\n# Sort by importance\nsorted_feature_importance &lt;- feature_importance[order(-feature_importance$Importance), ]\n\n# Print sorted feature importance\nhead(sorted_feature_importance, n=10)\n\n                                   Importance                           Feature\nenvelope_change_pospeak_mean      0.004966667      envelope_change_pospeak_mean\narm_power_pospeak_std             0.003269048             arm_power_pospeak_std\nleg_angJerk_sum_pospeak_mean      0.002706349      leg_angJerk_sum_pospeak_mean\narm_accKin_sum_range              0.002615079              arm_accKin_sum_range\nlowerbody_jerkKin_sum_Gmean       0.002487302       lowerbody_jerkKin_sum_Gmean\nhead_speedKin_sum_pospeak_n       0.002206349       head_speedKin_sum_pospeak_n\nlowerbody_jerkKin_sum_pospeak_std 0.002028571 lowerbody_jerkKin_sum_pospeak_std\nhead_angSpeed_sum_range           0.001966667           head_angSpeed_sum_range\narm_accKin_sum_Gmean              0.001816667              arm_accKin_sum_Gmean\nhead_moment_sum_change_integral   0.001815079   head_moment_sum_change_integral\n\n\nSet the parameters for the random forest.\n\n# Define the number of CPU cores to use\nnum_cores &lt;- detectCores()\n\n# Create a cluster with specified number of cores\ncl &lt;- makeCluster(num_cores)\n\nTuning the random forest.\n\ntuneMult &lt;- makeClassifTask(data = data_mult[,0:395], # with OV\n                           target = \"correction_info\")\n\ntuneMult &lt;- tuneRanger(tuneMult,\n                      measure = list(multiclass.brier),\n                      num.trees = 500)\n\n#Return hyperparameter values\n#tuneMult\n\n# Recommended parameter settings: \n#   mtry min.node.size sample.fraction\n# 1  232             3         0.70919\n# Results: \n#   multiclass.brier exec.time\n# 1        0.7385408       0.2\n\nmultTuned &lt;- ranger(\n  y = train_data$correction_info,\n  x = train_data[,0:394],  # without OV\n  num.trees = 5000, \n  mtry = 232, # Set the recommended mtry value (number of features).\n  min.node.size = 3, # Set the recommended min.node.size value (number of samples before a node terminates).\n  sample.fraction = 0.70919, # Set the recommended sample fraction value.(% of data for bagging).\n  importance = \"permutation\" # Permutation is a computationally intensive test.\n)\n\npredictions &lt;- predict(multTuned, data = test_data)$predictions\n\n# Create a confusion matrix\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$correction_info)\n\n# Print the confusion matrix\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction c0 c0_only c1 c2\n   c0       0       0  0  0\n   c0_only  0       0  0  0\n   c1       1       1  0  1\n   c2       0       0  1  0\n\nOverall Statistics\n                                     \n               Accuracy : 0          \n                 95% CI : (0, 0.6024)\n    No Information Rate : 0.25       \n    P-Value [Acc &gt; NIR] : 1          \n                                     \n                  Kappa : -0.3333    \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: c0 Class: c0_only Class: c1 Class: c2\nSensitivity               0.00           0.00      0.00    0.0000\nSpecificity               1.00           1.00      0.00    0.6667\nPos Pred Value             NaN            NaN      0.00    0.0000\nNeg Pred Value            0.75           0.75      0.00    0.6667\nPrevalence                0.25           0.25      0.25    0.2500\nDetection Rate            0.00           0.00      0.00    0.0000\nDetection Prevalence      0.00           0.00      0.75    0.2500\nBalanced Accuracy         0.50           0.50      0.00    0.3333\n\n# Calculate feature importance\nfeature_importance &lt;- importance(multTuned, num.threads = 1, type = 1) \n\n# Convert to data frame\nfeature_importance &lt;- as.data.frame(feature_importance, stringsAsFactors = FALSE)\nfeature_importance$Feature &lt;- rownames(feature_importance)\ncolnames(feature_importance) &lt;- c(\"Importance\", \"Feature\")\n\n# Sort by importance\nsorted_feature_importance &lt;- feature_importance[order(-feature_importance$Importance), ]\n\n# Print sorted feature importance\nhead(sorted_feature_importance, n=10)\n\n                                   Importance                           Feature\narm_power_pospeak_std             0.006726943             arm_power_pospeak_std\narm_accKin_sum_range              0.003974206              arm_accKin_sum_range\nleg_speedKin_sum_pospeak_mean     0.003076378     leg_speedKin_sum_pospeak_mean\nleg_accKin_sum_pospeak_mean       0.002952482       leg_accKin_sum_pospeak_mean\nenvelope_change_pospeak_mean      0.001955000      envelope_change_pospeak_mean\nlowerbody_jerkKin_sum_Gmean       0.001952301       lowerbody_jerkKin_sum_Gmean\nhead_angSpeed_sum_range           0.001771530           head_angSpeed_sum_range\nleg_angJerk_sum_pospeak_mean      0.001765200      leg_angJerk_sum_pospeak_mean\nlowerbody_jerkKin_sum_pospeak_std 0.001732114 lowerbody_jerkKin_sum_pospeak_std\narm_accKin_sum_Gstd               0.001284538               arm_accKin_sum_Gstd\n\n# Close the cluster when you're done with your parallel tasks\n#stopCluster(cl)\n\nCreate a tuned model only.\n\n# Create a classification task for tuning\ntuneMult &lt;- makeClassifTask(data = train_data[, 0:395], target = \"correction_info\") # with OV\n\n# Tune the model\ntuneMult &lt;- tuneRanger(tuneMult, measure = list(multiclass.brier), num.trees = 500)\n\n# Return hyperparameter values\n#tuneMult\n# Recommended parameter settings: \n# mtry min.node.size sample.fraction\n# 1  268             3       0.3295434\n# Results: \n#   multiclass.brier exec.time\n# 1        0.8275102     0.172\n\n# Fit the tuned model on the training data\nmultTuned &lt;- ranger(\n  y = train_data$correction_info,\n  x = train_data[, 0:394],  # without OV\n  num.trees = 5000,\n  mtry = 268,\n  min.node.size = 3,\n  sample.fraction = 0.3295434,\n  importance = \"permutation\"\n)\n\n# Predict on the test data\npredictions &lt;- predict(multTuned, data = test_data)$predictions\n\n# Create a confusion matrix\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$correction_info)\n\n# Print the confusion matrix\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction c0 c0_only c1 c2\n   c0       0       0  0  0\n   c0_only  0       0  0  0\n   c1       1       1  0  1\n   c2       0       0  1  0\n\nOverall Statistics\n                                     \n               Accuracy : 0          \n                 95% CI : (0, 0.6024)\n    No Information Rate : 0.25       \n    P-Value [Acc &gt; NIR] : 1          \n                                     \n                  Kappa : -0.3333    \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: c0 Class: c0_only Class: c1 Class: c2\nSensitivity               0.00           0.00      0.00    0.0000\nSpecificity               1.00           1.00      0.00    0.6667\nPos Pred Value             NaN            NaN      0.00    0.0000\nNeg Pred Value            0.75           0.75      0.00    0.6667\nPrevalence                0.25           0.25      0.25    0.2500\nDetection Rate            0.00           0.00      0.00    0.0000\nDetection Prevalence      0.00           0.00      0.75    0.2500\nBalanced Accuracy         0.50           0.50      0.00    0.3333\n\n# Calculate feature importance\nfeature_importance &lt;- importance(multTuned, num.threads = 1, type = 1)\n\n# Convert to data frame\nfeature_importance_df &lt;- as.data.frame(feature_importance, stringsAsFactors = FALSE)\nfeature_importance_df$Feature &lt;- rownames(feature_importance_df)\ncolnames(feature_importance_df) &lt;- c(\"Importance\", \"Feature\")\n\n# Sort by importance\nsorted_feature_importance &lt;- feature_importance_df[order(-feature_importance_df$Importance), ]\n\n# Print sorted feature importance\nhead(sorted_feature_importance, n=10)\n\n                                    Importance\narm_power_pospeak_std             0.0009376557\nleg_accKin_sum_pospeak_mean       0.0008753114\narm_accKin_sum_range              0.0007167766\nleg_speedKin_sum_pospeak_mean     0.0006603297\nhead_moment_sum_range             0.0005926007\nleg_angJerk_sum_pospeak_mean      0.0005246154\nhead_power_pospeak_std            0.0004887912\nenvelope_change_pospeak_mean      0.0004717216\nlowerbody_jerkKin_sum_pospeak_std 0.0004468864\nhead_angSpeed_sum_range           0.0004372161\n                                                            Feature\narm_power_pospeak_std                         arm_power_pospeak_std\nleg_accKin_sum_pospeak_mean             leg_accKin_sum_pospeak_mean\narm_accKin_sum_range                           arm_accKin_sum_range\nleg_speedKin_sum_pospeak_mean         leg_speedKin_sum_pospeak_mean\nhead_moment_sum_range                         head_moment_sum_range\nleg_angJerk_sum_pospeak_mean           leg_angJerk_sum_pospeak_mean\nhead_power_pospeak_std                       head_power_pospeak_std\nenvelope_change_pospeak_mean           envelope_change_pospeak_mean\nlowerbody_jerkKin_sum_pospeak_std lowerbody_jerkKin_sum_pospeak_std\nhead_angSpeed_sum_range                     head_angSpeed_sum_range\n\n# Close the cluster when you're done with your parallel tasks\n#stopCluster(cl)\n\nSave data frame.\n\nwrite.csv(data_mult, file = paste0(datasets, \"multDataXGB.csv\"), row.names = FALSE)",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution"
    ]
  },
  {
    "objectID": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#xgboost-2",
    "href": "08_Analysis_XGBoost/02_XGBoost_effortIndicators.html#xgboost-2",
    "title": "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution",
    "section": "XGBoost",
    "text": "XGBoost\nEnsure parallel processing.\n\n# Detect the number of available cores\ncores &lt;- detectCores() #- 1  # Leave one core free\n\n# Create a cluster with the detected number of cores\ncl &lt;- makeCluster(cores)\n\n# Register the parallel backend\nregisterDoParallel(cl)\n\nDefine the grid and estimate runtime.\n\ngrid_tune &lt;- expand.grid(\n  nrounds = c(5000, 10000), \n  max_depth = c(3, 6), \n  eta = c(0.05, 0.1), \n  gamma = c(0.1), \n  colsample_bytree = c(0.6, 0.8), \n  min_child_weight = c(1), \n  subsample = c(0.75, 1.0)\n)\n\n# Calculate total combinations\ntotal_combinations &lt;- nrow(grid_tune)\n\n# Estimate single model run time (assume 1 minute per run)\nsingle_model_time &lt;- 10 # minute\n\n# Total runs for cross-validation\nfolds &lt;- 5\ntotal_runs &lt;- total_combinations * folds\n\n# Total time estimation without parallel processing\ntotal_time &lt;- total_runs * single_model_time # in minutes\n\n# Convert to hours\ntotal_time_hours &lt;- total_time / 60\n\n# Output estimated time without parallel processing\nprint(paste(\"Estimated time for grid search without parallel processing:\", total_time_hours, \"hours\"))\n\n[1] \"Estimated time for grid search without parallel processing: 26.6666666666667 hours\"\n\n# Parallel processing with 4 cores\ncores &lt;- 24\ntotal_time_parallel &lt;- total_time / cores # in minutes\n\n# Convert to hours\ntotal_time_parallel_hours &lt;- total_time_parallel / 60\n\n# Output estimated time with parallel processing\nprint(paste(\"Estimated time for grid search with\", cores, \"cores:\", total_time_parallel_hours, \"hours\"))\n\n[1] \"Estimated time for grid search with 24 cores: 1.11111111111111 hours\"\n\nrm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)\n\n\nK-fold cross-validation\nCreate subsets to train and test data (80/20).\n\n# Set seed for reproducibility\nset.seed(998)\n\n# Set up train control\ntrain_control &lt;- trainControl(\n  method = \"cv\",        # Cross-validation\n  number = 5,           # 5-fold cross-validation\n  allowParallel = TRUE  # Enable parallel processing\n)\n\n# Define the number of subsets\nnumSubsets &lt;- 5\n\n# Load MICE-imputed data (using placeholder 'data_ges' as the input dataset)\nmultDataXGB &lt;- data_mult\n\n# Ensure 'correction_info' is a factor\nmultDataXGB$correction_info &lt;- as.factor(multDataXGB$correction_info)\n\n# Remove rows with only NA values\nmultDataXGB &lt;- multDataXGB[rowSums(is.na(multDataXGB)) &lt; ncol(multDataXGB), ]\n\n# Split data by levels of 'correction_info'\ncorrection_levels &lt;- levels(multDataXGB$correction_info)\nsplit_data &lt;- split(multDataXGB, multDataXGB$correction_info)\n\n# Initialize a list to store subsets\nmultSubsets &lt;- vector(\"list\", length = numSubsets)\n\n# Distribute rows for each level equally across subsets\nfor (level in correction_levels) {\n  level_data &lt;- split_data[[level]]\n  subset_sizes &lt;- rep(floor(nrow(level_data) / numSubsets), numSubsets)\n  remainder &lt;- nrow(level_data) %% numSubsets\n  \n  # Distribute remainder rows randomly\n  if (remainder &gt; 0) {\n    subset_sizes[seq_len(remainder)] &lt;- subset_sizes[seq_len(remainder)] + 1\n  }\n  \n  # Shuffle rows of the level and assign to subsets\n  shuffled_data &lt;- level_data[sample(nrow(level_data)), ]\n  indices &lt;- cumsum(c(0, subset_sizes))\n  \n  for (i in 1:numSubsets) {\n    if (is.null(multSubsets[[i]])) {\n      multSubsets[[i]] &lt;- shuffled_data[(indices[i] + 1):indices[i + 1], ]\n    } else {\n      multSubsets[[i]] &lt;- rbind(multSubsets[[i]], shuffled_data[(indices[i] + 1):indices[i + 1], ])\n    }\n  }\n}\n\n# Naming the subsets\nnames(multSubsets) &lt;- paste0(\"multData\", 1:numSubsets)\n\n# Verify balance in subsets\nfor (i in 1:numSubsets) {\n  cat(\"Subset\", i, \"contains rows:\", nrow(multSubsets[[i]]), \"and levels:\\n\")\n  print(table(multSubsets[[i]]$correction_info))\n}\n\nSubset 1 contains rows: 7 and levels:\n\n     c0 c0_only      c1      c2 \n      2       1       2       2 \nSubset 2 contains rows: 4 and levels:\n\n     c0 c0_only      c1      c2 \n      1       1       1       1 \nSubset 3 contains rows: 4 and levels:\n\n     c0 c0_only      c1      c2 \n      1       1       1       1 \nSubset 4 contains rows: 4 and levels:\n\n     c0 c0_only      c1      c2 \n      1       1       1       1 \nSubset 5 contains rows: 4 and levels:\n\n     c0 c0_only      c1      c2 \n      1       1       1       1 \n\n# Remove any rows with only NAs from subsets just to ensure cleanliness\nmultSubsets &lt;- lapply(multSubsets, function(subset) {\n  subset[rowSums(is.na(subset)) &lt; ncol(subset), ]\n})\n\n# Access the subsets\nmultData1 &lt;- multSubsets$multData1\nmultData2 &lt;- multSubsets$multData2\nmultData3 &lt;- multSubsets$multData3\nmultData4 &lt;- multSubsets$multData4\nmultData5 &lt;- multSubsets$multData5\n\n# Combine subsets into 80% groups\nmultData1234 &lt;- rbind(multData1, multData2, multData3, multData4)\nmultData1235 &lt;- rbind(multData1, multData2, multData3, multData5)\nmultData1245 &lt;- rbind(multData1, multData2, multData4, multData5)\nmultData1345 &lt;- rbind(multData1, multData3, multData4, multData5)\nmultData2345 &lt;- rbind(multData2, multData3, multData4, multData5)\n\n# Final verification of all levels in the combined datasets\ncombined_sets &lt;- list(multData1234, multData1235, multData1245, multData1345, multData2345)\nnames(combined_sets) &lt;- c(\"multData1234\", \"multData1235\", \"multData1245\", \"multData1345\", \"multData2345\")\n\nfor (set_name in names(combined_sets)) {\n  cat(\"Dataset\", set_name, \"contains rows:\", nrow(combined_sets[[set_name]]), \"and levels:\\n\")\n  print(table(combined_sets[[set_name]]$correction_info))\n}\n\nDataset multData1234 contains rows: 19 and levels:\n\n     c0 c0_only      c1      c2 \n      5       4       5       5 \nDataset multData1235 contains rows: 19 and levels:\n\n     c0 c0_only      c1      c2 \n      5       4       5       5 \nDataset multData1245 contains rows: 19 and levels:\n\n     c0 c0_only      c1      c2 \n      5       4       5       5 \nDataset multData1345 contains rows: 19 and levels:\n\n     c0 c0_only      c1      c2 \n      5       4       5       5 \nDataset multData2345 contains rows: 16 and levels:\n\n     c0 c0_only      c1      c2 \n      4       4       4       4 \n\n\n\n\nModels\nOnly run the models one time and then readRDS.\n\nModel 1\n\nmultModel1 &lt;- caret::train(\n  correction_info ~ .,              \n  data = multData1234,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(multModel1, file = paste0(models, \"multModel1.rds\"), compress = TRUE)\n\n\n\nModel 2\n\nmultModel2 &lt;- caret::train(\n  correction_info ~ .,              \n  data = multData1235,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(multModel2, file = paste0(models, \"multModel2.rds\"), compress = TRUE)\n\n\n\nModel 3\n\nmultModel3 &lt;- caret::train(\n  correction_info ~ .,              \n  data = multData1245,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(multModel3, file = paste0(models, \"multModel3.rds\"), compress = TRUE)\n\n\n\nModel 4\n\nmultModel4 &lt;- caret::train(\n  correction_info ~ .,              \n  data = multData1345,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\nsaveRDS(multModel4, file = paste0(models, \"multModel4.rds\"), compress = TRUE)\n\n\n\nModel 5\n\nmultModel5 &lt;- caret::train(\n  correction_info ~ .,              \n  data = multData2345,\n  method = \"xgbTree\",     \n  trControl = train_control,\n  tuneGrid = grid_tune    \n)\n\nsaveRDS(multModel5, file = paste0(models, \"multModel5.rds\"), compress = TRUE)\n\n\n\nLoad models\nLoad all models after running, if necessary.\n\nmultModel1 &lt;- readRDS(paste0(models, \"multModel1.rds\"))\nmultModel2 &lt;- readRDS(paste0(models, \"multModel2.rds\"))\nmultModel3 &lt;- readRDS(paste0(models, \"multModel3.rds\"))\nmultModel4 &lt;- readRDS(paste0(models, \"multModel4.rds\"))\nmultModel5 &lt;- readRDS(paste0(models, \"multModel5.rds\"))\n\n\n\nTest models\nGenerate predictions and confusion matrices\n\n# Generate predictions\nmultPredictions1 &lt;- predict(multModel1, newdata = multData5)\n\n[14:21:03] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\nmultPredictions2 &lt;- predict(multModel2, newdata = multData4)\n\n[14:21:03] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\nmultPredictions3 &lt;- predict(multModel3, newdata = multData3)\n\n[14:21:04] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\nmultPredictions4 &lt;- predict(multModel4, newdata = multData2)\n\n[14:21:04] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\nmultPredictions5 &lt;- predict(multModel5, newdata = multData1)\n\n[14:21:04] WARNING: src/learner.cc:553: \n  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n  older XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version. See:\n\n    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n\n  for more details about differences between saving model and serializing.\n\n# Compute confusion matrices\nmultCm1 &lt;- confusionMatrix(multPredictions1, multData5$correction_info)\nmultCm2 &lt;- confusionMatrix(multPredictions2, multData4$correction_info)\nmultCm3 &lt;- confusionMatrix(multPredictions3, multData3$correction_info)\nmultCm4 &lt;- confusionMatrix(multPredictions4, multData2$correction_info)\nmultCm5 &lt;- confusionMatrix(multPredictions5, multData1$correction_info)\n\n# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)\nmultPValues &lt;- c(multCm1$overall['AccuracyPValue'], \n              multCm2$overall['AccuracyPValue'], \n              multCm3$overall['AccuracyPValue'], \n              multCm4$overall['AccuracyPValue'], \n              multCm5$overall['AccuracyPValue'])\n\nCombine p-values using Fisher’s method\n\n# Fisher's method\nmultFisher_combined &lt;- -2 * sum(log(multPValues))\ndf &lt;- 2 * length(multPValues)\nmultPCcombined_fisher &lt;- 1 - pchisq(multFisher_combined, df)\nprint(multPCcombined_fisher)\n\n[1] 0.3920742\n\n# Stouffer's method\nmultZ_scores &lt;- qnorm(1 - multPValues/2)\nmultCombined_z &lt;- sum(multZ_scores) / sqrt(length(multPValues))\nmultP_combined_stouffer &lt;- 2 * (1 - pnorm(abs(multCombined_z)))\nprint(multP_combined_stouffer)\n\n[1] 0.05686547\n\n\nThe p-values should sum up to 0.\n\n\nFeature importance\n\nModel 1\n\nXGBmultModel1 &lt;- multModel1$finalModel\nimportanceXGBmultModel1 &lt;- xgb.importance(model = XGBmultModel1)\nhead(importanceXGBmultModel1, n=10)\n\n                                Feature       Gain      Cover  Frequency\n                                 &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:               arm_power_pospeak_std 0.16370920 0.10839576 0.08992806\n 2: head_moment_sum_change_pospeak_mean 0.08237537 0.09064362 0.09352518\n 3:                       head_duration 0.08032469 0.06534066 0.05755396\n 4:         lowerbody_jerkKin_sum_Gmean 0.06176702 0.07529737 0.06115108\n 5:                      head_inter_Kin 0.05656996 0.03174240 0.02517986\n 6:                envelope_change_Gstd 0.05258590 0.04236234 0.05755396\n 7:                arm_accKin_sum_range 0.05149798 0.05082393 0.04676259\n 8:       leg_speedKin_sum_pospeak_mean 0.05088554 0.06236849 0.05395683\n 9:                arm_moment_sum_range 0.04731327 0.05174319 0.05035971\n10:   lowerbody_jerkKin_sum_pospeak_std 0.04304124 0.03763756 0.03237410\n\nxgb.plot.importance(importanceXGBmultModel1)\n\n\n\n\n\n\n\n\n\n\nModel 2\n\nXGBmultModel2 &lt;- multModel2$finalModel\nimportanceXGBmultModel2 &lt;- xgb.importance(model = XGBmultModel2)\nhead(importanceXGBmultModel2, n=10)\n\n                                Feature       Gain      Cover   Frequency\n                                 &lt;char&gt;      &lt;num&gt;      &lt;num&gt;       &lt;num&gt;\n 1:         lowerbody_jerkKin_sum_Gmean 0.05671646 0.05851969 0.046052632\n 2:                 arm_accKin_sum_Gstd 0.04611643 0.02468970 0.019736842\n 3:                arm_accKin_sum_range 0.04296956 0.02853323 0.019736842\n 4:               head_angAcc_sum_range 0.04274041 0.03817195 0.032894737\n 5:                       head_duration 0.03394080 0.01916268 0.016447368\n 6:                      head_inter_Kin 0.02912452 0.03218462 0.032894737\n 7:               arm_power_pospeak_std 0.02882428 0.02028096 0.019736842\n 8:                        arm_inter_IK 0.02682163 0.01527494 0.016447368\n 9: head_moment_sum_change_pospeak_mean 0.02476986 0.01973325 0.016447368\n10:       lowerbody_accKin_sum_integral 0.02368903 0.01247853 0.009868421\n\nxgb.plot.importance(importanceXGBmultModel2)\n\n\n\n\n\n\n\n\n\n\nModel 3\n\nXGBmultModel3 &lt;- multModel3$finalModel\nimportanceXGBmultModel3 &lt;- xgb.importance(model = XGBmultModel3)\nhead(importanceXGBmultModel3, n=10)\n\n                                Feature       Gain      Cover  Frequency\n                                 &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:               arm_power_pospeak_std 0.11936301 0.06412536 0.04587156\n 2:         leg_moment_sum_pospeak_mean 0.06558456 0.06864607 0.05810398\n 3:        envelope_change_pospeak_mean 0.03526312 0.03716335 0.03363914\n 4:         lowerbody_jerkKin_sum_Gmean 0.03033355 0.03258257 0.02752294\n 5:              spine_moment_sum_Gmean 0.02960073 0.02155368 0.01529052\n 6:                       head_inter_IK 0.02905922 0.01726820 0.01529052\n 7:                       body_slope_80 0.02434499 0.04392720 0.05504587\n 8: head_moment_sum_change_pospeak_mean 0.02274487 0.03483516 0.03669725\n 9:                arm_accKin_sum_range 0.02268550 0.02720615 0.02752294\n10:               arm_speedKin_sum_Gstd 0.02068770 0.01205195 0.01223242\n\nxgb.plot.importance(importanceXGBmultModel3)\n\n\n\n\n\n\n\n\n\n\nModel 4\n\nXGBmultModel4 &lt;- multModel4$finalModel\nimportanceXGBmultModel4 &lt;- xgb.importance(model = XGBmultModel4)\nhead(importanceXGBmultModel4, n=10)\n\n                                Feature       Gain      Cover  Frequency\n                                 &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1: head_moment_sum_change_pospeak_mean 0.13433303 0.10993043 0.08828829\n 2:               arm_power_pospeak_std 0.11704856 0.08864239 0.07567568\n 3:                arm_moment_sum_range 0.07681624 0.06679121 0.05585586\n 4:             head_angSpeed_sum_range 0.07387166 0.06683661 0.05945946\n 5:                arm_accKin_sum_range 0.07129803 0.05788595 0.05045045\n 6:              head_angJerk_sum_range 0.06440727 0.05423596 0.03783784\n 7:        lowerbody_speedKin_sum_Gmean 0.05471338 0.04881843 0.03783784\n 8:        lowerbody_speedKin_sum_range 0.05336213 0.04938517 0.03783784\n 9:              arm_speedKin_sum_range 0.04297612 0.02185336 0.01441441\n10:               head_angAcc_sum_Gmean 0.03311430 0.03207859 0.02522523\n\nxgb.plot.importance(importanceXGBmultModel4)\n\n\n\n\n\n\n\n\n\n\nModel 5\n\nXGBmultModel5 &lt;- multModel5$finalModel\nimportanceXGBmultModel5 &lt;- xgb.importance(model = XGBmultModel5)\nhead(importanceXGBmultModel5, n=10)\n\n                        Feature       Gain      Cover  Frequency\n                         &lt;char&gt;      &lt;num&gt;      &lt;num&gt;      &lt;num&gt;\n 1:       arm_speedKin_sum_Gstd 0.25372488 0.18051833 0.14107884\n 2:              head_inter_Kin 0.10834497 0.08322603 0.07883817\n 3:     head_angSpeed_sum_range 0.08083857 0.05868575 0.04979253\n 4:      arm_speedKin_sum_Gmean 0.06179184 0.06637690 0.06224066\n 5:  head_speedKin_sum_integral 0.06069948 0.06488408 0.05394191\n 6: leg_moment_sum_pospeak_mean 0.03819193 0.02769087 0.02074689\n 7:     arm_accKin_sum_integral 0.03553192 0.03942087 0.03734440\n 8: lowerbody_jerkKin_sum_Gmean 0.03357866 0.03170172 0.02489627\n 9:        arm_accKin_sum_range 0.02801619 0.02222806 0.01659751\n10:       arm_power_pospeak_std 0.02737382 0.03537906 0.03734440\n\nxgb.plot.importance(importanceXGBmultModel5)\n\n\n\n\n\n\n\n\n\n\nCumulative feature importance\n\n# Function to extract and normalize importance\nget_normalized_importance &lt;- function(model) {\n  importance &lt;- xgb.importance(model = model)\n  importance$Gain &lt;- importance$Gain / sum(importance$Gain)\n  return(importance)\n}\n\n# Extract normalized importance for each model\nmultImportance1 &lt;- get_normalized_importance(multModel1$finalModel)\nmultImportance2 &lt;- get_normalized_importance(multModel2$finalModel)\nmultImportance3 &lt;- get_normalized_importance(multModel3$finalModel)\nmultImportance4 &lt;- get_normalized_importance(multModel4$finalModel)\nmultImportance5 &lt;- get_normalized_importance(multModel5$finalModel)\n\n# Combine importances\nmultAllImportances &lt;- list(multImportance1, multImportance2, multImportance3, multImportance4, multImportance5)\n\n# Function to merge importances\nmerge_importances &lt;- function(importances) {\n  for (i in 2:length(importances)) {\n    names(importances[[i]])[2:4] &lt;- paste0(names(importances[[i]])[2:4], \"_\", i)\n  }\n  merged &lt;- Reduce(function(x, y) merge(x, y, by = \"Feature\", all = TRUE), importances)\n  merged[is.na(merged)] &lt;- 0  # Replace NAs with 0\n  gain_cols &lt;- grep(\"Gain\", colnames(merged), value = TRUE)\n  merged$Cumulative &lt;- rowSums(merged[, ..gain_cols])\n  return(merged[, .(Feature, Cumulative)])\n}\n\n# Merge and sort importances\nmultCumulativeImportance &lt;- merge_importances(multAllImportances)\nmultCumulativeImportance &lt;- multCumulativeImportance[order(-multCumulativeImportance$Cumulative), ]\n\n# Print cumulative feature importance\nhead(multCumulativeImportance, n=10)\n\n                                Feature Cumulative\n                                 &lt;char&gt;      &lt;num&gt;\n 1:               arm_power_pospeak_std  0.4563189\n 2:               arm_speedKin_sum_Gstd  0.2938622\n 3: head_moment_sum_change_pospeak_mean  0.2651477\n 4:                arm_accKin_sum_range  0.2164673\n 5:                      head_inter_Kin  0.2109458\n 6:         lowerbody_jerkKin_sum_Gmean  0.1823957\n 7:             head_angSpeed_sum_range  0.1567240\n 8:                arm_moment_sum_range  0.1448105\n 9:                       head_duration  0.1419503\n10:         leg_moment_sum_pospeak_mean  0.1221006\n\n\n\n\n\nPCA\nNow let’s collect 10 features per component that have highest combined ranking from PCA and XGBoost. This means that for each feature we sum up the ranking it obtained in cumulative importance (XGBoost) and loading on a principal component (PCA).\n\n# Rank the features based on XGBoost importance (cumulative)\nmultCumulativeImportance$XGB_Rank &lt;- rank(-multCumulativeImportance$Cumulative)\n\n# Load in PCA for gesture\nmult_pca &lt;- read_csv(paste0(datasets, \"PCA_top_contributors_multi.csv\"))\n\n# For each PC (PC1, PC2, PC3), rank the features based on their loadings\ncombined_ranks_per_pc &lt;- list()\n\nfor (pc in c(\"PC1\", \"PC2\", \"PC3\")) {\n  # Extract the features and loadings for the current PC\n  pca_pc_loadings &lt;- mult_pca[, c(pc, paste0(pc, \"_Loading\"))]\n  colnames(pca_pc_loadings) &lt;- c(\"Feature\", \"Loading\")\n  \n  # Rank the features based on the absolute loading values (higher loadings should get lower rank)\n  pca_pc_loadings$PCA_Rank &lt;- rank(-abs(pca_pc_loadings$Loading))\n  \n  # Merge PCA loadings with XGBoost importance ranks\n  merged_data &lt;- merge(pca_pc_loadings, multCumulativeImportance[, c(\"Feature\", \"XGB_Rank\")], by = \"Feature\")\n  \n  # Calculate combined rank by summing XGBoost rank and PCA rank\n  merged_data$Combined_Rank &lt;- merged_data$XGB_Rank + merged_data$PCA_Rank\n  \n  # Sort by the combined rank (lower rank is better)\n  sorted_data &lt;- merged_data[order(merged_data$Combined_Rank), ]\n  \n  # Select the top n features based on the combined rank for the current PC\n  top_n_features &lt;- 10  # Adjust the number of top features as needed\n  combined_ranks_per_pc[[pc]] &lt;- head(sorted_data, top_n_features)\n}\n\n# Output the top features per PC based on combined ranking\nhead(combined_ranks_per_pc, n=10)\n\n$PC1\n                            Feature    Loading PCA_Rank XGB_Rank Combined_Rank\n196    lowerbody_speedKin_sum_range 0.08494744        1       17            18\n98           head_angJerk_sum_range 0.08114843        9       14            23\n94            head_angJerk_sum_Gstd 0.08057174       10       29            39\n93           head_angJerk_sum_Gmean 0.07951331       14       28            42\n193 lowerbody_speedKin_sum_integral 0.07981523       12       39            51\n128      head_speedKin_sum_integral 0.07633696       37       18            55\n161   lowerbody_accKin_sum_integral 0.07643321       36       21            57\n191    lowerbody_speedKin_sum_Gmean 0.07389508       49       13            62\n89            head_angAcc_sum_Gmean 0.07122083       57       11            68\n91         head_angAcc_sum_integral 0.07661845       34       34            68\n\n$PC2\n                               Feature     Loading PCA_Rank XGB_Rank\n64                envelope_change_Gstd -0.08227254     27.0       25\n157      leg_speedKin_sum_pospeak_mean  0.07903907     35.5       19\n132        leg_accKin_sum_pospeak_mean  0.07903907     35.5       24\n102            head_angSpeed_sum_range -0.07046288     70.0        7\n109      head_jerkKin_sum_pospeak_mean  0.08860597     14.0       68\n53        arm_speedKin_sum_pospeak_std -0.07747848     41.5       52\n58                           COPc_Gstd  0.08674708     22.0       76\n117 head_moment_sum_change_pospeak_std -0.07674698     45.0       58\n6                  arm_angAcc_sum_Gstd  0.07178235     67.0       43\n61                    COPc_pospeak_std  0.07857052     38.0       77\n    Combined_Rank\n64           52.0\n157          54.5\n132          59.5\n102          77.0\n109          82.0\n53           93.5\n58           98.0\n117         103.0\n6           110.0\n61          115.0\n\n$PC3\n                               Feature     Loading PCA_Rank XGB_Rank\n47               arm_power_pospeak_std -0.09134019       24        1\n175        lowerbody_jerkKin_sum_Gmean  0.09121169       25        6\n104                      head_duration -0.08806084       28        9\n179  lowerbody_jerkKin_sum_pospeak_std  0.10241640       17       20\n42                arm_moment_sum_range -0.08711107       30        8\n178 lowerbody_jerkKin_sum_pospeak_mean  0.12010976        6       40\n201         pelvis_moment_sum_integral -0.12537799        4       56\n102            head_angSpeed_sum_range -0.07039109       59        7\n49              arm_speedKin_sum_Gmean  0.07121858       57       12\n69               envelope_pospeak_mean  0.08530688       33       49\n    Combined_Rank\n47             25\n175            31\n104            37\n179            37\n42             38\n178            46\n201            60\n102            66\n49             69\n69             82\n\n\nFor modelling, we want to pick three features per component. Which would it be in this case?\n\n# Number of top features to display\ntop_n_features &lt;- 3\n\n# Print the top 3 features per component\nfor (pc in c(\"PC1\", \"PC2\", \"PC3\")) {\n  cat(\"\\nTop 3 Features for\", pc, \":\\n\")\n  \n  # Get the top 3 features based on combined rank for the current PC\n  top_features &lt;- head(combined_ranks_per_pc[[pc]], top_n_features)\n  \n  # Print the results\n  print(top_features[, c(\"Feature\", \"XGB_Rank\", \"PCA_Rank\", \"Combined_Rank\")])\n}\n\n\nTop 3 Features for PC1 :\n                         Feature XGB_Rank PCA_Rank Combined_Rank\n196 lowerbody_speedKin_sum_range       17        1            18\n98        head_angJerk_sum_range       14        9            23\n94         head_angJerk_sum_Gstd       29       10            39\n\nTop 3 Features for PC2 :\n                          Feature XGB_Rank PCA_Rank Combined_Rank\n64           envelope_change_Gstd       25     27.0          52.0\n157 leg_speedKin_sum_pospeak_mean       19     35.5          54.5\n132   leg_accKin_sum_pospeak_mean       24     35.5          59.5\n\nTop 3 Features for PC3 :\n                        Feature XGB_Rank PCA_Rank Combined_Rank\n47        arm_power_pospeak_std        1       24            25\n175 lowerbody_jerkKin_sum_Gmean        6       25            31\n104               head_duration        9       28            37\n\n\nNow we are ready to model the most predictive features to get deeper understanding of their causal relationship with our predictive variable, i.e., communicative attempt.",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Putting in the Effort: Modulation of Multimodal Effort in Communicative Breakdowns during a Gestural-Vocal Referential Game",
    "section": "",
    "text": "Authors: Šárka Kadavá, Wim Pouw, Susanne Fuchs, Judith Holler, Aleksandra Ćwiek\n\n\n\nMultimodal animation\n\n\nThis is a material to a study associated with ViCom project On the FLExibility and Stability of Gesture-speecH Coordination (FLESH) (see more here).\nThis website documents data processing pipeline developed for the project Modulation of Multimodal Effort in Communicative Breakdowns during a Gestural-Vocal Referential Game. The pipeline covers every step from raw data ingestion to feature extraction and analysis, focusing on synchronizing and processing motion, audio, and derived signal data.\nThe repository associated with this project can be found at Github.\n\n\nThis study investigates whether (and how) people become more effortful when they attempt to resolve misunderstanding in a novel-communication gestural-vocal task.\nIn the task, people are asked to …..\nWe recorded ….\nYou can read more about the theoretical reasonings in the introduction.\n\n\n\nThis study has been preregistered in two phases.\nIn phase I, we preregistered the experimental design, laboratory setup and power analysis. The preregistration is available at the OSF Registries\nIn phase II, we preregister the research questions and hypothesis, together with code pipeline covering pre-processing, processing and the analysis itself. The preregistration is available at the OSF Registries\n\n\n\n[✅] Preregistration of data collection  [✅] Data collection completed  [] Preregistration of analysis and processing steps  [] Preprint published  [] Manuscript published  [] Data available at open access repository \n\n\n\n\nThis study builds on multi-step pipeline that serves to - extract the raw data - process them - extract relevant features - analyze with regards to research questions\nIn this workflow, each step builds on the previous one. However, it is possible to use parts of the workflow for different purposes.\n\n\n\nIn Pre-Processing I: from XDF to raw files we load and clean raw XDF data, align streams, and prepare for downstream processing.\n\n\n\n\n\nIn Motion tracking I: Preparation of videos we crop video recordings and prepare them for motion capture.\nIn Motion tracking II: 2D pose estimation via OpenPose we use OpenPose for 2D pose estimation.\nMotion tracking III: Triangulation via Pose2sim we convert 2D coordinates to 3D using pose2sim.\nMotion tracking IV: Modeling inverse kinematics and dynamics we compute inverse kinematics and dynamics using OpenSim\n\n\n\n\n\nIn Processing I: Motion tracking and balance we clean and interpolate motion signals, and extract derivatives such as speed, acceleration and jerk.\nProcessing II: Acoustics we extract relevant acoustic features.\nProcessing III: Merging multimodal data we merge motion and acoustic time series.\n\n\n\n\n\nIn Movement annotation I: Preparing training data and data for classifier we prepare our multimodal time series for training purposes.\nIn Movement annotation II: Training movement classifier, and annotating timeseries data we train and evaluate classifiers for movement detection.\nIn Movement annotation III: Computing interrater agreement between manual and automatic annotation we evaluate inter-annotator agreement.\n\n\n\n\n\nIn Final merge we merge annotations with timeseries.\n\n\n\n\n\nIn Computing concept similarity using ConceptNet word embeddings: we assess semantic similarity between concepts using ConceptNet.\n\n\n\n\n\nIn Extraction of effort-related features we extract features from the multimodal time series for modelling purposes.\n\n\n\n\n\nIn Exploratory Analysis I: Using PCA to identify effort dimensions we explore dimensionality of extracted features using Principal Component Analysis.\nExploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution we assess feature importance using eXtreme Gradient Boosting.\n\n\n\n\n\nStatistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort we build causal and statistical models testing our hypothesis.\n\n\n\n\n\n\nWe would like to thank to all participants of this study. Special thanks belong also to the Donders lab coordinator Jiska Koemans and the Donders research integrity officer Miriam Kos. Lastly, thank to members of Donders Technical Support Group, namely Erik van den Berge, Norbert Hermesdorf, Gerard van Oijen, Maarten Snellen and Pascal de Water, for helping with the technical setup.\n\n\n\nadd contact",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#this-study",
    "href": "index.html#this-study",
    "title": "Putting in the Effort: Modulation of Multimodal Effort in Communicative Breakdowns during a Gestural-Vocal Referential Game",
    "section": "",
    "text": "This study investigates whether (and how) people become more effortful when they attempt to resolve misunderstanding in a novel-communication gestural-vocal task.\nIn the task, people are asked to …..\nWe recorded ….\nYou can read more about the theoretical reasonings in the introduction.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#two-phase-preregistration",
    "href": "index.html#two-phase-preregistration",
    "title": "Putting in the Effort: Modulation of Multimodal Effort in Communicative Breakdowns during a Gestural-Vocal Referential Game",
    "section": "",
    "text": "This study has been preregistered in two phases.\nIn phase I, we preregistered the experimental design, laboratory setup and power analysis. The preregistration is available at the OSF Registries\nIn phase II, we preregister the research questions and hypothesis, together with code pipeline covering pre-processing, processing and the analysis itself. The preregistration is available at the OSF Registries",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#updates",
    "href": "index.html#updates",
    "title": "Putting in the Effort: Modulation of Multimodal Effort in Communicative Breakdowns during a Gestural-Vocal Referential Game",
    "section": "",
    "text": "[✅] Preregistration of data collection  [✅] Data collection completed  [] Preregistration of analysis and processing steps  [] Preprint published  [] Manuscript published  [] Data available at open access repository",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#pipeline-overview",
    "href": "index.html#pipeline-overview",
    "title": "Putting in the Effort: Modulation of Multimodal Effort in Communicative Breakdowns during a Gestural-Vocal Referential Game",
    "section": "",
    "text": "This study builds on multi-step pipeline that serves to - extract the raw data - process them - extract relevant features - analyze with regards to research questions\nIn this workflow, each step builds on the previous one. However, it is possible to use parts of the workflow for different purposes.\n\n\n\nIn Pre-Processing I: from XDF to raw files we load and clean raw XDF data, align streams, and prepare for downstream processing.\n\n\n\n\n\nIn Motion tracking I: Preparation of videos we crop video recordings and prepare them for motion capture.\nIn Motion tracking II: 2D pose estimation via OpenPose we use OpenPose for 2D pose estimation.\nMotion tracking III: Triangulation via Pose2sim we convert 2D coordinates to 3D using pose2sim.\nMotion tracking IV: Modeling inverse kinematics and dynamics we compute inverse kinematics and dynamics using OpenSim\n\n\n\n\n\nIn Processing I: Motion tracking and balance we clean and interpolate motion signals, and extract derivatives such as speed, acceleration and jerk.\nProcessing II: Acoustics we extract relevant acoustic features.\nProcessing III: Merging multimodal data we merge motion and acoustic time series.\n\n\n\n\n\nIn Movement annotation I: Preparing training data and data for classifier we prepare our multimodal time series for training purposes.\nIn Movement annotation II: Training movement classifier, and annotating timeseries data we train and evaluate classifiers for movement detection.\nIn Movement annotation III: Computing interrater agreement between manual and automatic annotation we evaluate inter-annotator agreement.\n\n\n\n\n\nIn Final merge we merge annotations with timeseries.\n\n\n\n\n\nIn Computing concept similarity using ConceptNet word embeddings: we assess semantic similarity between concepts using ConceptNet.\n\n\n\n\n\nIn Extraction of effort-related features we extract features from the multimodal time series for modelling purposes.\n\n\n\n\n\nIn Exploratory Analysis I: Using PCA to identify effort dimensions we explore dimensionality of extracted features using Principal Component Analysis.\nExploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution we assess feature importance using eXtreme Gradient Boosting.\n\n\n\n\n\nStatistical Analysis: Modelling the Effect of Communicative Attempt (H1) and Answer Similarity (H2) on Effort we build causal and statistical models testing our hypothesis.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Putting in the Effort: Modulation of Multimodal Effort in Communicative Breakdowns during a Gestural-Vocal Referential Game",
    "section": "",
    "text": "We would like to thank to all participants of this study. Special thanks belong also to the Donders lab coordinator Jiska Koemans and the Donders research integrity officer Miriam Kos. Lastly, thank to members of Donders Technical Support Group, namely Erik van den Berge, Norbert Hermesdorf, Gerard van Oijen, Maarten Snellen and Pascal de Water, for helping with the technical setup.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Putting in the Effort: Modulation of Multimodal Effort in Communicative Breakdowns during a Gestural-Vocal Referential Game",
    "section": "",
    "text": "add contact",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "01_XDF_processing/xdf_workflow.html",
    "href": "01_XDF_processing/xdf_workflow.html",
    "title": "Pre-Processing I: from XDF to raw files",
    "section": "",
    "text": "Overview\nData for this experiment has been collected been collected using LabStreamLayer, allowing for recording of precisely synchronized streams (see documentation).\nThese streams include: - Audio stream (16kHz) - Webcam Frame stream (ca. 60 Hz) - Marker stream (sent from custom buttonbox) - Balance board stream (500 Hz)\nLSL outputs single file in Extensible Data Format (XDF). This file contains all the data streams in form of a timeseries. In this script, we extract all the streams from the file and create raw (audio, video, and other) files for further processing.\nAdditionally, in this script, we - correct faulty delimitations of trials - align 16kHz LSL audio and external 48kHz audio that has been recorded outside of LSL, and cut this aligned audio to trial-sized files - combine video and audio into single file\nMore information about the complete lab setup can be found in our previous preregistration where we archived list of all equipment and software used in the experiment. It is available here at OSF Registries.\n\n\nCode to prepare the environment\n# import packages\nimport os\nimport pyxdf\nimport glob\nimport pandas as pd\nimport numpy as np\nimport wave, struct, random\nfrom scipy.io import wavfile\nimport noisereduce as nr\nimport cv2\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio, Video\nimport tempfile\nimport subprocess\nimport ffmpeg\n\n\n# Set folders\ncurfolder = os.getcwd()\n\n# If folder data doesn't exist, create it\nif not os.path.exists(curfolder+'\\\\data\\\\'):\n    os.makedirs(curfolder+'\\\\data\\\\')\ndatafolder = curfolder+'\\\\data\\\\'\n\n# Also error_logs\nif not os.path.exists(datafolder+'\\\\error_logs\\\\'):\n    os.makedirs(datafolder+'\\\\error_logs\\\\')\nerrorlogs = datafolder+'\\\\error_logs\\\\'\n\nexperiment_to_process = curfolder + '\\\\..\\\\00_RAWDATA\\\\'\n\n# Also CsvDataTS_raw\nif not os.path.exists(datafolder+'\\\\Data_processed\\\\CsvDataTS_raw\\\\'):\n    os.makedirs(datafolder+'\\\\Data_processed\\\\CsvDataTS_raw\\\\')\noutputfolder = datafolder+'\\\\Data_processed\\\\CsvDataTS_raw\\\\' # outputfolder raw\n\n# Audio in\nif not os.path.exists(datafolder+'\\\\Data_processed\\\\CsvDataTS_raw\\\\Audio\\\\'):\n    os.makedirs(datafolder+'\\\\Data_processed\\\\CsvDataTS_raw\\\\Audio\\\\')\n\n# Also Data_trials\nif not os.path.exists(datafolder+'\\\\Data_processed\\\\Data_trials\\\\'):\n    os.makedirs(datafolder+'\\\\Data_processed\\\\Data_trials\\\\')\ntrialfolder = datafolder+'\\\\Data_processed\\\\Data_trials\\\\' # outputfolder trialbased\n\n\n# Add audio in\nif not os.path.exists(datafolder+'\\\\Data_processed\\\\Data_trials\\\\Audio\\\\'):\n    os.makedirs(datafolder+'\\\\Data_processed\\\\Data_trials\\\\Audio\\\\')\n\n# Get all the folders in the target folder\ndatafolders = glob.glob(experiment_to_process+'*\\\\')\n\n# Extract the folder IDs\ndatafolders_id = [x.split('\\\\')[-2] for x in datafolders]\n\n# print(curfolder)\n# print(targetfolder)\nprint(datafolders_id)\n\n# Identify all xdf files and all the associated triallist info\nxdffiles = []\ntrialdatas = []\n\nfor i in datafolders_id:\n    file = glob.glob(experiment_to_process+i+'\\\\*.xdf')\n    trialfile = experiment_to_process+i+'\\\\'+i+'_results'+'.csv'\n    trialdatas.append(trialfile)\n    xdffiles.extend(file)\n\n# These are the xdf files we need to process\n#print(xdffiles[0:10])\n#print(trialdatas[0:10])\n\n\n\n\nExtracting streams from XDF file\nUsing pyxdf package, we can extract the streams from the XDF file.\nFirst, we extract streams for the whole recording session such that we end up with a file per each stream that represents the whole timeseries from the start to the end of the recording.\nThen, we cut the whole streams into trial-sized chunks using the marker stream.Marker stream is saving presses from button box that has been administered by the experimentor. The presses indicates the start and end of each trial. We also collect the trial start and end time via the PsychoPy custom script that is used to run the experiment. We use this information to connect the marker to the trial metadata such as - what kind of concept is being performed - is this trial baselina, correction 1, correction 2 - who performs this trial - in which modality is this trial\n(Note that this code takes some time to execute.)\n\n\nCode with functions\n# Audio write function\ndef to_audio(fileloc, timeseries, samplerate = 16000, channels = 1):\n    obj = wave.open(fileloc,'w')\n    obj.setnchannels(channels) # mono\n    obj.setsampwidth(2)\n    obj.setframerate(float(samplerate))\n    for i in timeseries:\n        data = struct.pack('&lt;h', int(i[0]))\n        obj.writeframesraw( data )\n    obj.close()\n\n# Function to retrieve closest value\ndef find_closest_value_and_retrieve(df, target_number, target_column, retrieve_column):\n    # Get the absolute differences between a value in column and the target number\n    differences = abs(df[target_column] - target_number)\n    \n    # Find the index of the minimum difference\n    min_difference_index = differences.idxmin()\n    \n    # Retrieve the corresponding value from the column\n    result_value = df.loc[min_difference_index, retrieve_column]\n\n    return result_value\n\ndef write_video(vidloc, fourcc, originalfps, frameWidth, frameHeight, capture, frames):\n    out = cv2.VideoWriter(vidloc, fourcc, fps = originalfps, frameSize = (int(frameWidth), int(frameHeight)))\n    #print('Looping over frames')\n\n    # index the frames of the current trial \n    for fra in frames:\n        capture.set(cv2.CAP_PROP_POS_FRAMES, fra)\n        ret, frame = capture.read()\n        if ret:\n            out.write(frame)\n            \n        if not ret:\n            print('a frame was dropped: ' + str(fra))\n        \n    capture.release()\n    out.release()\n\n\n\nerrorlist = []\n\nfor dat in datafolders_id:\n    print('Loading in data from participant: ' + dat)\n    trialdata = pd.read_csv(experiment_to_process+dat+'\\\\'+dat+'_results.csv', sep=\",\")\n    #print(trialdata)\n    \n    # Get the xdf file\n    files = glob.glob(experiment_to_process+dat+'\\\\*.xdf')\n    streams, header = pyxdf.load_xdf(files[0])   \n    # We go through each stream and save it as a csv\n    for stream in streams:\n        timeseriestype = stream['info']['name'][0]\n        print(timeseriestype)\n        samplerate = round(float(stream['info']['nominal_srate'][0]))\n        # In the xdf loop over the streams and save it as csv if not yet exists\n        channelcount = stream['info']['channel_count'][0]\n        print('working on stream: ' + timeseriestype + '  with a channel count of ' + str(channelcount) +'\\n and a sampling rate of ' + str(samplerate))\n        timevec = stream['time_stamps']\n        timeseries = stream['time_series']\n        matrix_aux = np.vstack([np.transpose(timevec),np.transpose(timeseries)])\n        matrix = np.transpose(matrix_aux)\n        df_lab = pd.DataFrame(matrix)\n        df_lab.to_csv(outputfolder+dat+'_'+timeseriestype+'_nominal_srate'+str(samplerate)+'.csv',index=False)\n        # For audio stream also create a wav file\n        if timeseriestype == 'Mic':\n            wavloc = outputfolder+'Audio/'+dat+'_'+timeseriestype+'_nominal_srate'+str(samplerate)+'.wav'\n            to_audio(wavloc, timeseries)\n            # Load data\n            rate, data = wavfile.read(wavloc)\n            # Perform noise reduction\n            reduced_noise = nr.reduce_noise(y=data, sr=rate, n_std_thresh_stationary=1.5,stationary=True)\n            wavloc2 = outputfolder+'Audio/'+dat+'_'+timeseriestype+'_nominal_srate'+str(samplerate)+'_denoised.wav'\n            wavfile.write(wavloc2, rate, reduced_noise)\n\n        print('done with processing a complete time series and audio data')\n        print('we will now start making trial snipped data') \n\n        # Load MyMarkerStream\n        markers = pd.read_csv(outputfolder+dat+'_MyMarkerStream_nominal_srate0.csv')\n        \n        # Cut all timeseries to trial level based on the markers\n        if timeseriestype != 'MyMarkerStream':\n            \n            beginlist = []\n            endlist = []\n            timestamps = []\n            timestamps_2 = []\n            tpose_starts = []\n            tpose_ends = []\n\n            # Iterate over markers and save times of trial starts and ends\n            for row in markers.iterrows():\n                if 'Trial_start' in row[1].iloc[1] or 'Practice trial starts' in row[1].iloc[1]:\n                    beginlist.append(row[1].iloc[0])\n                if 'Trial_end' in row[1].iloc[1] or 'Practice trial ends' in row[1].iloc[1]:\n                    endlist.append(row[1].iloc[0])\n                if 'Experiment_start' in row[1].iloc[1]:\n                    timestamps.append(row[1].iloc[0])\n                if 'New block starts' in row[1].iloc[1]:\n                    timestamps_2.append(row[1].iloc[0])\n                if 'Tpose starts' in row[1].iloc[1]:\n                    tpose_starts.append(row[1].iloc[0])\n                if 'Tpose ends' in row[1].iloc[1]:\n                    tpose_ends.append(row[1].iloc[0])\n                    \n            # Converting coefficient for lsl to psychopy time\n            exp_start_pp = float(trialdata['exp_start'][0])\n            block_start_pp = float(trialdata['block_start'][0])\n            \n            # Get to lsl_to_pp coefficient\n            if timestamps != []:\n                lsl_to_pp = timestamps[0] - exp_start_pp\n            else:\n                lsl_to_pp = timestamps_2[0] - block_start_pp\n        \n            # Now we can proceed to cutting   \n            for i in range(len(beginlist)):\n                # Prepare the range of the trial\n                begin = beginlist[i]\n                end = endlist[i]\n                indices = (df_lab.loc[:,0] &gt; begin) & (df_lab.loc[:,0] &lt; end)\n                beginst = min(df_lab.loc[:,0]) # start time of the timeseries\n                endst = max(df_lab.loc[:,0])  # end time of the timeseries\n                subset = df_lab.loc[indices, :]\n                # Convert the beginst to psychopy time\n                beginst_pp = begin - lsl_to_pp\n                # Now find in trialdata the closest time to the beginst_pp to gather info\n                # Whether it is practice or trial\n                practice = find_closest_value_and_retrieve(trialdata, beginst_pp, 'trial_start', 'practice')\n                if practice == 'practice':\n                    trialtype = 'pr'\n                else:\n                    trialtype = 'trial'\n                \n                # Which participant it is\n                cycle = find_closest_value_and_retrieve(trialdata, beginst_pp, 'trial_start', 'cycle')\n                if cycle == 0:\n                    participant = 'p0'\n                else:\n                    participant = 'p1'\n\n                # What concept it is\n                word = find_closest_value_and_retrieve(trialdata, beginst_pp, 'trial_start', 'word')\n                # Modality\n                modality = find_closest_value_and_retrieve(trialdata, beginst_pp, 'trial_start', 'modality')\n                # Correction, if applicable\n                correction_info = find_closest_value_and_retrieve(trialdata, beginst_pp, 'trial_start', 'correction')\n                if correction_info == 0:\n                    correction = '_c0'\n                elif correction_info == 1:\n                    correction = '_c1'\n                elif correction_info == 2:\n                    correction = '_c2'\n                else:\n                    correction = ''\n                \n                # Continue saving\n                if(len(subset.axes[0])&lt;2):\n                    errorlist.append(dat + \" for \"+ timeseriestype + \" for trial \" + str(i) + 'NO DATA WITHIN RANGE...')\n                if(len(subset.axes[0])&gt;2):\n                     # Save subset to csv\n                      subset.to_csv(trialfolder+dat+'_'+trialtype+'_'+ str(i) +'_'+timeseriestype+'_nominal_srate'+str(samplerate)+'_'+participant+'_'+word+'_'+modality+correction+'.csv', index=False)\n                      if timeseriestype == 'Mic':\n                            wavloc = trialfolder+'Audio/'+dat+'_'+trialtype+'_'+ str(i) +'_'+timeseriestype+'_nominal_srate'+str(samplerate)+'_'+participant+'_'+word+'_'+modality+correction+'.wav'\n                            to_audio(wavloc, timeseries[indices])\n                          # Also apply denoising\n                            reduced_noiseclip = reduced_noise[indices]\n                            wavloc2 = trialfolder+'Audio/'+dat+'_'+trialtype+'_'+ str(i) +'_'+timeseriestype+'_nominal_srate'+str(samplerate)+'_'+participant+'_'+word+'_'+modality+correction+'_denoised.wav'\n                            wavfile.write(wavloc2, rate, reduced_noiseclip)\n            \n            # Get information about the tpose for camera\n            if timeseriestype == 'MyWebcamFrameStream':\n                for i in range(len(tpose_starts)):\n                    begin = tpose_starts[i]\n                    end = tpose_ends[i]\n                    indices = (df_lab.loc[:,0] &gt; begin) & (df_lab.loc[:,0] &lt; end)\n                    beginst = min(df_lab.loc[:,0])\n                    endst = max(df_lab.loc[:,0])\n                    subset = df_lab.loc[indices, :]\n                    # Save subset to csv\n                    subset.to_csv(trialfolder+dat+'_'+'tpose_'+ str(i) +'_'+timeseriestype+'_nominal_srate'+str(samplerate)+'.csv', index=False)\n        \n        # After every stream we'll save the error log\n        errors = pd.DataFrame(errorlist, columns=['file_error'])\n        # Get todays date\n        today = pd.Timestamp(\"today\").strftime(\"%Y_%m\")\n        file_path = errorlogs+'error_log_cuttingtrails' + today + '.csv'\n        errors.to_csv(file_path, index=False) \n        \nprint('Were done: proceed to snipping videos to triallevel')\n\nNow we have a csv file of a timeseries for each trial. The timeseries include - center of pressure (from the balance board stream) - video frame number - sound pressure (from the audio stream)\nHere is an audio example of a trial:\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nHere is a visualization of center of pressure (all four channels) for a trial:\n\n\n\n\n\n\n\n\n\n\n\nWriting video files\nNow we have almost all data in desirable form. However, we still need to write videos for each trial because currently, we have only the frame numbers saved for each trial.\nWe use these files to access the range of frames in the original raw video file for the whole session, and cut it out from it\nYou will notice that beside regular trial files, there is few files named ‘tpose’. This is a specific kind of trial that preceeds the whole experiment. Before they start, participants are asked to stand straight with their arms extended to the side (so to resemble a T-shape). We use this video later for scaling a computational biomechanical model (see OpenSim script).\n\n\nCode to prepare folders\n# This is a folder with timeseries\ntsfolder = datafolder+'\\\\Data_processed\\\\Data_trials\\\\'\n# Keep only the csv files\ntsfiles = glob.glob(tsfolder+'*.csv')\n# This is where the raw long video is\nvideofolder = curfolder + '\\\\..\\\\00_RAWDATA\\\\'\n\n\n\n# Loop through the csv's in tsfolder that has string 'MyWebcamStream' in name\nfor file in tsfiles:\n    if 'MyWebcamFrameStream' in file:\n        print('Now processing file '+ file)\n\n        filename = file.split('\\\\')[-1].split('.')[0]\n\n        # If it is a tpose file, the name looks a bit different\n        if 'tpose' in filename:\n            sessionIndex = filename.split('_')[0] + '_' + filename.split('_')[1]\n            videoname = sessionIndex+'_tpose_'+filename.split('_')[3]\n\n        else:\n            # The name looks like this 0_1_trial_0_MyWebcamFrameStream_nominal_srate500_p0_bitter_geluiden.csv\n            dyadIndex = filename.split('_')[0]   # this is dyad number\n            partIndex = filename.split('_')[1]   # this is part of the session\n            sessionIndex = dyadIndex + '_' + partIndex # this is the session index\n            trialIndex = filename.split('_')[3] # this is trial number\n            participant = filename.split('_')[7] # this is participant 0/1\n            word = filename.split('_')[8] # this is the concept\n            modality = filename.split('_')[9].split('.')[0] # this is the modality\n\n            # Assess the correction\n            if 'c0' in filename:\n                correction = '_c0'\n            elif 'c1' in filename:\n                correction = '_c1'\n            elif 'c2' in filename:\n                correction = '_c2'\n            else:\n                correction = ''\n\n            # Assess the trial type\n            if 'pr' in file:\n                trialtype = 'pr'\n            else:\n                trialtype = 'trial'\n\n            # Get the filename\n            videoname = sessionIndex+'_'+trialtype+'_'+ str(trialIndex) +'_'+participant+'_'+word+'_'+modality+correction\n\n        trialdata = pd.read_csv(file)\n        #print(trialdata)\n        videolong = videofolder+sessionIndex+'\\\\'+sessionIndex+'-video.avi' # this is the long video \n        begin_time = trialdata['0'].min() # begin time of the trial\n        end_time = trialdata['0'].max() # end time of the trial\n        # Get the begin and end frame\n        begin_frame = trialdata['1'].min().astype(int)\n        end_frame = trialdata['1'].max().astype(int)\n        totframes = end_frame-begin_frame # total number of frames in the trial\n        frames = range(begin_frame, end_frame) # get all the frames in trial\n        #print(frames)\n        \n        # Load in the long video\n        capture = cv2.VideoCapture(videolong) \n        originalfps = round((totframes/(end_time-begin_time)),3) # original fps\n        \n        # This is the location where the video will be saved\n        vidloc = trialfolder+videoname+'_video_raw'+'.avi'\n        # Metadata\n        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n        frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n        frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n\n        # Start writing video\n        print('Starting to write the video')\n        write_video(vidloc, fourcc, originalfps, frameWidth, frameHeight, capture, frames)\n        print('Video is done')\n\nprint('All done!')\n\nNow we have for each trial also a video.\nHere is an example:\n\n\n\n \n Your browser does not support the video tag.\n \n\n\nThis is how T-pose looks like:\n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\nCorrecting some trials\nAs mentioned, trials are meant to be delimited by button box markers that are triggered by presses from experimentor. However, due to occasional errors (e.g., participants forgot to signal start of the trial), we need to visually inspect all trials, and correct the ones that are not properly delimited.\nFor each session, we create a separate csv file where we note the correct beginning and end frame (as shown in the video itself) of the wrongly delimited trials. Then we cut these new frames from the long video - similarly as we did for the regular trials. Lastly, to make sure that other timeseries are also corrected in order to be correctly synchronize, we assess the LSL time of the new frames and cut the corresponding audio and balance board data.\n\n\nCode with functions\n# audio write function\ndef to_audio(fileloc, timeseries, samplerate = 16000, channels = 1):\n    obj = wave.open(fileloc,'w')\n    obj.setnchannels(channels) # mono\n    obj.setsampwidth(2)\n    obj.setframerate(float(samplerate))\n    for i in timeseries:\n        data = struct.pack('&lt;h', int(i))\n        obj.writeframesraw( data )\n    obj.close()\n\ndef correct_file(i, begin, end, errorlog, ts, stream):\n    # get the start and end of the corrected range of frames\n    # begin = error['frame_begin'][i]\n    # end = error['frame_end'][i]\n    if stream == 'video':\n        indexcol = 1\n    else:\n        indexcol = 0\n    # cut from the ts everything that is below start and above end\n    indices = (ts.iloc[:,indexcol] &gt; begin) & (ts.iloc[:,indexcol] &lt; end)\n    #beginst = min(ts.iloc[:,1]) #start time of the timeseries\n    #endst = max(ts.iloc[:,1])  #end time of the timeseries\n    subset = ts.loc[indices, :]\n    # save also the time of the first and last frame as new variables\n    if stream == 'video':\n        errorlog.loc[i, 'begin_time'] = subset.iloc[0,1]\n        errorlog.loc[i, 'end_time'] = subset.iloc[len(subset)-1,1]\n\n    #print(errorlog['end_time'][i])\n    # save the new timeseries using the path of the video column with _corrected as appended\n    file_name = errorlog.loc[i, stream]\n    # replace .csv by _corrected.csv\n    file_name = file_name.replace('.csv', '_corrected.csv')\n    return errorlog, file_name, subset\n\n\n\n\nCode to prepare environment\ndatafolder = curfolder + '/data/Data_processed/Data_trials/'\nerrorfolder = datafolder + 'Handlock_error/' # here we store all the corrected frame ranges\ntimeseries = curfolder + '/data/Data_processed/CsvDataTS_raw/'\n\nerrorfiles = glob.glob(errorfolder+'*.csv')\n\n\nThis is how the error file looks like:\n\n\n\n\n\n\n\n\n\n\nword\npart\ncorrection\nframe_begin\nframe_end\n\n\n\n\n0\ndansen\n1\nNaN\n19400\n19541\n\n\n1\nlangzaam\n1\nNaN\n45729\n46055\n\n\n2\nauto\n1\nNaN\n46575\n46895\n\n\n3\nsnel\n1\nNaN\n63206\n63597\n\n\n4\nbitter\n1\nNaN\n22160\n22327\n\n\n5\nwalgen\n1\nNaN\n26725\n26897\n\n\n\n\n\n\n\n\n\n\nCode to correct some faulty trials\nfor file in errorfiles:\n    # load the df\n    error = pd.read_csv(file)\n    session = file.split('\\\\')[-1].split('_')[0]\n\n    # make new columns audio, video and bb\n    error['audio'] = ''\n    error['video'] = ''\n    error['bb'] = ''\n\n    # now let's find corresponding wrong files in audio and video\n    audio = datafolder + 'Audio/'\n    csv = glob.glob(datafolder + '*.csv')\n\n    # loop over the error words\n    for i in range(len(error)):\n        # get the word\n        word = error['word'][i]\n        # get the part\n        part = error['part'][i]\n        # if the correction is not NaN, get it and transform to integer\n        if pd.isna(error['correction'][i]) == False:\n            correction = int(error['correction'][i])\n            # if correction is 0, it is c0\n            if correction == 0:\n                correction = 'c0'\n            elif correction == 1:\n                correction = 'c1'\n            else:\n                correction = 'c2'\n            \n            part = '2'\n            sessionID = session + '_' + part\n\n        else:\n            correction = ''\n            part = '1'\n            sessionID = session + '_' + part\n\n        # loop over the csv files\n        for j in range(len(csv)):\n            # if the word is in the csv file\n            if word in csv[j] and sessionID in csv[j] and correction in csv[j]:\n                # get the corresponding csv file\n                csv_file = csv[j]\n                # if the file has Mic in it, save its name to column audio\n                if 'Mic' in csv_file:\n                    error.loc[i, 'audio'] = csv_file\n                # if the file has Webcam in it, save its name to column video\n                elif 'Webcam' in csv_file:\n                    error.loc[i, 'video'] = csv_file\n                # if BalanceBoard in it, save its name to column bb\n                elif 'BalanceBoard' in csv_file:\n                    error.loc[i, 'bb'] = csv_file\n\n\n    # create begin_time and end_time columns\n    error['begin_time'] = 0.0\n    error['end_time'] = 0.0\n\n    ########################## re-cutting csv files ##############################\n\n    # now we will get to each timeseries, and correct the frame/time range\n    csv_ts = glob.glob(timeseries + '*.csv')\n\n    webcam_ts = [x for x in csv_ts if 'Webcam' in x]\n    mic_ts = [x for x in csv_ts if 'Mic_nominal' in x]\n    bb_ts = [x for x in csv_ts if 'BalanceBoard' in x]\n    \n    # loop over the error words\n    for i in range(len(error)):\n        print(error['word'][i])\n        # if the session is 0_1 load 0_1_MyWebcamFrameStream\n        if error['part'][i] == 1:\n            sessionID = session + '_1'\n            webcam_ts = timeseries + sessionID + '_MyWebcamFrameStream_nominal_srate500.csv'\n            ts_w = pd.read_csv(webcam_ts)\n            mic_ts = timeseries + sessionID + '_Mic_nominal_srate16000.csv'\n            ts_m = pd.read_csv(mic_ts)\n            bb_ts = timeseries + sessionID + '_BalanceBoard_stream_nominal_srate500.csv'\n            ts_bb = pd.read_csv(bb_ts)\n        elif error['part'][i] == 2:\n            sessionID = session + '_2'\n            webcam_ts = timeseries + session + '_2_MyWebcamFrameStream_nominal_srate500.csv'\n            ts_w = pd.read_csv(webcam_ts)\n            mic_ts = timeseries + sessionID + '_Mic_nominal_srate16000.csv'\n            ts_m = pd.read_csv(mic_ts)\n            bb_ts = timeseries + sessionID + '_BalanceBoard_stream_nominal_srate500.csv'\n            ts_bb = pd.read_csv(bb_ts)\n            # get the start and end of the corrected range of frames\n            begin = error['frame_begin'][i]\n            end = error['frame_end'][i]\n\n        # video correction \n        begin = error['frame_begin'][i]\n        end = error['frame_end'][i]\n        error, file_name, subset = correct_file(i, begin, end, error, ts_w, 'video')\n        if len(subset) == 0:\n            print('This file is empty: ' + error['video'][i])\n        else:\n            # save also the begin_time and end_time\n            error.loc[i, 'begin_time'] = subset.iloc[0,0]\n            error.loc[i, 'end_time'] = subset.iloc[len(subset)-1,0]\n            subset.to_csv(file_name, index = False)  \n\n        # audio correction\n        begin_time = error.loc[i, 'begin_time']\n        end_time = error.loc[i, 'end_time']\n        error, filename, subset = correct_file(i, begin_time, end_time, error, ts_m, 'audio')\n        if len(subset) == 0:\n            print('This file is empty: ' + error['audio'][i])\n        else:\n            subset.to_csv(filename, index = False)\n            \n        # balanceboard correction\n        error, filename, subset = correct_file(i, begin_time, end_time, error, ts_bb, 'bb')\n        if len(subset) == 0:\n            print('This file is empty: ' + error['bb'][i])\n        else:\n            subset.to_csv(filename, index = False)\n    \n    ################### rewriting audio and video ############################\n\n    # get all the csv files with _corrected in it\n    correct = glob.glob(datafolder + '*_corrected.csv')\n    audio = datafolder + 'Audio/'\n        \n    # loop over them \n    for i in range(len(correct)):\n        print(correct[i])\n        sessionID = correct[i].split('\\\\')[-1].split('_')[0] + '_' + correct[i].split('\\\\')[-1].split('_')[1]\n\n        # if it has Mic in it, create an audio\n        if 'Mic' in correct[i]:\n            file_name = correct[i]\n            # take only the last part of the path\n            file_name = file_name.split('\\\\')[-1]\n            # replace _corrected.csv by .wav\n            file_name = file_name.replace('.csv', '.wav')          \n            print(file_name)\n            wavloc = audio+file_name\n            ts = pd.read_csv(correct[i])\n            # make second column into a list\n            sound = ts.iloc[:,1].tolist()\n            to_audio(wavloc, sound)\n            # load data    \n            rate, data = wavfile.read(wavloc)\n            # perform noise reduction\n            reduced_noise = nr.reduce_noise(y=data, sr=rate, n_std_thresh_stationary=1.5,stationary=True)\n            file_name2 = file_name.replace('.wav', '_denoised.wav')\n            wavloc2 = audio + file_name2\n            wavfile.write(wavloc2, rate, reduced_noise)\n\n        # if it has webcam in it, create a video\n        elif 'Webcam' in correct[i]:\n            print('This is a video')\n            ts = pd.read_csv(correct[i])\n            videolong = experiment_to_process+ '\\\\' + sessionID + '\\\\' + sessionID + '-video.avi'\n            print(videolong)\n            begin_time = ts['0'].min() # begin time of the trial\n            end_time = ts['0'].max() # end time of the trial\n            # get the begin and end frame\n            begin_frame = ts['1'].min().astype(int)\n            end_frame = ts['1'].max().astype(int)\n            totframes = end_frame-begin_frame # total number of frames in the trial\n            frames = range(begin_frame, end_frame) #get all the frames in trial\n            #load in the long video\n            print('Loading the original video')\n            capture = cv2.VideoCapture(videolong) \n            originalfps = round((totframes/(end_time-begin_time)),3)\n            print('original fps: '+str(originalfps))\n            \n            # filename\n            filename = correct[i].split('\\\\')[-1]\n            # replace MyWebcamFrameStream_nominal_srate500_ with ''\n            filename = filename.replace('MyWebcamFrameStream_nominal_srate500_', '')\n            # replace csv. with ''\n            filename = filename.replace('.csv', '')\n            # this is the location where the video will be saved\n            vidloc = datafolder + filename + '_video_raw' + '.avi'\n            # video metadata\n            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n            frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n            frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n            # start writing video\n            print('Starting to write the video')\n            write_video(vidloc, fourcc, originalfps, frameWidth, frameHeight, capture, frames)\n        else:\n            print('BB do not need any processing')\n\n    print('All streams have been corrected')\n\n\nNow we have all the data streams (i.e., audio, video, balance board) as single csv file per trial wherein each trial correctly starts with the beginning of the performance and capture the whole behaviour until the end of the performance.\nAdditionally, we also have wav file for each trial and a video file for each trial.\n\n\nAligning high-sampling audio and cutting it to trial-sized files\nAlongside the 16kHz audio stream that is recorded within LSL, we have also recorded additional 48kHz audio from the same source (see method preregistration).\nHowever, this audio is not natively synchronized with the rest of the data. However, because both audios come from the same source and record the same event, we can align them using cross-correlation.\n\n\nCode to load packages and prepare the environment\nfrom __future__ import print_function\nfrom shign import shift_align\nimport numpy as np\nimport wave\nimport soundfile as sf\nimport noisereduce as nr\nfrom scipy.io import wavfile\n\naudio_48 = experiment_to_process\naudio_48_files = glob.glob(audio_48 + '**/*.wav', recursive=True)\naudio_16 = curfolder + '\\\\data\\\\Data_processed\\\\CsvDataTS_raw\\\\Audio\\\\'\naudio_16_files = glob.glob(audio_16 + '*denoised.wav')\n\n\nWe will use shign package (see Github) that defines time shift of the two audios by looking at the amount of shift that maximizes the correlation between loudness envelopes of the audios.\nBefore aligning the audios, we first denoise the 48kHz audio and downsample it to 16kHz to match the LSL audio. Then we use the shift_aling function from the shign package to align the audios.\n\nfor file48 in audio_48_files:\n    print('working on' + file48)\n    # get session ID\n    sessionID = file48.split('\\\\')[-1].split('.')[0].split('_')[0] + '_' + file48.split('\\\\')[-1].split('.')[0].split('_')[1]\n    # find the corresponding 16kHz file\n    file16 = [x for x in audio_16_files if sessionID in x][0]\n    print('corresponding file:' + file16)\n\n    # first we reduce noise in the 48kHz file\n    rate, data = wavfile.read(file48)\n    reduced_noise = nr.reduce_noise(y=data, sr=rate, n_std_thresh_stationary=1.5,stationary=True)#\n    # replace .wav by _denoised.wav\n    wavloc2 = file48.replace('.wav', '_denoised.wav')\n    wavfile.write(wavloc2, rate, reduced_noise)\n\n    # now open the new denoised file\n    file48_newpath = wavloc2\n    audio48_new = wave.open(file48_newpath, 'rb')\n    audio16 = wave.open(file16, 'rb')\n\n    # get both audio files  \n    audio_data1 = np.frombuffer(audio16.readframes(-1), dtype=np.int16)\n    sample_rate1 = audio16.getframerate()\n    audio_data2 = np.frombuffer(audio48_new.readframes(-1), dtype=np.int16)\n    sample_rate2 = audio48_new.getframerate()\n\n    print(sample_rate1, sample_rate2)\n\n    my_aligned_audio1, my_aligned_audio2 = shift_align(audio_a=file16, audio_b=file48_newpath, sr_a=16000, sr_b=48000, align_how='pad_and_crop_one_to_match_other')\n\n    # save the output\n    wav2_path = file48_newpath.replace('.wav', '_aligned.wav')\n    sf.write(wav2_path, my_aligned_audio2, 48000)\n\n\n\nCode with functions\ndef find_closest_rows(df, target_number):\n    # Calculate the absolute difference between each value in the 'trial_start' column and the target number\n    df['abs_difference'] = abs(df.iloc[:, 0] - target_number)\n    \n    # Find the row with the smallest absolute difference for each row\n    closest_rows = df[df['abs_difference'] == df['abs_difference'].min()]\n\n    return closest_rows\n\n\nNow we have 48kHz audio that is aligned with the rest of the data collected via LSL. In order to be able to use this audio in further processing, we too cut it to trial-sized files.\nBecause both audio streams have stable sampling rate, we can simply use the trial-sized files of the 16Khz audio, find their indices in the whole-session timeseries we created earlier, apply a conversion coefficient to the indices delimiting this trial to get the indices for the 48kHz audio, and cut the 48kHz audio accordingly.\nThe conversion coeeficient is sr2/sr1, where sr1 is the sampling rate of the 16kHz audio and sr2 is the sampling rate of the 48kHz audio. In our case sr1=16000 and sr2=48000, so the conversion coefficient is 3.\n\n# setup folders\naudio_48_files = glob.glob(experiment_to_process + '**/*aligned.wav', recursive=True)\nts_audio = curfolder + '\\\\data\\\\Data_processed\\\\CsvDataTS_raw\\\\'\n\n# get the whole-session audio timeseries\ncsvfiles = glob.glob(ts_audio + '*.csv')\nts_audio_files = [x for x in csvfiles if 'Mic' in x] # keep only those that have Mic\n\n# get trial-sized audio timeseries\ntrials = curfolder + '\\\\data\\\\Data_processed\\\\Data_trials\\\\'\ncsvfiles = glob.glob(trials + '*.csv')\nts_audio_trials = [x for x in csvfiles if 'Mic' in x] # keep only those that have Mic\n\n# here we will store the new audio\naudio_trials_48 = curfolder+'\\\\data\\\\Data_processed\\\\Data_trials\\\\Audio_48'\n\n# this is our coefficient\nconvert = 3 # 48000/16000\n\nfor file in ts_audio_files:\n    print('working on' + file)\n    # get session ID\n    sessionID = file.split('\\\\')[-1].split('.')[0].split('_')[0] + '_' + file.split('\\\\')[-1].split('.')[0].split('_')[1]\n\n    # find the corresponding 48kHz file\n    file48 = [x for x in audio_48_files if sessionID in x][0]\n    print('corresponding file: ' + file48)\n\n    # open audio\n    audio48 = wave.open(file48, 'rb')\n    data48 = np.frombuffer(audio48.readframes(-1), dtype=np.int16)\n\n    # open the csv file\n    ts_audio = pd.read_csv(file)\n\n    # now get all files that are in ts_audio_files and have same sessionID\n    audiotrials = [x for x in ts_audio_trials if sessionID in x]\n\n    for trial in audiotrials:\n        print('working on trial: ' + trial)\n        # open the csv file\n        df = pd.read_csv(trial)\n\n        # first and last value\n        start = df.iloc[0, 0]\n        end = df.iloc[len(df)-1, 0]\n\n        # find the closest row in ts_audio\n        closest_row_start = find_closest_rows(ts_audio, start)\n        start_index = closest_row_start.index[0]    \n        closest_row_end = find_closest_rows(ts_audio, end)\n        end_index = closest_row_end.index[0]\n\n        # convert\n        start_index_2 = start_index*convert\n        end_index_2 = end_index*convert\n        print('new indices ', start_index_2, end_index_2)\n\n        # cut the data48\n        data2_trial = data48[int(start_index_2):int(end_index_2)]\n        filename_new = trial.split('\\\\')[-1].split('.')[0]\n        # replace 16000 with 48000\n        filename_new = filename_new.replace('16000', '48000')\n        file_path = audio_trials_48 + '\\\\' + filename_new + '.csv'\n        # save the csv to audio_48\n        df = pd.DataFrame(data2_trial)\n        df.to_csv(file_path, index=False)\n\n        # now to audio\n        wavloc = audio_trials_48 + '\\\\' + filename_new + '.wav'\n        to_audio(wavloc, data2_trial, samplerate = 48000)\n\nThis is an example of the newly cut 48kHz audio\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\nConcatenating audio and video\nWe do not necessarilly need to have video and audio combined, but we do want to check whether they are synchronized. They should be if LSL is working correctly, but it’s always good to do a sanity check as LSL can be sensitive to CPU-overload and other issues.\nWe will combine the audio and video for each trial using ffmpeg package, and save it as a new video file.\n\nwavloc = trialfolder+'Audio_48\\\\'\nwavfiles = glob.glob(wavloc+'*.wav')\naudiovideo = datafolder+'Data_processed\\\\AudioVideo\\\\'\nvideofolder = datafolder+'Data_processed\\\\Data_trials\\\\'\n\n\n# loop over Audio files\nfor file in wavfiles:\n\n    print('Now processing file '+file)\n\n    filename = file.split('\\\\')[-1].split('.')[0]\n\n    dyadIndex = filename.split('_')[0]   # this is dyad number\n    partIndex = filename.split('_')[1]   # this is part of the session\n    sessionIndex = dyadIndex + '_' + partIndex # this is the session index\n    print(sessionIndex)\n    trialIndex = filename.split('_')[3] # this is trial number\n\n    participant = filename.split('_')[7] # this is participant 0/1\n    word = filename.split('_')[8] # this is the word\n    modality = filename.split('_')[9] # this is the modality\n\n    # handle correction\n    if 'c0' in file:\n        correction = '_c0'\n    elif 'c1' in file:\n        correction = '_c1'\n    elif 'c2' in file:\n        correction = '_c2'\n    else:\n        correction = ''\n    # trial type\n    if '_pr_' in file:\n        trialtype = 'pr'\n    else:\n        trialtype = 'trial'\n\n    # if it's corrected file or not\n    if 'corrected' in file:\n        add = '_corrected'\n    else:\n        add = ''\n\n    #load in the audio\n    print('Loading the audio')\n    audio_path = os.path.join(wavloc, file)\n    if not os.path.exists(audio_path):\n        print(f\"Audio file not found: {audio_path}\")\n\n    # input the video with ffmpg\n    input_audio = ffmpeg.input(audio_path)\n    print(input_audio)\n    #load in the video with matchich trialIndex and SessionIndex\n    print('Loading the video')\n    video_path = os.path.join(videofolder + f\"{sessionIndex}_{trialtype}_{trialIndex}_{participant}_{word}_{modality}{add}{correction}_video_raw.avi\")\n\n    if not os.path.exists(video_path):\n        print(f\"Video file not found: {video_path}\")\n    input_video = ffmpeg.input(video_path)\n    print(input_video)\n    \n    #combine the audio and video\n    print('Combining audio and video')\n    output_path = os.path.abspath(os.path.join(audiovideo, f\"{sessionIndex}_{trialtype}_{trialIndex}_{participant}_{word}_{modality}{correction}_final.avi\"))\n    ffmpeg.concat(input_video, input_audio, v=1, a=1).output(\n        output_path,\n        vcodec='libx264',\n        acodec='aac',\n        video_bitrate='2M',         \n        \n        ).run(overwrite_output=True)\n    \nprint('All done!')\n\nHere is an example of the combined audio and video for a trial\n\n\n\n \n Your browser does not support the video tag.",
    "crumbs": [
      "Processing",
      "Pre-Processing I: from XDF to raw files"
    ]
  },
  {
    "objectID": "02_MotionTracking_processing/02_Track_OpenPose.html",
    "href": "02_MotionTracking_processing/02_Track_OpenPose.html",
    "title": "Motion tracking II: 2D pose estimation via OpenPose",
    "section": "",
    "text": "Overview\nIn this script, we take the videos prepared in the previous script and perform OpenPose 2D pose estimation (Cao et al. (2019)) on them.\nWe are using the BODY_135 model with 135 keypoints.\nDemo of this pipeline has been published on EnvisionBOX\nSee OpenPose documentation for more information.\n\n\nCode to prepare the environment\nimport os\nimport subprocess\nimport glob\nimport tempfile\nfrom IPython.display import Video\nimport random\n\ncurfolder = os.getcwd()\n\n# Openpose demo.exe location\nopenposefol = curfolder+'/openpose/'\nopenpose_demo_loc = openposefol + '/bin/OpenPoseDemo.exe'\n\n# This is the model to employ\nmodel_to_employ = 'BODY_135'\n\n# List folders in a main folder\nfolderstotrack = glob.glob(curfolder +'/projectdata/*')\n\n# Get all folders per participant, per session\npcnfolders = []\n\nfor i in folderstotrack:\n    pcn1folders = glob.glob(i + '/P0/*')\n    pcn2folders = glob.glob(i + '/P1/*')\n    pcnfolders_in_session = pcn1folders + pcn2folders\n\n    # Append to the list\n    pcnfolders = pcnfolders + pcnfolders_in_session\n\n# There might be some other things we don't want now\npcnfolders = [x for x in pcnfolders if 'Config' not in x]\npcnfolders = [x for x in pcnfolders if 'opensim' not in x]\npcnfolders = [x for x in pcnfolders if 'xml' not in x]\npcnfolders = [x for x in pcnfolders if 'ResultsInverseDynamics' not in x]\npcnfolders = [x for x in pcnfolders if 'ResultsInverseKinematics' not in x]\npcnfolders = [x for x in pcnfolders if 'sto' not in x]\npcnfolders = [x for x in pcnfolders if 'txt' not in x]\n\nprint(pcnfolders[0:10])\n\n\n\n\nRunning OpenPose as a subprocess\nInstead of running OpenPose in the notebook, we will send commands to the OpenPose executable as a subprocess. This is more efficient and allows us to run OpenPose in parallel for multiple videos.\n\n\nCode to run OpenPose as a subprocess\ndef runcommand(command):\n    # Run the command using subprocess for OPENPOSE TRACKING\n    try:\n        subprocess.run(command, shell=True, check=True)\n    except subprocess.CalledProcessError as e:\n        print(f\"Command execution failed with error code {e.returncode}\")\n    except FileNotFoundError:\n        print(\"The OpenPoseDemo.exe executable was not found.\")\n\n\n\nfor i in pcnfolders:\n    os.chdir(openposefol)\n    print('working on ' + i)\n\n    # Identify all avi files in folder\n    direc = glob.glob(i + '/raw-2d/' +'*.avi')\n\n    # 3 cameras\n    video0 = direc[0]\n    video1 = direc[1]\n    video2 = direc[2]\n\n    videolist = [video0, video1, video2]\n    \n    # Make a new directory if it doesn't exist\n    if not os.path.exists(i+'/pose/'):\n        os.makedirs(i+'/pose/')\n    if not os.path.exists(i+'/pose/pose_cam1_json/'):\n        os.makedirs(i+'/pose/pose_cam1_json/')\n    if not os.path.exists(i+'/pose/pose_cam2_json/'):\n        os.makedirs(i+'/pose/pose_cam2_json/')\n    if not os.path.exists(i+'/pose/pose_cam3_json/'):\n        os.makedirs(i+'/pose/pose_cam3_json/')\n    \n\n    # Also make directory for openpose videos (pose-2d-trackingvideos)\n    if not os.path.exists(i+'/pose-2d-trackingvideos/'):\n        os.makedirs(i+'/pose-2d-trackingvideos/')\n\n    # Initialize the pose2 folder\n    outputfol1 = i+'/pose/pose_cam1_json/'\n    outputfol2 = i+'/pose/pose_cam2_json/'\n    outputfol3 = i+'/pose/pose_cam3_json/'\n\n    outputfollist = [outputfol1, outputfol2, outputfol3]\n\n    for it, j in enumerate(outputfollist):\n        # Prepare the command\n        openposelocation = ' ' + openpose_demo_loc + ' '\n        model = '--model_pose' + ' ' + model_to_employ + ' '\n        video = '--video ' + videolist[it] + ' '\n        todo = '--write_json '\n        outputfol = j + ' '\n        videoadd = '--write_video '\n        videopath = i+'/pose-2d-trackingvideos/' + 'video'+str(it)+'.avi' + ' '\n        # Send the command via subprocess\n        command = r' '+openposelocation+model+video+todo+outputfol+videoadd+videopath\n        print('were going to send this to command prompt: ' + command)\n        runcommand(command)\n    \n\nThe results are saved in /pose folder in json format, and are used for triangulation via Pose2sim (Pagnon, Domalain, and Reveret (2022)) in the next script.\nThe videos with the estimated poses are saved in the /pose-2d-trackingvideos folder\nHere is an example of the output:\n\n\n\n \n Your browser does not support the video tag.\n \n\n\n\n\nReferences\n\n\n\n\nCao, Zhe, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2019. “OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields.” May 30, 2019. https://doi.org/10.48550/arXiv.1812.08008.\n\n\nPagnon, David, Mathieu Domalain, and Lionel Reveret. 2022. “Pose2Sim: An End-to-End Workflow for 3D Markerless Sports Kinematics—Part 2: Accuracy.” Sensors 22 (7, 7): 2712. https://doi.org/10.3390/s22072712.",
    "crumbs": [
      "Processing",
      "Motion tracking II: 2D pose estimation via OpenPose"
    ]
  },
  {
    "objectID": "02_MotionTracking_processing/04_Track_InverseKinDyn.html",
    "href": "02_MotionTracking_processing/04_Track_InverseKinDyn.html",
    "title": "Motion tracking IV: Modeling inverse kinematics and dynamics",
    "section": "",
    "text": "Overview\nIn this script, we will use the OpenSim (Seth et al. (26. 7. 2018)) to model the inverse kinematics (i.e., joint angles) and dynamics (i.e., joint forces) of the motion tracking data.\nThe documentation of OpenSim project is available here\n\n\nCode to prepare the environment\nimport opensim\nimport os\nimport glob\nimport shutil\nimport pandas as pd\nfrom scipy.signal import savgol_filter\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ncurfolder = os.getcwd()\n\n# This is where we store the data\nprojectdata = os.path.join(curfolder, 'projectdata')\n# These are the sessions we want to track\nsessionstotrack = glob.glob(os.path.join(projectdata, 'Session*'))\nprint(sessionstotrack)\n\n# Here we store the metadata about weight, height\nMETA = pd.read_csv(curfolder + '\\\\..\\\\00_RAWDATA\\META_mass.txt', sep='\\t')\n\n# Here are the config files for the OpenSim pipeline\nscalefile = curfolder + '/Pose2Sim/OpenSim_Setup/Scaling_Setup_Pose2Sim_Body135_FLESH.xml'\nikfile = curfolder + '/Pose2Sim/OpenSim_Setup/IK_Setup_Pose2Sim_Body135_FLESH.xml'\nidfile = curfolder + '/Pose2Sim/OpenSim_Setup/ID_Setup_Pose2Sim_Body135_FLESH.xml'\n\n# Get sessionIDs\nsessionIDs = []\nfor session in sessionstotrack:\n    sessionIDs.append(session.split('\\\\')[-1])\n    sessionIDs[-1] = sessionIDs[-1].split('_')[1]\n    # Keep only unique values\n    sessionIDs = list(set(sessionIDs))\n\nprint(sessionIDs)\n\n\n\n\nCustom functions\n# Function to update XML file\ndef update_xml_file(dir, input_file, output_file, new_mass=None, new_model_file=None, new_marker_file=None, new_timerange=None, new_output_model_file=None, new_coord_file=None):\n\n    # Load the XML document\n    tree = ET.parse(input_file)\n    root = tree.getroot()\n\n    # If the file has Scaling in the name, we need to update the marker file\n    if 'Scaling' in input_file:\n        \n        # Update the &lt;mass&gt; element if a new value is provided\n        if new_mass is not None:\n            mass_element = root.find('.//mass')\n            if mass_element is not None:\n                mass_element.text = new_mass\n\n        # Update the &lt;model_file&gt; element within &lt;GenericModelMaker&gt; if a new value is provided\n        if new_model_file is not None:\n            model_file_element = root.find('.//GenericModelMaker/model_file')\n            if model_file_element is not None:\n                model_file_element.text = new_model_file\n\n        # Update the &lt;marker_file&gt; element within &lt;ModelScaler&gt; and &lt;MarkerPlacer&gt; if a new value is provided\n        if new_marker_file is not None:\n            marker_file_elements = root.findall('.//marker_file')\n            for marker_file_element in marker_file_elements:\n                marker_file_element.text = new_marker_file\n\n        # Update all time ranges\n        if new_timerange is not None:\n            timerange_elements = root.findall('.//time_range')\n            for timerange_element in timerange_elements:\n                timerange_element.text = new_timerange\n\n        # Update the &lt;output_model_file&gt; element within &lt;MarkerPlacer&gt; if a new value is provided\n        if new_output_model_file is not None:\n            output_model_file_element = root.find('.//MarkerPlacer/output_model_file')\n            if output_model_file_element is not None:\n                output_model_file_element.text = new_output_model_file\n\n        # Update the &lt;output_model_file&gt; element within &lt;ModelScaler&gt; if a new value is provided\n        if new_output_model_file is not None:\n            output_model_file_element = root.find('.//ModelScaler/output_model_file')\n            if output_model_file_element is not None:\n                output_model_file_element.text = new_output_model_file\n    \n    elif 'IK' in input_file:\n        # We update model_file\n        if new_model_file is not None:\n            model_file_element = root.find('.//model_file')\n            if model_file_element is not None:\n                model_file_element.text = new_model_file\n\n        # We need to update time range\n        if new_timerange is not None:\n            timerange_elements = root.findall('.//time_range')\n            for timerange_element in timerange_elements:\n                timerange_element.text = new_timerange\n\n        # And we need to update the path to trc file\n        if new_marker_file is not None:\n            marker_file_element = root.find('.//marker_file')\n            if marker_file_element is not None:\n                marker_file_element.text = new_marker_file\n\n        # And output_motion_file\n        if new_output_model_file is not None:\n            output_model_file_element = root.find('.//output_motion_file')\n            if output_model_file_element is not None:\n                output_model_file_element.text = new_output_model_file\n\n    elif 'ID' in input_file:\n        # We update model_file\n        if new_model_file is not None:\n            model_file_element = root.find('.//model_file')\n            if model_file_element is not None:\n                model_file_element.text = new_model_file\n\n        # We need to update time range\n        if new_timerange is not None:\n            timerange_elements = root.findall('.//time_range')\n            for timerange_element in timerange_elements:\n                timerange_element.text = new_timerange\n\n        # And we need to update the path to mot file\n        if new_coord_file is not None:\n            coord_file_element = root.find('.//coordinates_file')\n            if coord_file_element is not None:\n                coord_file_element.text = new_coord_file\n\n        # And output_motion_file\n        if new_output_model_file is not None:\n            output_model_file_element = root.find('.//output_gen_force_file')\n            if output_model_file_element is not None:\n                output_model_file_element.text = new_output_model_file\n\n    if 'Scaling' in input_file:\n        output_file_path = os.path.join(dir, output_file) # Scaling is saved in the participant folder\n        tree.write(output_file_path, encoding='UTF-8', xml_declaration=True)\n    else:\n        tree.write(output_file, encoding='UTF-8', xml_declaration=True)\n\n# Function to extract time range from a trial\ndef extract_first_and_last_time(file_path):\n\n    df = pd.read_csv(file_path, sep='\\t', skiprows=4)\n    # Extract the first and last time values, time is the second column\n    first_time = df.iloc[0, 1]\n    last_time = df.iloc[-1, 1]\n    \n    return first_time, last_time\n\n# Function to smooth .mot file\ndef smooth_data(input_path, output_path, smoothing_params, plot=False, plot_column=None):\n\n    # Read the entire file\n    with open(input_path, 'r') as file:\n        lines = file.readlines()\n\n    # Identify header and data section\n    header_lines = []\n    data_start_index = 0\n\n    for i, line in enumerate(lines):\n        if line.strip() == 'endheader':\n            header_lines = lines[:i + 1]\n            data_start_index = i + 1\n            break\n\n    # Extract the column headers and numerical data\n    column_headers = lines[data_start_index].split()\n    data_lines = lines[data_start_index + 1:]\n    data = np.array([list(map(float, line.split())) for line in data_lines])\n\n    # Identify the column index for plotting (if applicable)\n    plot_column_idx = column_headers.index(plot_column) if plot_column else None\n\n    # Apply smoothing to each column except 'time'\n    smoothed_data = data.copy()\n    for col_idx in range(1, data.shape[1]):  # Skip 'time' (assumed to be the first column)\n        smoothed_data[:, col_idx] = savgol_filter(\n            data[:, col_idx], \n            window_length=smoothing_params['window_length'], \n            polyorder=smoothing_params['polyorder']\n        )\n\n    if plot == True:\n        #Plot unsmoothed vs smoothed data for the specified column\n        if plot_column and plot_column_idx is not None:\n            plt.figure(figsize=(10, 6))\n            plt.plot(data[:, 0], data[:, plot_column_idx], label='Unsmoothed', alpha=0.7)\n            plt.plot(data[:, 0], smoothed_data[:, plot_column_idx], label='Smoothed', alpha=0.7)\n            plt.xlabel('Time')\n            plt.ylabel(plot_column)\n            plt.title(f'Unsmoothed vs Smoothed: {plot_column}')\n            plt.legend()\n            plt.grid(True)\n            plt.show()\n\n    # Write back the original structure with smoothed data\n    with open(output_path, 'w') as output_file:\n        # Write the header\n        output_file.writelines(header_lines)\n        # Write the column headers\n        output_file.write('\\t'.join(column_headers) + '\\n')\n        # Write the smoothed data row by row - this is necessary to maintain the same formatting, otherwise inverse dynamics will fail\n        for row in smoothed_data:\n            output_file.write('\\t'.join(f'{x:.6f}' for x in row) + '\\n')\n    \n\n\n\n\nInverse kinematics and dynamics with OpenSim\nThe opensim pipeline has three steps - scaling - inverse kinematics - inverse dynamics\nIn scaling, we scale the model to match the anthropometry of the subject. We use the Pose2sim model with 135 keypoints (BODY_135) and use a pre-recorded t-pose video to scale this model and create new model, scaled for each participant.\nIn inverse kinematics, we use the scaled model to estimate the joint angles of the participant. We use the motion tracking data to estimate the joint angles. Joint angles are saved as .mot files in /ResultsInverseKinematics folder per each trial.\nIn inverse dynamics, we use the joint angles to estimate the joint forces. Joint forces are saved as .sto files in /ResultsInverseDynamics folder per each trial.\n\n# Note that session contains of two parts that have the same participants (it's always sessionID_1 and sessionID_2)\n\nfor sessionID in sessionIDs:\n    # Get the session path of the first session\n    session1path = os.path.join(projectdata, 'Session_' + sessionID + '_1')\n    session2path = os.path.join(projectdata, 'Session_' + sessionID + '_2')\n    # Get p0 folders from both sessions\n    p0session1 = os.path.join(session1path, 'P0')\n    p0session2 = os.path.join(session2path, 'P0')\n    # Get p1 folders from both sessions\n    p1session1 = os.path.join(session1path, 'P1')\n    p1session2 = os.path.join(session2path, 'P1')\n\n    # Merge them\n    participants = [p0session1, p0session2, p1session1, p1session2] #p0session1,p0session2\n    print(participants)\n\n    for p in participants:\n        os.chdir(p)\n\n        ###### SCALING ######\n        # We do scaling only for Session x_1 (and copy it to x_2)\n        if 'Session_' + sessionID + '_1' in p:\n            print(p)\n\n            # Get weight from META for this pcn\n            pcn = p.split('\\\\')[-1].lower()\n            weight = META.loc[META['pcn'] == pcn, 'weight'].values[0]\n            new_mass = str(weight)\n\n            # Get the path to the input model\n            new_model_file = 'opensim\\Model_Pose2Sim_Body135.osim'\n            new_model_file = os.path.join(p, new_model_file)\n\n            # Get the path to the marker file\n            tposefolder = glob.glob(os.path.join(p, '*tpose*'))[0]\n            trcfolder = os.path.join(tposefolder, 'pose-3d')\n            trcfiles = glob.glob(os.path.join(trcfolder, '*.trc'))\n            # Keep only the one with 'butterworth' in its name (this is filtered file)\n            new_marker_file = [trc for trc in trcfiles if 'butterworth' in trc][0]\n            # Get the time range of this file\n            first_time, last_time = extract_first_and_last_time(new_marker_file)\n            new_timerange = str(first_time) + ' ' + str(last_time)\n\n            # New output model file\n            participant = p.split('\\\\')[-1]\n            new_output_model_file = 'opensim\\Model_Pose2Sim_scaled_' + sessionID + '_' + participant + '.osim'\n\n            # Update the XML file with new values\n            new_scalefile = 'Scaling_Setup_Pose2Sim_Body135_FLESH_' + sessionID + '_' + participant + '.xml'\n            update_xml_file(p, scalefile, new_scalefile, new_mass=new_mass, new_model_file=new_model_file, new_marker_file=new_marker_file, new_timerange=new_timerange, new_output_model_file=new_output_model_file)\n\n            print('Scaling...')\n            opensim.ScaleTool(new_scalefile).run()\n            \n            # Copy scaling setup also to session 2 of the same participant\n            session2Path = os.path.join(projectdata, 'Session_' + sessionID + '_2', participant)\n            os.makedirs(session2Path, exist_ok=True)\n            shutil.copy(new_scalefile, session2Path)\n            # And scaled model too\n            session2ScaledModelPath = os.path.join(session2Path, 'opensim')\n            os.makedirs(session2ScaledModelPath, exist_ok=True)\n            shutil.copy(new_output_model_file, session2ScaledModelPath)\n\n        # If its session x_2, we directly go to IK and ID\n        else:\n            print('Session x_2, skipping scaling...')\n\n        ###### INVERSE KINEMATICS ######\n\n        # Create folder ResultsInverseKinematics\n        if not os.path.exists(os.path.join(p, 'ResultsInverseKinematics')):\n            os.makedirs(os.path.join(p, 'ResultsInverseKinematics'))\n            \n        # Collect all folders in p\n        folders = glob.glob(os.path.join(p, '*'))\n\n        # Get rid of all folders/files that we don't want now\n        folders = [f for f in folders if 'opensim' not in f]\n        folders = [f for f in folders if 'toml' not in f]\n        folders = [f for f in folders if 'txt' not in f]\n        folders = [f for f in folders if 'xml' not in f]\n        folders = [f for f in folders if 'sto' not in f]\n        folders = [f for f in folders if 'tpose' not in f]\n        folders = [f for f in folders if 'ResultsInverseKinematics' not in f]\n        folders = [f for f in folders if 'ResultsInverseDynamics' not in f]\n        #print(folders)\n\n        for f in folders:\n            print(f)\n            trialid = f.split('\\\\')[-1]\n\n            # Get trc file\n            trcfiles = glob.glob(os.path.join(f, '*/*.trc'), recursive=True)\n            new_trc_file = [trc for trc in trcfiles if 'butterworth' in trc][0]\n\n            # Get the time range from it\n            first_time, last_time = extract_first_and_last_time(new_trc_file)\n            new_timerange = str(first_time) + ' ' + str(last_time)\n\n            # Get the scaled model\n            scaled_model = new_output_model_file\n      \n            # Output motion file\n            output_motion_file = 'ResultsInverseKinematics/' + sessionID + '_' + trialid + '.mot'\n            output_motion_file = os.path.join(p, output_motion_file)\n\n            # Update the XML file\n            new_ikfile_name = 'IK_Setup_Pose2Sim_Body135_FLESH_' + sessionID + '_' + trialid + '.xml'\n            new_ikfile = os.path.join(p, new_ikfile_name)\n            update_xml_file(p, ikfile, new_ikfile, new_model_file=scaled_model, new_marker_file=new_trc_file, new_timerange=new_timerange, new_output_model_file=output_motion_file)\n\n            print('Inverse Kinematics...')\n            try:\n                opensim.InverseKinematicsTool(new_ikfile).run()\n            except:\n                print('Error in IK')\n                continue\n\n            # opensim doesn't smooth the data, but we need to smooth them before inverse dynamics, otherwise we will magnify the noise\n            smoothing_params = {'window_length': 35, 'polyorder': 3}\n            smooth_data(output_motion_file, output_motion_file, smoothing_params, plot=True, plot_column='arm_flex_r')\n\n            ###### INVERSE DYNAMICS ######\n\n            # Create folder ResultsInverseKinematics\n            if not os.path.exists(os.path.join(p, 'ResultsInverseDynamics')):\n                os.makedirs(os.path.join(p, 'ResultsInverseDynamics'))\n\n            # Time range is the same as in IK\n            # Scaled model is the same\n            # mot file is the new_ikfile\n            # Output force file\n            output_gen_force_file = 'ResultsInverseDynamics/' + sessionID + '_' + trialid + '_ID.sto'\n            output_gen_force_file = os.path.join(p, output_gen_force_file)\n\n            # Update the XML file\n            new_idfile_name = 'ID_Setup_Pose2Sim_Body135_FLESH_' + sessionID + '_' + trialid + '.xml'\n            new_idfile = os.path.join(p, new_idfile_name)\n            update_xml_file(p, idfile, new_idfile, new_model_file=scaled_model, new_timerange=new_timerange, new_coord_file=output_motion_file, new_output_model_file=output_gen_force_file)\n\n            print('Inverse Dynamics...')\n            try:\n                opensim.InverseDynamicsTool(new_idfile).run()\n            except:\n                print('Error in ID')\n                continue\n\nHere is an example of angle (full line) and moment/torque (dashed) data for right hip. They are further processed in motion processing script.\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nSeth, Ajay, Jennifer L. Hicks, Thomas K. Uchida, Ayman Habib, Christopher L. Dembia, James J. Dunne, Carmichael F. Ong, et al. 26. 7. 2018. “OpenSim: Simulating Musculoskeletal Dynamics and Neuromuscular Control to Study Human and Animal Movement.” PLOS Computational Biology 14 (7): e1006223. https://doi.org/10.1371/journal.pcbi.1006223.",
    "crumbs": [
      "Processing",
      "Motion tracking IV: Modeling inverse kinematics and dynamics"
    ]
  },
  {
    "objectID": "03_TS_processing/02_TS_processing_acoustics.html",
    "href": "03_TS_processing/02_TS_processing_acoustics.html",
    "title": "Processing II: Acoustics",
    "section": "",
    "text": "Overview\nIn this script, we will work with the audio files we extracted from XDF file. We will extract the following features:\n\nintensity\nf0\nspectral centroid / spectral center of gravity\nformants\n\n\n\nCode to load packages and prepare the environment\n# packages\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy.signal import butter, filtfilt\nimport librosa\nimport parselmouth\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nimport seaborn as sns\nfrom scipy.signal import find_peaks, peak_widths\n\ncurfolder = os.getcwd()\nprint(curfolder)\n\n# files to work with\nACfolder = curfolder + '\\\\..\\\\01_XDF_processing\\\\data\\\\Data_processed\\\\Data_trials\\\\Audio_48'\n\n# folders to save the processed data\nACfolder_processed = curfolder + '\\\\TS_acoustics\\\\'\n\nactotrack = glob.glob(ACfolder + \"/*.wav\", recursive=True)\n#print(actotrack)\n\n# get rid of the first file because it's faulty\nactotrack = actotrack[1:]\n\n\nFirst, we need to take care that we are not working with wrongly cut audio files, but only with the corrected version (if it exists). From the list of audios, we will hence exclude all files that have also corrected version in the list.\n\n# Get all corrected audios from the list\nac_cor = []\nfor file in actotrack:\n    if 'corrected' in file:\n        ac_cor.append(file)\n\n# Now get the name of this trial without the corrected part\nac_old = []\nfor file in ac_cor:\n    ac_old.append(file.replace('_corrected', ''))\n\n# From actotrack, remove trials that are in videos_old\nactotrack = [x for x in actotrack if x not in ac_old]\n\nHere is an audio example\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nAnd here it is visualized as a waveform\n\n\n\n\n\n\n\n\n\n\n\nCustom functions\ndef chunk_and_smooth(df, var, window=25, order=3):\n\n    df['chunk'] = None\n\n    chunk = 0\n    for index, row in df.iterrows():\n        if np.isnan(row[var]):\n            continue\n        else:\n            df.loc[index, 'chunk'] = chunk\n            # if the next value is NaN or this is the last row, increase the chunk\n            if index == len(df)-1:\n                continue\n            elif np.isnan(df.loc[index+1, var]):\n                chunk += 1\n\n    # now we can smooth the spectralCent values in each chunk\n    chunks = df['chunk'].unique()\n\n    # skip if chunks are empty (that means that there is no var trace)\n    if len(chunks) &gt; 1:\n        # ignore the first chunk (None)\n        chunks = chunks[1:]\n        for chunk in chunks:\n            # get the rows of the chunk\n            chunkrows = df[df['chunk'] == chunk].copy()\n            # dont smooth chunks shorter than 5\n            if len(chunkrows) &lt; 5:\n                continue\n            else:\n                # smooth var with savgol filter\n                chunkrows[var] = scipy.signal.savgol_filter(chunkrows[var], window, order) \n                # put it back to the df\n                df.loc[df['chunk'] == chunk, var] = chunkrows[var]\n\n    # get rid of the chunk column\n    df = df.drop('chunk', axis=1)\n\n    return df\n\n\n\n\nExtracting intensity (vocalic energy)\nTo extract the amplitude envelope of the acoustic signal, we follow a method by Tilsen and Arvaniti (2013), adapted by Pouw (2024) (see EnvisionBOX). We use bandpass and 2nd order 10Hz low-pass zero-phase Butterworth filter.\n\n\nCode with functions to extract the amplitude envelope\n# Define the bandpass filter\ndef butter_bandpass(lowcut, highcut, fs, order=2):\n    nyquist = 0.5 * fs\n    low = lowcut / nyquist\n    high = highcut / nyquist\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\ndef butter_bandpass_filtfilt(data, lowcut, highcut, fs, order=2):\n    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n    y = filtfilt(b, a, data)\n    return y\n\n# Define the lowpass filter\ndef butter_lowpass(cutoff, fs, order=2):\n    nyquist = 0.5 * fs\n    normal_cutoff = cutoff / nyquist\n    b, a = butter(order, normal_cutoff, btype='low')\n    return b, a\n\ndef butter_lowpass_filtfilt(data, cutoff, fs, order=2):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = filtfilt(b, a, data)\n    return y\n\n# Function to extract amplitude envelope\ndef amp_envelope(audiofilename):\n    # load audio with librosa\n    audio, sr = librosa.load(audiofilename, sr=None, mono=True)\n    # Bandpass filter 400-4000Hz\n    data = butter_bandpass_filtfilt(audio, 400, 4000, sr, order=2)\n    # Lowpass filter 10Hz\n    data = butter_lowpass_filtfilt(np.abs(data), 10, sr, order=2)\n    # scale from 0 to 1\n    data = (data - np.min(data)) / (np.max(data) - np.min(data))\n    return data, sr\n\n\nHere is an example how the vocalic energy is extracted\n\n\n\n\n\n\n\n\n\nNow we loop over all the audio files and extract the vocalic energy\n\n# Loop over wav files\nfor audiofile in actotrack:\n\n    # get the trialid\n    trialid = audiofile.split('\\\\')[-1].split('.')[0]\n    trialid = '_'.join(trialid.split('_')[0:1] + trialid.split('_')[1:2] + trialid.split('_')[3:4] + trialid.split('_')[7:8])\n\n    print('working on ' + trialid)\n\n    # apply the function\n    ampv, sr = amp_envelope(audiofile)\n\n    # Extract and plot the original signal\n    rawaudio, sr = librosa.load(audiofile, sr=None)\n\n    # create a time vector\n    time_env = np.arange(0, len(rawaudio)/sr, 1/sr)\n    \n    # Ensure the lengths match by padding ampv if necessary (Note that is a quick fix)\n    if len(ampv) &lt; len(time_env):\n        ampv = np.pad(ampv, (0, len(time_env) - len(ampv)), mode='constant')\n    elif len(ampv) &gt; len(time_env):\n        ampv = ampv[:len(time_env)]\n\n    # the same for rawaudio\n    if len(rawaudio) &lt; len(time_env):\n        rawaudio = np.pad(rawaudio, (0, len(time_env) - len(rawaudio)), mode='constant')\n    elif len(rawaudio) &gt; len(time_env):\n        rawaudio = rawaudio[:len(time_env)]\n    \n    # save the audio and envelope\n    try:\n        audio = pd.DataFrame({'time': time_env, 'audio': rawaudio, 'envelope': ampv, 'trialID': trialid})\n        # convert time to ms\n        audio['time'] = audio['time'] * 1000\n\n        # perform also envelope change\n        audio['envelope_change'] = np.insert(np.diff(audio['envelope']), 0, 0)\n        # smooth\n        audio['envelope_change'] = butter_lowpass_filtfilt(np.abs(audio['envelope_change']), 10, sr, order=2)\n        \n        # write as csv\n        audio.to_csv(ACfolder_processed + '/env_' + trialid + '.csv', index=False)\n\n    except ValueError:\n        print('ValueError: ' + trialid)\n        continue\n\nThis is an example of a file\n\n\n\n\n\n\n\n\n\n\ntime\naudio\nenvelope\ntrialID\nenvelope_change\n\n\n\n\n0\n0.000000\n-0.000031\n0.016587\n0_1_18_p0\n-4.792690e-09\n\n\n1\n0.020833\n-0.000031\n0.016587\n0_1_18_p0\n-4.779707e-09\n\n\n2\n0.041667\n-0.000031\n0.016587\n0_1_18_p0\n-4.766729e-09\n\n\n3\n0.062500\n-0.000031\n0.016587\n0_1_18_p0\n-4.753755e-09\n\n\n4\n0.083333\n-0.000031\n0.016587\n0_1_18_p0\n-4.740787e-09\n\n\n5\n0.104167\n-0.000031\n0.016587\n0_1_18_p0\n-4.727823e-09\n\n\n6\n0.125000\n-0.000031\n0.016587\n0_1_18_p0\n-4.714864e-09\n\n\n7\n0.145833\n-0.000031\n0.016587\n0_1_18_p0\n-4.701909e-09\n\n\n8\n0.166667\n-0.000031\n0.016587\n0_1_18_p0\n-4.688959e-09\n\n\n9\n0.187500\n-0.000031\n0.016587\n0_1_18_p0\n-4.676015e-09\n\n\n10\n0.208333\n-0.000031\n0.016587\n0_1_18_p0\n-4.663075e-09\n\n\n11\n0.229167\n-0.000031\n0.016587\n0_1_18_p0\n-4.650139e-09\n\n\n12\n0.250000\n-0.000031\n0.016587\n0_1_18_p0\n-4.637209e-09\n\n\n13\n0.270833\n0.000000\n0.016587\n0_1_18_p0\n-4.624284e-09\n\n\n14\n0.291667\n0.000000\n0.016587\n0_1_18_p0\n-4.611364e-09\n\n\n\n\n\n\n\n\nHere it is visualized\n\n\n\n\n\n\n\n\n\n\n\nExtracting fundamental frequency (f0)\nNow we extract pitch using the parselmouth library (Jadoul, Thompson, and de Boer (2018)).\nBecause we need take into consideration the sex of participant to set the f0 range accordingly, prior to this script we have extracted the speakers’ register using Praat script Get_Speakers_register.praat from Celine De Looze and save it in file SpeakerRegister.txt.\nNow, we first check the mean min and max f0 values across all available data and set the range accordingly.\n\n# this is where we store the min-max f0 values of each speaker\nregister = pd.read_csv(curfolder + '\\\\SpeakerRegister.txt', sep='\\t') \n\n# here we store metadata for each session about sex\nmeta = pd.read_csv(curfolder + '\\\\..\\\\00_RAWDATA\\\\META_gender.txt', sep='\\t')\n\n# now we want to find out the range for males and females\nregister['sex'] = None\n\n# make f0min and f0max numeric\nregister['f0min'] = pd.to_numeric(register['f0min'], errors='coerce')\nregister['f0max'] = pd.to_numeric(register['f0max'], errors='coerce')\n\n# loop over rows in register,\nfor idx, row in register.iterrows():\n    #  get sessionID from FILE (first part)\n    sessionID = row['FILE'].split('_')[0]\n    # get pcn id\n    pcn = row['FILE'].split('_')[7]\n    # merge it\n    ID = sessionID + '_' + pcn\n    # find this id in meta and save in sex the value in column sex\n    sex = meta[meta['ID'] == ID]['sex'].values[0]\n    # save value of sex in current row\n    register.at[idx, 'sex'] = sex\n\n# now group sex by each value and find the mean of f0min and f0max\nf0min = register.groupby('sex')['f0min'].mean()\nf0max = register.groupby('sex')['f0max'].mean()\n\n# bind in df\ndf_register = pd.DataFrame({'f0min': f0min, 'f0max': f0max})\ndf_register.head(5)\n    \n\n\n\n\n\n\n\n\n\nf0min\nf0max\n\n\nsex\n\n\n\n\n\n\nf\n185.895425\n380.660131\n\n\n\n\n\n\n\n\nDyad 0 consists of two females, and the f0 min is 22 Hz and f0 max is 381 Hz.\n\n\nCode with function to extract the fundamental frequency\ndef extract_f0(locationsound, sex):\n\n    # read the sound file as numpy array\n    audio, sr = librosa.load(locationsound, sr=48000)\n\n    # read the sound file into Python\n    snd = parselmouth.Sound(audio, sampling_frequency=sr)\n\n    if sex == 'f':\n        f0min = 186      ## calculated by previous chunk\n        f0max = 381\n    else:\n        f0min = 75      ## Note: don't have any males in dyad 0 so this is only placeholder\n        f0max = 300\n\n    pitch = snd.to_pitch(time_step = 0.002, pitch_floor=f0min, pitch_ceiling=f0max) # time_step to get 500Hz\n\n    f0_values = pitch.selected_array['frequency']\n\n    return snd, f0_values\n\n\nNow we loop over all audio files and extract f0 from each. Resulting f0 contours were smoothed with a Savitzky-Golay 3rd-polynomial filter with a span of 50 ms (following Fuchs, Reichel, and Rochet-Capellan (2016)) applied to continuous runs of phonated vocalization to maintain discontinuities typical of the f0 signal.\n\nfreq=48000    \nmeta = pd.read_csv(curfolder + '\\\\..\\\\00_RAWDATA\\\\META.txt', sep='\\t')\n\n# Loop over wav files\nfor audiofile in actotrack:\n\n    # get the trialid\n    trialid = audiofile.split('\\\\')[-1].split('.')[0]\n    #trial id is the first, second, fourth and eighth element\n    trialid = '_'.join(trialid.split('_')[0:1] + trialid.split('_')[1:2] + trialid.split('_')[3:4] + trialid.split('_')[7:8])\n\n    print('working on ' + trialid)\n\n    # first element is sessionid, fourth element is participantid\n    sessionid = trialid.split('_')[0]\n    participantid = trialid.split('_')[3]\n    ID = sessionid + '_' + participantid\n\n    # what sex has this ID in meta\n    sex = meta[meta['ID'] == ID]['sex'].values[0]\n\n    # apply the function\n    snd, f0 = extract_f0(audiofile, sex)\n\n    length = len(f0)\n\n    # replace 0 values with NaN\n    f0 = np.where(f0 == 0, np.nan, f0)\n\n    # create time vector\n    F0_time = np.linspace(0, snd.duration, len(f0)) * 1000  # Generate time vector\n\n    # create df\n    f0_df = pd.DataFrame({'time_ms': F0_time, 'f0': f0, 'ID': trialid})\n\n    # Smooth the f0 values\n    try:\n        f0_df = chunk_and_smooth(f0_df, 'f0') # do it with window 25\n    except ValueError:\n        # unless there is only tiny chunk of f0 and then we need window of 5\n        print('ValueError: ' + trialid + ', f0 trace is smaller than window length, resuming to window=5')\n        f0_df = chunk_and_smooth(f0_df, 'f0', window=5)\n\n    # write as csv\n    f0_df.to_csv(ACfolder_processed + '/f0_' + trialid + '.csv', index=False)\n\nHere is an example of a file\n\n\n\n\n\n\n\n\n\n\ntime_ms\nf0\nID\n\n\n\n\n0\n0.000000\nNaN\n0_1_10_p1\n\n\n1\n2.052699\nNaN\n0_1_10_p1\n\n\n2\n4.105398\nNaN\n0_1_10_p1\n\n\n3\n6.158097\nNaN\n0_1_10_p1\n\n\n4\n8.210796\nNaN\n0_1_10_p1\n\n\n5\n10.263495\nNaN\n0_1_10_p1\n\n\n6\n12.316194\nNaN\n0_1_10_p1\n\n\n7\n14.368892\nNaN\n0_1_10_p1\n\n\n8\n16.421591\nNaN\n0_1_10_p1\n\n\n9\n18.474290\nNaN\n0_1_10_p1\n\n\n10\n20.526989\nNaN\n0_1_10_p1\n\n\n11\n22.579688\nNaN\n0_1_10_p1\n\n\n12\n24.632387\nNaN\n0_1_10_p1\n\n\n13\n26.685086\nNaN\n0_1_10_p1\n\n\n14\n28.737785\nNaN\n0_1_10_p1\n\n\n\n\n\n\n\n\nAnd here visualized\n\n\n\n\n\n\n\n\n\n\n\nExtracting spectral centroid\nTo extract the are of the main spectral energy, we will compute spectral centroid / spectral center of gravity using parselmouth package (Jadoul, Thompson, and de Boer (2018)). We are filtering out the fundamental frequency (F0) to remove low-frequency components that might interfere with the analysis (code adapted from here).\n\n\nCode with function to filter out f0\nfrom scipy.signal import sosfilt\n\ndef remove_f0(x, fundamental, fs):\n    # Normalize the frequency for digital filter design\n    f_dig = 2 * np.pi * fundamental / fs\n    \n    # Define the notch filter parameters for F0\n    p1 = 0.999 * np.exp(1j * f_dig)  # Pole near the F0 frequency\n    z1 = np.exp(1j * f_dig)          # Zero near the F0 frequency\n    \n    # Define zeros and poles for the notch filter\n    zeros = np.array([z1, z1])\n    poles = np.array([p1, p1])\n    \n    # Create a 2nd-order section (sos) filter array\n    sos = np.array([[1, -2 * np.real(p1), 1, 1, -2 * np.real(z1), np.abs(z1)**2]])\n\n    # Apply the notch filter to the signal\n    y = sosfilt(sos, x)\n    \n    return y\n\n\n\n# Parameters\nwindow_length = 0.03 # 30ms analysis window\n\n# Loop over wav files\nfor audiofile in actotrack:\n    # Extract trial ID from filename\n    trialid = os.path.basename(audiofile).split('.')[0]\n    trialid = '_'.join(trialid.split('_')[0:1] + trialid.split('_')[1:2] + trialid.split('_')[3:4] + trialid.split('_')[7:8])\n\n    print(f'Working on {trialid}')\n\n    # Extract session and participant ID\n    sessionid = trialid.split('_')[0]\n    participantid = trialid.split('_')[3]\n    ID = f\"{sessionid}_{participantid}\"\n\n    # Load sound\n    snd = parselmouth.Sound(audiofile)\n\n    # Get sampling rate\n    fs = snd.sampling_frequency\n\n    # Load f0 file with the same trialid\n    f0file = ACfolder_processed + '/f0_' + trialid + '.csv'\n    f0_df = pd.read_csv(f0file)\n\n    # Get the mean (from non-NaN values) of the f0\n    f0_df = f0_df.dropna()\n    mean_f0 = f0_df['f0'].mean()\n\n    # Sound values\n    sound_values = snd.values[0]\n\n    # Remove F0 from the signal using the mean F0\n    filtered_signal = remove_f0(sound_values, mean_f0, fs)\n\n    # Recreate the sound object with the filtered signal\n    filtered_sound = parselmouth.Sound(filtered_signal, sampling_frequency=fs)\n\n    # Compute spectrogram of the filtered signal\n    spectrogram = filtered_sound.to_spectrogram(window_length=window_length)\n\n    # Extract time values from the spectrogram\n    times = spectrogram.xs()  # Time vector in seconds\n\n    # Compute CoG for each time step\n    cog_values = [spectrogram.to_spectrum_slice(time=t).get_centre_of_gravity(power=2.0) for t in times]\n\n    # Convert time to milliseconds\n    time_cog = np.array(times) * 1000  \n\n    # Convert CoG values to numpy array\n    cog_values = np.array(cog_values)\n\n    # Create DataFrame\n    cog_df = pd.DataFrame({'time': time_cog, 'CoG': cog_values, 'TrialID': trialid})\n\n    # Replace zeros with NaN\n    cog_df['CoG'] = cog_df['CoG'].replace(0, np.nan)\n\n    # Smooth the data\n    try:\n        cog_df = chunk_and_smooth(cog_df, 'CoG')\n    except ValueError:\n        print(f'ValueError: {trialid}, CoG trace is smaller than window length, using window=5')\n        cog_df = chunk_and_smooth(cog_df, 'CoG', window=5)\n\n    # Save DataFrame\n    output_path = os.path.join(ACfolder_processed, f'cog_{trialid}.csv')\n    cog_df.to_csv(output_path, index=False)\n\nThis is a visual example of a file\n\n\n\n\n\n\n\n\n\n\n\nExtracting formants\nTo extract formant values, we use Chris Carignan’s Praat script (see Github) which optimizes the F1-F5 values.\nTo verify the sensibility of the data, we will do some visual inspections. Moreover, we will consider taking formant values from the windows of envelope amplitude peaks.\n\n\nCode to prepare the environment\n# Here we store formants from praat\nformantfolder = curfolder + '/TS_formants/Carignan_formants/'\nformants = glob.glob(formantfolder + '*.Table')\n\n# Here we store processed envelope \nenvfiles = glob.glob(ACfolder_processed + '/env_*.csv')\n\n\nChris Carignan’s Praat-script outputs formants as a .Table. Let’s therefore first read these files and resave them as .csv files.\n\nfor formant in formants:\n    formant_df = pd.read_csv(formant, sep='\\t')\n\n    # get the name of the file\n    filename = os.path.basename(formant)\n    # get the name of the file without the extension\n    filename = os.path.splitext(filename)[0]\n\n    # add to df\n    formant_df['filename'] = filename\n\n    # get trialid from the file name \n    trialid = '_'.join(filename.split('_')[0:2] + filename.split('_')[3:4] + filename.split('_')[7:8]) \n\n    print('working on ' + trialid)\n\n    # add empty row in the beginning with time 0 and rest as first row\n    copy_row = formant_df.iloc[0].copy()\n    # time of this row is 0\n    copy_row['time'] = 0\n\n    # add this row to the beginning of the df\n    formant_df = pd.concat([pd.DataFrame(copy_row).T, formant_df], ignore_index=True)\n    \n    # add trialid to the df\n    formant_df['trialid'] = trialid\n\n    # write it as csv to formantfolder1\n    formant_df.to_csv(ACfolder_processed + 'praat_formants_' + trialid + '.csv', index=False)\n\n\n# inititate empty df\nformants_df = pd.DataFrame()\n\n# get all formant files we just created\nformantfiles = glob.glob(ACfolder_processed + 'praat_formants_*.csv')\n\n# loop over formants 2 and make a giga df from all\nfor formant in formantfiles:\n    print('working on ' + formant)\n    for_df = pd.read_csv(formant)\n\n    # get the name of the file\n    filename = for_df['filename'][0]\n\n    # in filename, look for c1, c2, c0\n    if 'c1' in filename:\n        for_df['correction'] = 'c1'\n    elif 'c2' in filename:\n        for_df['correction'] = 'c2'\n    elif 'c0' in filename:\n        for_df['correction'] = 'c0'\n    else:\n        for_df['correction'] = 'none'\n    \n    # concatenate\n    formants_df = pd.concat([formants_df, for_df])\n\n# get rid of rows with correction = none\nformants_df = formants_df[formants_df['correction'] != 'none']\n\nThis is how the formants look like in a table\n\n\n\n\n\n\n\n\n\n\ntime\nf1\nf2\nf3\nf4\nf5\nfilename\ntrialid\ncorrection\n\n\n\n\n0\n0.000000\n1910.072640\n3380.992452\n5896.442699\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n1\n0.026031\n1910.072640\n3380.992452\n5896.442699\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n2\n0.031031\n1890.080560\n3366.165092\n5896.610205\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n3\n0.036031\n1910.066181\n3380.974399\n5896.442306\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n4\n0.041031\n1890.092319\n3366.156404\n5896.609863\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n5\n0.046031\n1910.073207\n3380.962077\n5896.441999\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n6\n0.051031\n1890.087965\n3366.149616\n5896.609697\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n7\n0.056031\n1910.062802\n3380.953514\n5896.441903\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n8\n0.061031\n1890.085211\n3366.138214\n5896.609723\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n9\n0.066031\n1910.063561\n3380.941686\n5896.442068\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n10\n0.071031\n1890.080307\n3366.122801\n5896.610225\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n11\n0.076031\n1910.048401\n3380.925190\n5896.443055\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n12\n0.081031\n1890.073211\n3366.114427\n5896.611974\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n13\n0.086031\n1910.043940\n3380.916181\n5896.446468\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n14\n0.091031\n1153.005672\n2425.537494\n3774.555756\n0.0\n0.0\n0_2_pr_0_Mic_nominal_srate48000_p0_juichen_com...\n0_2_0_p0\nc0\n\n\n\n\n\n\n\n\nThis is how the formants look for a single trial.\n\n\n\n\n\n\n\n\n\nNow let’s look at the vowel space area across all data.\n\n\n\n\n\n\n\n\n\nAnd this is distribution of f1 across all data.\n\n\n\n\n\n\n\n\n\nThis all looks reasonable. However, we should still be careful. Formant values are most reliable where f0 is present. Since in this project, we work with non-speech sounds, they are frequently unvoiced. Because research shows that there are also weak ‘formants’ beyond f0 contour (e.g., resonances of sub- and supraglottal tract during breathing, see (werneretal?) ), we will also consider formant values in the moments of envelope peaks. This will maximize the number of data points we can use for analysis.\nWe can use findpeaks() function from the signal package to find the peaks in the envelope. We can then use these peaks as a reference point for formant extraction.\n\nenv_df = pd.DataFrame()\n\n# loop over env files and make a giga df from all\nfor envfile in envfiles:\n    df = pd.read_csv(envfile)\n    env_df = pd.concat([env_df, df])\n\nUsing peak_width function, we can extract the window of an envelope peak. Further, we can define the relative height of the peak to adjust the window size. Here, we try relative height of 0.5 and 0.9\n\n# rename trialID to trialid\nenv_df = env_df.rename(columns={'trialID': 'trialid'})\n\n# pick a random trialid from env_df\ntrialid = '0_2_62_p1'\n\n# get the env for this trialid from env_df\nenv_trial = env_df[env_df['trialid'] == trialid]\n\n# find peaks, min height is mean of the env\npeaks, _ = find_peaks(env_trial['envelope'], height=np.mean(env_df['envelope']))\n\n# get the width of the peaks\nresults_half = peak_widths(env_trial['envelope'], peaks, rel_height=0.5)\nresults_full = peak_widths(env_trial['envelope'], peaks, rel_height=0.9)\n\n\n\n\n\n\n\n\n\n\nNow we can check envelope peak widths against formant values. In merged dataframe with both formants and envelope, we will annotate peak widths, so that we know which values of formants to consider (the rest we turn to NA)\n\n# find formants2 df with the same trialid\nformants_trial = formants_df[formants_df['trialid'] == trialid]\n\n# convert time to ms\nformants_trial['time'] = formants_trial['time'] * 1000\n\n# merge formants1 and formants2 on trialid and time, outer method\nmerged_df = pd.merge(env_trial, formants_trial, on=['trialid', 'time'], how='outer')\n\n# cols to int\ncolstoint = ['f1', 'f2', 'f3', 'f4', 'f5']\n\n#interpolate \nfor col in colstoint:\n    merged_df[col] = merged_df[col].interpolate(method='linear', x = merged_df['time'])\n\n#delete rows where envelope is NaN\nmerged_df = merged_df.dropna(subset=['envelope'])\n\n# check the width of the peaks\npeaks, _ = find_peaks(merged_df['envelope'], height=np.mean(env_df['envelope'])) # minimum height of the peak is mean of the envelope (across all data)\n\n# get the width of the peaks\nresults_width = peak_widths(merged_df['envelope'], peaks, rel_height=0.9)\n\n# create column peak_width and put 1 everywhere between start and end of the peak\nmerged_df['peak_width'] = 0\n\n# create a table from the results_half[2] and results_half[3]\npeak_w = pd.DataFrame({'start': results_width[2], 'end': results_width[3]})\n\n# loop over the rows of the peak_w and put 1 in the peak_width column between start and end\nfor i, row in peak_w.iterrows():\n    merged_df.loc[row['start']:row['end'], 'peak_width'] = 1\n\n# for each formant column, create new f_clean column and put the value of the formant where peak_width = 1\nfor col in colstoint:\n    merged_df[col + '_clean'] = merged_df[col] * merged_df['peak_width']\n    #instead of 0, put NaN\n    merged_df[col + '_clean'] = merged_df[col + '_clean'].replace(0, np.nan)\n\nHere we can see visualized overlap of formants and envelope (peaks). The darker part of the formants signal is the window of an envelope peak.\n\n\n\n\n\n\n\n\n\nIn merging script, we will get back to this and use both envelope peaks and f0 to define the relevant formant windows.\n\n\nReferences\n\n\n\n\nFuchs, Susanne, Uwe Reichel, and Amélie Rochet-Capellan. 2016. “F0 Declination and Speech Planning in Face to Face Dialogues.” In. https://doi.org/10.13140/RG.2.1.4909.0320.\n\n\nJadoul, Yannick, Bill Thompson, and Bart de Boer. 2018. “Introducing Parselmouth: A Python Interface to Praat.” Journal of Phonetics 71 (November): 1–15. https://doi.org/10.1016/j.wocn.2018.07.001.\n\n\nPouw, Wim. 2024. “Wim Pouw’s EnvisionBOX Modules for Social Signal Processing.” https://github.com/WimPouw/envisionBOX_modulesWP.\n\n\nTilsen, Sam, and Amalia Arvaniti. 2013. “Speech Rhythm Analysis with Decomposition of the Amplitude Envelope: Characterizing Rhythmic Patterns Within and Across Languages.” The Journal of the Acoustical Society of America 134 (1): 628–39. https://doi.org/10.1121/1.4807565.",
    "crumbs": [
      "Processing",
      "Processing II: Acoustics"
    ]
  },
  {
    "objectID": "04_TS_movementAnnotation/01_Classify_preparation.html",
    "href": "04_TS_movementAnnotation/01_Classify_preparation.html",
    "title": "Movement annotation I: Preparing training data and data for classifier",
    "section": "",
    "text": "Overview\nSince we have around 9000 trials in the final dataset, it is not feasible to manually annotate the movement onset and offset for each trial. Instead, we will use a simple logistic regression model to predict the movement onset and offset from all the movement features we have collected into the merge dataset.\nWe have annotated the movement onset and offset in Wittenburg et al. (2006) for a pilot data (dyad 0). Two annotators have independently annotated the movement onset and offset for four tiers: - upper body - lower body - arms - head\nParent tier ‘movement’ summarizes overal movement across all tiers.\nNow, we will use these ground truth annotations to create a training set for the logistic regression model.\n\n\nCode to prepare the environment\n# Packages\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nimport xml.etree.ElementTree as ET\nimport glob\n\n\ncurfolder = os.getcwd()\n\n# Here we store our timeseries data\nprocessedfolder = os.path.join(curfolder + '\\\\..\\\\03_TS_processing\\\\TS_merged\\\\')\nprocessedfiles = glob.glob(processedfolder + '\\\\merged*.csv')\nprocessedfiles = [x for x in processedfiles if 'anno' not in x]\n\n# Here we will store the training data\ndatasetfolder = os.path.join(curfolder + '\\\\TrainingData\\\\')\n\n# Here we store the data ready to classify\nchunked_folder = os.path.join(curfolder + '\\\\TS_forClassifying\\\\')\n\n\n\n\nPreparing manual annotations\nOur annotators are annotating only movement, so first we need to also fill in the missing space by nomovement values .\n\n\nCustom functions\n# Function to add no-movement annotations to the ELAN file\ndef add_nomovement_annotations(xml_file_path, newfilepath):\n    # Load the XML file\n    tree = ET.parse(xml_file_path)\n    root = tree.getroot()\n\n    # Extract all time slots\n    time_slots = {}\n    for time_slot in root.find('TIME_ORDER').findall('TIME_SLOT'):\n        time_slots[time_slot.attrib['TIME_SLOT_ID']] = int(time_slot.attrib['TIME_VALUE'])\n\n    # Sort time slots by TIME_VALUE\n    sorted_time_slots = sorted(time_slots.items(), key=lambda x: x[1])\n    time_slot_ids = [ts[0] for ts in sorted_time_slots]\n    time_values = [ts[1] for ts in sorted_time_slots]\n\n    # Loop over all tiers\n    for tier in root.findall('TIER'):\n        annotations = tier.findall('ANNOTATION/ALIGNABLE_ANNOTATION')\n\n        if not annotations:\n            # If no annotations exist, add a single 'nomovement' annotation covering the whole tier\n            new_annotation = ET.Element('ANNOTATION')\n            alignable_annotation = ET.SubElement(new_annotation, 'ALIGNABLE_ANNOTATION')\n            alignable_annotation.set('TIME_SLOT_REF1', time_slot_ids[0])\n            alignable_annotation.set('TIME_SLOT_REF2', time_slot_ids[-1])\n            annotation_value = ET.SubElement(alignable_annotation, 'ANNOTATION_VALUE')\n            annotation_value.text = 'nomovement'\n            tier.append(new_annotation)\n        else:\n            # Sort annotations by start time\n            sorted_annotations = sorted(annotations, key=lambda x: time_slots[x.attrib['TIME_SLOT_REF1']])\n            \n            # Handle the first annotation\n            first_annotation = sorted_annotations[0]\n            first_start_time = time_slots[first_annotation.attrib['TIME_SLOT_REF1']]\n            if first_start_time &gt; time_values[0]:\n                new_annotation = ET.Element('ANNOTATION')\n                alignable_annotation = ET.SubElement(new_annotation, 'ALIGNABLE_ANNOTATION')\n                alignable_annotation.set('TIME_SLOT_REF1', time_slot_ids[0])\n                alignable_annotation.set('TIME_SLOT_REF2', first_annotation.attrib['TIME_SLOT_REF1'])\n                annotation_value = ET.SubElement(alignable_annotation, 'ANNOTATION_VALUE')\n                annotation_value.text = 'nomovement'\n                tier.append(new_annotation)\n\n            # Handle gaps between annotations\n            for i in range(len(sorted_annotations) - 1):\n                current_annotation = sorted_annotations[i]\n                next_annotation = sorted_annotations[i + 1]\n                current_end_time = time_slots[current_annotation.attrib['TIME_SLOT_REF2']]\n                next_start_time = time_slots[next_annotation.attrib['TIME_SLOT_REF1']]\n                if current_end_time &lt; next_start_time:\n                    new_annotation = ET.Element('ANNOTATION')\n                    alignable_annotation = ET.SubElement(new_annotation, 'ALIGNABLE_ANNOTATION')\n                    alignable_annotation.set('TIME_SLOT_REF1', current_annotation.attrib['TIME_SLOT_REF2'])\n                    alignable_annotation.set('TIME_SLOT_REF2', next_annotation.attrib['TIME_SLOT_REF1'])\n                    annotation_value = ET.SubElement(alignable_annotation, 'ANNOTATION_VALUE')\n                    annotation_value.text = 'nomovement'\n                    tier.append(new_annotation)\n\n            # Handle the last annotation\n            last_annotation = sorted_annotations[-1]\n            last_end_time = time_slots[last_annotation.attrib['TIME_SLOT_REF2']]\n            if last_end_time &lt; time_values[-1]:\n                new_annotation = ET.Element('ANNOTATION')\n                alignable_annotation = ET.SubElement(new_annotation, 'ALIGNABLE_ANNOTATION')\n                alignable_annotation.set('TIME_SLOT_REF1', last_annotation.attrib['TIME_SLOT_REF2'])\n                alignable_annotation.set('TIME_SLOT_REF2', time_slot_ids[-1])\n                annotation_value = ET.SubElement(alignable_annotation, 'ANNOTATION_VALUE')\n                annotation_value.text = 'nomovement'\n                tier.append(new_annotation)\n\n    # Save the modified XML file as a new file\n    tree.write(newfilepath, encoding='UTF-8', xml_declaration=True)\n\n\n\nmanualanno_folder_r1 = curfolder + '/ManualAnno/R1/'            # Annotator 1 (AC)\nmanualanno_folder_r3 = curfolder + '/ManualAnno/R3/'            # Annotator 2 (GR)\n\nmanualannofiles1 = glob.glob(manualanno_folder_r1 + '/*.eaf')\n# get rid of those with ELAN_tiers.eaf\nmanualannofiles1 = [x for x in manualannofiles1 if 'ELAN_tiers' not in x]\nmanualannofiles3 = glob.glob(manualanno_folder_r3 + '/*.eaf')\n# get rid of those with ELAN_tiers.eaf\nmanualannofiles3 = [x for x in manualannofiles3 if 'ELAN_tiers' not in x]\n\n\nfor file in manualannofiles3:\n    print('working on ' + file)\n\n    # New filename is without third part of the name\n    newfile = file.split('\\\\')[-1]\n    chunks = newfile.split('_')\n    if 'corrected' in file:\n        if 'c0' in file or 'c1' in file or 'c2' in file:\n            newfile = '_'.join(chunks[:-4])\n        else:\n            newfile = '_'.join(chunks[:-3])\n    else:\n        if 'c0' in file or 'c1' in file or 'c2' in file:\n            newfile = '_'.join(chunks[:-3])\n        else:\n            newfile = '_'.join(chunks[:-2]) \n\n    newfile = newfile.replace('trial_', '')\n    \n    # Save it again\n    newfile = manualanno_folder_r3 + newfile + '_ELAN_tiers.eaf'\n\n    add_nomovement_annotations(file, newfile)\n\nNow, we need to get the manual annotation from ELAN to simple text file so that we can merge the timeseries we prepared in ?@sec-procmerge with information about movement in the trial\n\n\nCustom functions\n# Function to parse elan file\ndef parse_eaf_file(eaf_file, rel_tiers):\n    tree = ET.parse(eaf_file)\n    root = tree.getroot()\n\n    time_order = root.find('TIME_ORDER')\n    time_slots = {time_slot.attrib['TIME_SLOT_ID']: time_slot.attrib['TIME_VALUE'] for time_slot in time_order}\n\n    annotations = []\n    relevant_tiers = {rel_tiers}\n\n    for tier in root.findall('TIER'):\n        tier_id = tier.attrib['TIER_ID']\n        if tier_id in relevant_tiers:\n            for annotation in tier.findall('ANNOTATION/ALIGNABLE_ANNOTATION'):\n                # Ensure required attributes are present\n                if 'TIME_SLOT_REF1' in annotation.attrib and 'TIME_SLOT_REF2' in annotation.attrib:\n                    ts_ref1 = annotation.attrib['TIME_SLOT_REF1']\n                    ts_ref2 = annotation.attrib['TIME_SLOT_REF2']\n                    # Get annotation ID if it exists, otherwise set to None\n                    ann_id = annotation.attrib.get('ANNOTATION_ID', None)\n                    annotation_value = annotation.find('ANNOTATION_VALUE').text.strip()\n                    annotations.append({\n                        'tier_id': tier_id,\n                        'annotation_id': ann_id,\n                        'start_time': time_slots[ts_ref1],\n                        'end_time': time_slots[ts_ref2],\n                        'annotation_value': annotation_value\n                    })\n\n    return annotations\n\n# Function to load annotations into csv\ndef fillAnno(TSfile, ANNOfile, colname):\n    TSfile[colname] = None\n    for row in ANNOfile.iterrows():\n        start = row[1][0]\n        end = row[1][1]\n        TSfile.loc[(TSfile['time'] &gt;= start) & (TSfile['time'] &lt;= end), colname] = row[1][2]\n\n\nNow we create text file for each tier separately, saving only the start time, end time and value (movement or nomovement) for each annotation file.\n\n# These are the manual annotations adapted with no-movement annotations\nannofolder_manu = os.path.join(curfolder + '\\\\ManualAnno\\\\R1\\\\')\nannofiles_manu = glob.glob(annofolder_manu + '*ELAN_tiers.eaf')\n\n################\n#### arms ######\n################\n\narms_anno = curfolder + '/annotations_groundTruth/arms_annotations.txt'\n\nwith open(arms_anno, 'w') as f:\n    for file in annofiles_manu:\n        print('working on ' + file)\n        # get the filename as the last element\n        filename = file.split('\\\\')[-1]\n        # replace _ELAN_tiers.eaf with ''\n        filename = filename.replace('_ELAN_tiers.eaf', '')\n        # parse the file\n        annotations = parse_eaf_file(file, 'arms')\n        # write the annotations\n        for annotation in annotations:\n            f.write(f\"{annotation['start_time']}\\t{annotation['end_time']}\\t{annotation['annotation_value']}\\t{filename}\\n\")\n\n###################\n#### upper body####\n###################\n\nupperbody_anno = curfolder + '/annotations_groundTruth/upperbody_annotations.txt'\n\nwith open(upperbody_anno, 'w') as f:\n    for file in annofiles_manu:\n        print('working on ' + file)\n        # get the filename as the last element\n        filename = file.split('\\\\')[-1]\n        # replace _ELAN_tiers.eaf with ''\n        filename = filename.replace('_ELAN_tiers.eaf', '')\n        # parse the file\n        annotations = parse_eaf_file(file, 'upper_body')\n        # write the annotations\n        for annotation in annotations:\n            f.write(f\"{annotation['start_time']}\\t{annotation['end_time']}\\t{annotation['annotation_value']}\\t{filename}\\n\")\n\n###################\n#### lower body####\n###################\n\nlowerbody_anno = curfolder + '/annotations_groundTruth/lowerbody_annotations.txt'\n\nwith open(lowerbody_anno, 'w') as f:\n    for file in annofiles_manu:\n        print('working on ' + file)\n        # get the filename as the last element\n        filename = file.split('\\\\')[-1]\n        # replace _ELAN_tiers.eaf with ''\n        filename = filename.replace('_ELAN_tiers.eaf', '')\n        # parse the file\n        annotations = parse_eaf_file(file, 'lower_body')\n        # write the annotations\n        for annotation in annotations:\n            f.write(f\"{annotation['start_time']}\\t{annotation['end_time']}\\t{annotation['annotation_value']}\\t{filename}\\n\")\n\n###################\n##### head ########\n###################\n\nhead_anno = curfolder + '/annotations_groundTruth/head_annotations.txt'\n\nwith open(head_anno, 'w') as f:\n    for file in annofiles_manu:\n        print('working on ' + file)\n        # get the filename as the last element\n        filename = file.split('\\\\')[-1]\n        # replace _ELAN_tiers.eaf with ''\n        filename = filename.replace('_ELAN_tiers.eaf', '')\n        # parse the file\n        annotations = parse_eaf_file(file, 'head_mov')\n        # write the annotations\n        for annotation in annotations:\n            f.write(f\"{annotation['start_time']}\\t{annotation['end_time']}\\t{annotation['annotation_value']}\\t{filename}\\n\")\n\n\n\nPreparing data for classifier\nIn the following code, we merge the annotations with our merged files so that we can later sample from the data based on the annotations.\nWe will now also filter some superfluous information, as well as add some more such as:\n\ndistance of LIndex to RIndex\ndistance of Wrist to Hip\ndistance of Head to Hip\ndistance of Head to Ankle\n\n\n# These are the annotations per tier that we just created from manual annotations\narms_anno = curfolder + '/annotations_groundTruth/arms_annotations.txt'\nupperbody_anno = curfolder + '/annotations_groundTruth/upperbody_annotations.txt'\nlowerbody_anno = curfolder + '/annotations_groundTruth/lowerbody_annotations.txt'\nhead_anno = curfolder + '/annotations_groundTruth/head_annotations.txt'\n\nfor file in processedfiles:\n\n    # TrialID\n    trialid = file.split('\\\\')[-1].split('.')[0]\n    trialid = trialid.replace('merged_', '')\n\n    print('working on ' + trialid)\n\n    # Load the merged file\n    merged = pd.read_csv(file)\n    \n    # Load the annotations as df\n    arms = pd.read_csv(arms_anno, sep='\\t', header=None)\n    ub = pd.read_csv(upperbody_anno, sep='\\t', header=None)\n    lb = pd.read_csv(lowerbody_anno, sep='\\t', header=None)\n    head = pd.read_csv(head_anno, sep='\\t', header=None)\n\n    annos = [arms, ub, lb, head]\n\n    practice = False\n\n    # Loop over each tier and fill values into timeseries\n    for anno_df in annos:\n        # Get the annotations for the trialid\n        anno_trial = anno_df[anno_df[3] == trialid] \n        \n        if anno_trial.empty:\n            print('no annotations for ' + trialid)  # This will be the case of practice trials that were not annotated\n            practice = True\n            continue\n        \n        else:\n            if anno_df.equals(arms):\n                fillAnno(merged, anno_trial, 'arms')\n            elif anno_df.equals(ub):\n                fillAnno(merged, anno_trial, 'upper_body')\n            elif anno_df.equals(lb):\n                fillAnno(merged, anno_trial, 'lower_body')\n            elif anno_df.equals(head):\n                fillAnno(merged, anno_trial, 'head_mov')\n            else:\n                print('something went wrong')\n\n    if practice:\n        continue\n\n    df = merged.copy()\n\n    # Now we will also add some features that might be relevant for the classifier\n    ## RWrist to LWrist in all dimensions\n    df['wristDistance_x'] = df['RWrist_x'] - df['LWrist_x']\n    df['wristDistance_y'] = df['RWrist_y'] - df['LWrist_y']\n    df['wristDistance_z'] = df['RWrist_z'] - df['LWrist_z']\n\n    ## RWrist to RHip\n    df['RwristRhipDistance_x'] = df['RWrist_x'] - df['RHip_x']\n    df['RwristRhipDistance_y'] = df['RWrist_y'] - df['RHip_y']\n    df['RwristRhipDistance_z'] = df['RWrist_z'] - df['RHip_z']\n\n    ## RWrist to LHip\n    df['RwristLhipDistance_x'] = df['RWrist_x'] - df['LHip_x']\n    df['RwristLhipDistance_y'] = df['RWrist_y'] - df['LHip_y']\n    df['RwristLhipDistance_z'] = df['RWrist_z'] - df['LHip_z']\n\n    ## LWrist to LHip\n    df['LwristLhipDistance_x'] = df['LWrist_x'] - df['LHip_x']\n    df['LwristLhipDistance_y'] = df['LWrist_y'] - df['LHip_y']\n    df['LwristLhipDistance_z'] = df['LWrist_z'] - df['LHip_z']\n\n    ## LWrist to RHip\n    df['LwristRhipDistance_x'] = df['LWrist_x'] - df['RHip_x']\n    df['LwristRhipDistance_y'] = df['LWrist_y'] - df['RHip_y']\n    df['LwristRhipDistance_z'] = df['LWrist_z'] - df['RHip_z']\n\n    ## Head to RHip\n    df['HeadRhipDistance_x'] = df['Head_x'] - df['RHip_x']\n    df['HeadRhipDistance_y'] = df['Head_y'] - df['RHip_y']\n    df['HeadRhipDistance_z'] = df['Head_z'] - df['RHip_z']\n\n    ## Head to RAnkle\n    df['HeadRankleDistance_x'] = df['Head_x'] - df['RAnkle_x']\n    df['HeadRankleDistance_y'] = df['Head_y'] - df['RAnkle_y']\n    df['HeadRankleDistance_z'] = df['Head_z'] - df['RAnkle_z']\n\n\n    # Get rid of superfluous columns\n    df = df.drop(columns=['left_back', 'right_forward', 'right_back', 'left_forward', 'COPXc', 'COPYc', 'FileInfo'])\n\n    # And we also don't need vocal features\n    cols = df.columns\n    colstodrop = ['envelope', 'audio', 'envelope_change', 'audio', 'f0', 'f1', 'f2', 'f3', 'env_', 'CoG']\n    newcols = [col for col in cols if not any(x in col for x in colstodrop)]\n    df = df[newcols]   \n                \n    # Save it\n    df.to_csv(curfolder + '\\\\TS_annotated\\\\merged_anno_' + trialid + '.csv', index=False)\n\nNow we are ready to create the training set for the logistic regression model.\n\n\nSumarizing features for training dataset\nNow we will sample windows from movement and nomovement for each tier and summarize the available features in terms of mean, sd, min and max. Note that we will not create training data for each tier separately. Sometimes, it may be useful to predict movement of a specific body part with the information about other body part.\nEach tier varies in the length of movement and non-movement chunk, but we will proceed in uniform way, setting threshold of 50 rows, i.e., 100 ms. In such a short period of time, it is anyway difficult to initiate any meaningful movement in any of the tiers of interest - head, arms, upper body, lower body.\nWe will sample the windows randomly, but also make sure there is enough border cases (i.e., windows that capture end or beginning of the movement). Our participants are ‘locking’ hands in the beginning and in the end of each performance. Our classifier should know that these are not ‘communicative’ movements per se.\n(Note that this code takes a while to execute.)\n\n\nCustom functions\n# Function to sample random consecutive rows from a df\ndef select_random_consecutive_rows(df, change_col, threshold):\n    # Group the DataFrame by the 'change' column\n    grouped = df.groupby(change_col)\n    \n    # List to hold the selected rows\n    selected_rows = []\n\n    # Loop over each group\n    for group_df in grouped:\n        # get the first index\n        idx_start = group_df[1].index[0]\n        # get the last index\n        idx_last = group_df[1].index[-1]\n        # Check if the group is large enough to select 'threshold' rows\n        if len(group_df[1]) &gt;= threshold:\n            # Randomly choose a starting index for consecutive selection that is within the index range of the group\n            start_idx = np.random.randint(idx_start, idx_last - threshold + 1)\n            # Select consecutive rows from that start index\n            selected = df.loc[start_idx:start_idx + threshold - 1]\n            selected_rows.append(selected)\n    \n    # Concatenate all selected rows into a single DataFrame\n    result_df = pd.concat(selected_rows)\n    \n    return result_df\n\n# Transforming the dictionary into a df\ndef dict_to_df(data):\n    # Flatten the dictionary into a format with keys like 'feature_mean', 'feature_std', etc.\n    flat_data = {}\n    for feature, stats in data.items():\n        for stat, value in stats.items():\n            flat_data[f'{feature}_{stat}'] = value\n\n    # Convert the flat dictionary to a DataFrame with a single row\n    df = pd.DataFrame(flat_data, index=[0])\n    \n    return df\n\n\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# These are our timeseries to be sampled from\nsamplingfolder = os.path.join(curfolder + '/TS_annotated/')\nsamplingfiles = glob.glob(samplingfolder + '*.csv')\n\ntiers = ['arms', 'upper_body', 'lower_body', 'head_mov']\nthreshold_m = 50 # threshold for movement (100 ms)\nthreshold_nm = 50 # threshold for no movement\n\nfor tierofinterest in tiers:\n    dataset_features = pd.DataFrame()\n    summaries_m = {}\n    summaries_nm = {}\n\n    counter = 1\n\n    for file in samplingfiles:\n        df = pd.read_csv(file)\n\n        # If the df doesn't have columns arms, upper_body, lower_body, head_mov, skip it\n        if 'arms' not in df.columns or 'upper_body' not in df.columns or 'lower_body' not in df.columns or 'head_mov' not in df.columns:\n            print('skipping ' + file)\n            continue\n\n        # TrialID\n        trialid = file.split('\\\\')[-1].split('.')[0]\n\n        # Annotate unique movement/no movement chunks\n        df['row_change'] = df[tierofinterest].ne(df[tierofinterest].shift()).cumsum()\n\n        # Sample random 5 samples of the threshold length in both movement and no movement in tier\n        tier_m = df[df[tierofinterest] == 'movement']\n        tier_nm = df[df[tierofinterest] == 'nomovement']\n\n        if not tier_m.empty:\n            # 10 samples\n            for i in range(10):\n                tier_m_sample = select_random_consecutive_rows(tier_m, 'row_change', threshold_m)\n\n                # Get summaries for numerical columns\n                num_cols = df.select_dtypes(include=np.number).columns\n                num_cols = [x for x in num_cols if x not in ['time', 'row_change']]\n                for col in num_cols:\n                    # Get stats and save them to dictionary\n                    stats = tier_m_sample[col].describe().to_dict()\n                    summaries_m[col] = stats\n\n                # Dictionary to df row\n                summary_row_m = dict_to_df(summaries_m)\n                # We don't need count stats\n                summary_row_m = summary_row_m.loc[:, ~summary_row_m.columns.str.contains('count|%', regex=True)]\n                # Add metainfo\n                summary_row_m['trialid'] = trialid\n                summary_row_m['eventid'] = trialid + '_mov_' + str(counter)\n                summary_row_m['anno_value'] = 'movement'\n\n                # Add row to the main df\n                dataset_features = pd.concat([dataset_features, summary_row_m])\n                counter += 1\n            \n        counter = 1\n\n        if not tier_nm.empty:\n            for i in range(10):\n                tier_nm_sample = select_random_consecutive_rows(tier_nm, 'row_change', threshold_nm)\n                # Get summaries for numerical columns\n                num_cols = df.select_dtypes(include=np.number).columns\n                num_cols = [x for x in num_cols if x not in ['time', 'row_change']]\n                for col in num_cols:\n                    # Get stats and save them to dictionary\n                    stats = tier_nm_sample[col].describe().to_dict()\n                    summaries_nm[col] = stats\n\n                # Dictionary to df row\n                summary_row_nm = dict_to_df(summaries_nm)\n                summary_row_nm = summary_row_nm.loc[:, ~summary_row_nm.columns.str.contains('count|%', regex=True)]\n\n                # Add metainfo\n                summary_row_nm['trialid'] = trialid\n                summary_row_nm['eventid'] = trialid + '_nonmov_' + str(counter)\n                summary_row_nm['anno_value'] = 'nomovement'\n\n                # Add row to the main df\n                dataset_features = pd.concat([dataset_features, summary_row_nm])\n                counter += 1\n\n        counter = 1\n\n        ###################### Process border windows\n\n        border_rows = []\n\n        # Identify the rows where the tierofinterest changes\n        change_points = df[df['row_change'].diff().abs() &gt; 0].index\n\n        for idx in change_points:\n            # Get the window before the change\n            before_start = max(0, idx - 25)  # Ensure no negative index\n            before_end = idx  # Up to the change point\n            before_window = df.iloc[before_start:before_end]\n            # Get the annotation value\n            anno_value = df.loc[idx, tierofinterest]\n\n            # Get the window after the change\n            after_start = idx\n            after_end = min(len(df), idx + 25)  # Ensure no index exceeds the DataFrame length\n            after_window = df.iloc[after_start:after_end]\n\n            # Process the 'before' window in the same way like classic chunks above\n            if not before_window.empty:\n                num_cols = df.select_dtypes(include=np.number).columns\n                num_cols = [x for x in num_cols if x not in ['time', 'row_change']]\n                summaries_before = {col: before_window[col].describe().to_dict() for col in num_cols}\n                summary_row_before = dict_to_df(summaries_before)\n                summary_row_before = summary_row_before.loc[:, ~summary_row_before.columns.str.contains('count|%', regex=True)]\n                summary_row_before['trialid'] = trialid\n                if anno_value == 'movement':\n                    summary_row_before['eventid'] = f\"{trialid}_border_mov_{counter}\"\n                else:\n                    summary_row_before['eventid'] = f\"{trialid}_border_nonmov_{counter}\"\n                summary_row_before['anno_value'] = anno_value\n                dataset_features = pd.concat([dataset_features, summary_row_before])\n                counter += 1\n\n            # Process the 'after' window in the same way like classic chunks above\n            if not after_window.empty:\n                summaries_after = {col: after_window[col].describe().to_dict() for col in num_cols}\n                summary_row_after = dict_to_df(summaries_after)\n                summary_row_after = summary_row_after.loc[:, ~summary_row_after.columns.str.contains('count|%', regex=True)]\n                summary_row_after['trialid'] = trialid\n                if anno_value == 'movement':\n                    summary_row_after['eventid'] = f\"{trialid}_border_mov_{counter}\"\n                else:\n                    summary_row_after['eventid'] = f\"{trialid}_border_nonmov_{counter}\"\n                summary_row_after['anno_value'] = anno_value\n                dataset_features = pd.concat([dataset_features, summary_row_after])\n                counter += 1\n\n    # Drop all columns with NaN values\n    dataset_features = dataset_features.dropna(axis=1)\n    # Save it\n    filename = '\\\\dataset_' + tierofinterest + '_features.csv'\n    dataset_features.to_csv(datasetfolder + filename, index=False)\n        \n        \n\nThis is how the dataset looks like\n\n\n\n\n\n\n\n\n\n\nCOPc_mean\nCOPc_std\nCOPc_min\nCOPc_max\npelvis_tilt_moment_mean\npelvis_tilt_moment_std\npelvis_tilt_moment_min\npelvis_tilt_moment_max\npelvis_list_moment_mean\npelvis_list_moment_std\n...\nHeadRankleDistance_y_std\nHeadRankleDistance_y_min\nHeadRankleDistance_y_max\nHeadRankleDistance_z_mean\nHeadRankleDistance_z_std\nHeadRankleDistance_z_min\nHeadRankleDistance_z_max\ntrialid\neventid\nanno_value\n\n\n\n\n0\n0.001029\n0.000353\n0.000497\n0.001539\n11.916939\n0.523783\n10.259013\n12.440273\n40.593621\n1.214134\n...\n0.388264\n-47.812929\n-46.498456\n142.714523\n0.264402\n142.303580\n143.190783\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_mov_1\nmovement\n\n\n1\n0.002310\n0.000357\n0.001384\n0.002592\n-5.251434\n6.670124\n-14.455521\n6.433171\n-43.388846\n11.359658\n...\n0.951654\n-46.226361\n-43.045054\n147.654005\n0.819087\n146.352625\n149.044777\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_mov_2\nmovement\n\n\n2\n0.001229\n0.000654\n0.000325\n0.002082\n4.686266\n1.693475\n0.823981\n6.247819\n24.678865\n12.110897\n...\n0.668766\n-50.287726\n-48.094340\n139.471133\n0.828517\n138.035172\n140.793319\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_mov_3\nmovement\n\n\n3\n0.010437\n0.003827\n0.005062\n0.016617\n9.534314\n4.984207\n1.780355\n16.748078\n-22.117817\n12.681626\n...\n0.335743\n-48.799024\n-47.725756\n143.580503\n0.088882\n143.457698\n143.710256\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_mov_4\nmovement\n\n\n4\n0.001004\n0.000326\n0.000497\n0.001522\n11.714350\n0.759333\n9.564795\n12.252050\n40.096545\n1.999604\n...\n0.406749\n-47.938470\n-46.575235\n142.787307\n0.272442\n142.360696\n143.261214\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_mov_5\nmovement\n\n\n5\n0.001461\n0.000432\n0.000692\n0.002762\n13.462955\n1.314506\n12.131364\n15.747833\n48.942522\n10.038141\n...\n0.266764\n-46.886244\n-46.003796\n142.217696\n0.202835\n141.921413\n142.578043\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_mov_6\nmovement\n\n\n6\n0.005301\n0.000282\n0.004481\n0.005716\n-9.424995\n13.097252\n-24.188497\n14.602104\n-123.839005\n12.716710\n...\n0.598752\n-26.614199\n-24.623861\n159.472153\n0.330003\n158.899835\n160.012364\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_mov_7\nmovement\n\n\n7\n0.004990\n0.000920\n0.003090\n0.007364\n12.518248\n2.241052\n9.648455\n15.750968\n87.260236\n6.093794\n...\n0.132448\n-45.982839\n-45.587361\n141.964505\n0.093210\n141.876556\n142.181769\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_mov_8\nmovement\n\n\n8\n0.003358\n0.000680\n0.002573\n0.004292\n14.285585\n8.257143\n-3.787133\n23.009216\n4.935150\n15.441791\n...\n0.914203\n-31.859550\n-28.740297\n156.858607\n0.546987\n155.927757\n157.763944\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_mov_9\nmovement\n\n\n9\n0.000474\n0.000222\n0.000257\n0.001104\n7.607259\n1.802191\n3.210280\n9.359289\n-80.437708\n8.600917\n...\n0.621470\n-31.547830\n-29.416171\n158.248435\n0.507886\n157.346089\n159.038608\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_mov_10\nmovement\n\n\n10\n0.000327\n0.000172\n0.000081\n0.000551\n1.142706\n2.774018\n-2.489563\n6.568694\n-59.022974\n8.534245\n...\n0.808444\n-22.051561\n-20.066378\n159.953388\n0.388046\n159.529543\n160.445080\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_nonmov_1\nnomovement\n\n\n11\n0.000859\n0.000627\n0.000175\n0.001681\n1.956649\n2.745580\n-1.319160\n6.667728\n-60.887893\n1.757583\n...\n0.224639\n-21.057542\n-20.421462\n160.343627\n0.493233\n159.797253\n160.919489\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_nonmov_2\nnomovement\n\n\n12\n0.000508\n0.000227\n0.000243\n0.001041\n1.658757\n0.735249\n0.017040\n3.188685\n-67.195356\n6.031778\n...\n2.212455\n-25.605054\n-20.390822\n160.404137\n0.263193\n159.981512\n160.727456\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_nonmov_3\nnomovement\n\n\n13\n0.000446\n0.000291\n0.000050\n0.001093\n2.139294\n1.726763\n-0.846748\n6.088638\n-62.201977\n2.714513\n...\n0.336568\n-21.325884\n-20.390822\n160.209853\n0.453761\n159.732042\n160.736602\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_nonmov_4\nnomovement\n\n\n14\n0.000276\n0.000111\n0.000125\n0.000471\n1.090214\n1.638015\n-1.945115\n3.100118\n-63.428529\n5.286197\n...\n1.425956\n-24.065452\n-20.488453\n160.027971\n0.174932\n159.755498\n160.246507\nmerged_anno_0_1_44_p0\nmerged_anno_0_1_44_p0_nonmov_5\nnomovement\n\n\n\n\n15 rows × 1263 columns\n\n\n\n\n\n\nPreparing timeseries for classifying\nNow we also prepare data for the classifier. We will use the same features as in the previous step, but now we will summarize whole timeseries from beginning to the end. Every time, we take 100ms chunk, and slide with 25ms step, such that we have overlap between the chunks and can later assess more accurately when exactly is the movement onsent/offset.\n\n\nCustom functions\n# Function to summarize every 50 rows with overlapping intervals, sliding by 12 rows\ndef summarize_consecutive_rows(df, trialid, num_cols, summary_interval=50, slide_step=12):\n    summary_df = pd.DataFrame()\n    counter = 1\n\n    for start_idx in range(0, len(df), slide_step):\n        # Select a slice of 50 rows (or fewer for the last chunk)\n        selected = df.iloc[start_idx:start_idx + summary_interval]\n        \n        # Stop if there are no more rows to process\n        if selected.empty:\n            break\n            \n        summary_stats = {}\n\n        # Calculate statistics for each numerical column\n        for col in num_cols:\n            stats = selected[col].describe().to_dict()\n            summary_stats[col] = stats\n\n        # Convert to DataFrame row format\n        summary_row = dict_to_df(summary_stats)\n\n        # Add start and end time for the chunk\n        summary_row['start_time'] = selected['time'].iloc[0]\n        summary_row['end_time'] = selected['time'].iloc[-1]\n\n        # Add chunk number\n        summary_row['eventid'] = f\"{trialid}_chunk_{counter}\"\n\n        # Get rid of all columns that contain 'count' or '%' in the name\n        summary_row = summary_row.loc[:, ~summary_row.columns.str.contains('count|%', regex=True)]\n\n        # Append to the main DataFrame\n        summary_df = pd.concat([summary_df, summary_row], ignore_index=True)\n\n        counter += 1\n\n        # If the selected already contains time of the last row, finish\n        if selected['time'].iloc[-1] == df['time'].iloc[-1]:\n            return summary_df\n\n    return summary_df\n\n\n\n# Main df to store all summaries\nsummary_df = pd.DataFrame()\n\nfor file in samplingfiles:\n\n    # TrialID\n    trialid = file.split('\\\\')[-1].split('.')[0]\n\n    df = pd.read_csv(file)\n\n    # If the df doesn't have columns arms, upper_body, lower_body, head_mov, skip it\n    if 'arms' not in df.columns or 'upper_body' not in df.columns or 'lower_body' not in df.columns or 'head_mov' not in df.columns:\n        print('skipping ' + trialid)\n        continue\n    else:\n        print('working on ' + trialid)\n\n    # Define numerical columns (excluding 'time' and 'change' if present)\n    num_cols = [col for col in df.select_dtypes(include=np.number).columns if col != 'change' and col != 'time']\n\n    # Summarize data in intervals of 50 rows, sliding by 12 rows\n    summary_df = summarize_consecutive_rows(df, trialid, num_cols, summary_interval=50, slide_step=12)\n\n    # Add trial ID \n    summary_df['trialid'] = trialid\n\n    # Save it\n    summary_df.to_csv(chunked_folder + trialid + '_chunked.csv', index=False)\n\nprint('All done, now we can proceed with annotation with our classifier')\n\nThis is an example file that contains timeseries processed into chunks\n\n\n\n\n\n\n\n\n\n\nCOPc_mean\nCOPc_std\nCOPc_min\nCOPc_max\npelvis_tilt_moment_mean\npelvis_tilt_moment_std\npelvis_tilt_moment_min\npelvis_tilt_moment_max\npelvis_list_moment_mean\npelvis_list_moment_std\n...\nHeadRankleDistance_y_min\nHeadRankleDistance_y_max\nHeadRankleDistance_z_mean\nHeadRankleDistance_z_std\nHeadRankleDistance_z_min\nHeadRankleDistance_z_max\nstart_time\nend_time\neventid\ntrialid\n\n\n\n\n0\n0.000209\n0.000104\n0.000023\n0.000360\n-4.822713\n0.329021\n-5.373228\n-4.274271\n24.058442\n0.542426\n...\n157.008942\n157.158540\n7.932183\n0.016104\n7.913146\n7.966076\n0.0\n98.0\nmerged_anno_0_1_4_p0_chunk_1\nmerged_anno_0_1_4_p0\n\n\n1\n0.000264\n0.000098\n0.000118\n0.000380\n-4.552120\n0.326263\n-5.103672\n-4.004715\n23.612342\n0.537878\n...\n157.050898\n157.187292\n7.921748\n0.009694\n7.912271\n7.944289\n24.0\n122.0\nmerged_anno_0_1_4_p0_chunk_2\nmerged_anno_0_1_4_p0\n\n\n2\n0.000316\n0.000073\n0.000136\n0.000390\n-4.283394\n0.326321\n-4.834117\n-3.735160\n23.169319\n0.537974\n...\n157.089436\n157.213695\n7.916291\n0.004669\n7.912271\n7.928391\n48.0\n146.0\nmerged_anno_0_1_4_p0_chunk_3\nmerged_anno_0_1_4_p0\n\n\n3\n0.000345\n0.000029\n0.000288\n0.000403\n-4.014668\n0.326350\n-4.564561\n-3.465605\n22.726297\n0.538022\n...\n157.124298\n157.237953\n7.914763\n0.002189\n7.912271\n7.919865\n72.0\n170.0\nmerged_anno_0_1_4_p0_chunk_4\nmerged_anno_0_1_4_p0\n\n\n4\n0.000396\n0.000097\n0.000288\n0.000649\n-3.745942\n0.326350\n-4.295006\n-3.196049\n22.283274\n0.538022\n...\n157.156162\n157.260390\n7.916547\n0.004392\n7.912271\n7.926353\n96.0\n194.0\nmerged_anno_0_1_4_p0_chunk_5\nmerged_anno_0_1_4_p0\n\n\n5\n0.000463\n0.000139\n0.000288\n0.000669\n-3.477216\n0.326321\n-4.025450\n-2.926494\n21.840251\n0.537974\n...\n157.185195\n157.281624\n7.920605\n0.006376\n7.912421\n7.931427\n120.0\n218.0\nmerged_anno_0_1_4_p0_chunk_6\nmerged_anno_0_1_4_p0\n\n\n6\n0.000517\n0.000134\n0.000288\n0.000669\n-3.208490\n0.326263\n-3.755895\n-2.656938\n21.397228\n0.537878\n...\n157.211721\n157.303893\n7.923801\n0.005444\n7.914729\n7.931427\n144.0\n242.0\nmerged_anno_0_1_4_p0_chunk_7\nmerged_anno_0_1_4_p0\n\n\n7\n0.000596\n0.000070\n0.000380\n0.000669\n-2.939764\n0.326176\n-3.486340\n-2.406044\n20.954205\n0.537735\n...\n157.236187\n157.327390\n7.920155\n0.012288\n7.885567\n7.931427\n168.0\n266.0\nmerged_anno_0_1_4_p0_chunk_8\nmerged_anno_0_1_4_p0\n\n\n8\n0.000629\n0.000029\n0.000576\n0.000669\n-2.672253\n0.324061\n-3.216784\n-2.142126\n20.510529\n0.538635\n...\n157.258711\n157.353423\n7.906374\n0.026949\n7.853243\n7.931427\n192.0\n290.0\nmerged_anno_0_1_4_p0_chunk_9\nmerged_anno_0_1_4_p0\n\n\n9\n0.000612\n0.000038\n0.000488\n0.000661\n-2.419432\n0.304439\n-2.965890\n-1.963964\n20.058402\n0.553000\n...\n157.278528\n157.370746\n7.888315\n0.031005\n7.848620\n7.931427\n216.0\n314.0\nmerged_anno_0_1_4_p0_chunk_10\nmerged_anno_0_1_4_p0\n\n\n10\n0.000570\n0.000096\n0.000373\n0.000661\n-2.185570\n0.269905\n-2.677673\n-1.763470\n19.591719\n0.575306\n...\n157.302064\n157.383235\n7.878944\n0.023500\n7.848620\n7.921558\n240.0\n338.0\nmerged_anno_0_1_4_p0_chunk_11\nmerged_anno_0_1_4_p0\n\n\n11\n0.000520\n0.000107\n0.000373\n0.000661\n-1.971889\n0.240694\n-2.424706\n-1.589662\n19.105494\n0.609702\n...\n157.325592\n157.388509\n7.893162\n0.046331\n7.848620\n8.008189\n264.0\n362.0\nmerged_anno_0_1_4_p0_chunk_12\nmerged_anno_0_1_4_p0\n\n\n12\n0.000524\n0.000115\n0.000373\n0.000714\n-1.779204\n0.223516\n-2.156786\n-1.407064\n18.598208\n0.628551\n...\n157.351755\n157.390481\n7.941962\n0.088600\n7.848620\n8.132930\n288.0\n386.0\nmerged_anno_0_1_4_p0_chunk_13\nmerged_anno_0_1_4_p0\n\n\n13\n0.000518\n0.000119\n0.000373\n0.000714\n-1.599161\n0.217004\n-1.977071\n-1.252625\n18.074484\n0.648964\n...\n157.369463\n157.391156\n8.024363\n0.122492\n7.858972\n8.245924\n312.0\n410.0\nmerged_anno_0_1_4_p0_chunk_14\nmerged_anno_0_1_4_p0\n\n\n14\n0.000479\n0.000162\n0.000197\n0.000714\n-1.426408\n0.203184\n-1.777833\n-1.089737\n17.541551\n0.649080\n...\n157.382657\n157.408323\n8.126996\n0.137761\n7.915174\n8.367000\n336.0\n434.0\nmerged_anno_0_1_4_p0_chunk_15\nmerged_anno_0_1_4_p0\n\n\n\n\n15 rows × 2080 columns\n\n\n\n\nNow we are ready to train the logistic regression classifier.\n\n\n\n\nWittenburg, Peter, Hennie Brugman, Albert Russel, Alex Klassmann, and Han Sloetjes. 2006. “ELAN: A Professional Framework for Multimodality Research.” Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006), January.",
    "crumbs": [
      "Time Series Annotation",
      "Movement annotation I: Preparing training data and data for classifier"
    ]
  },
  {
    "objectID": "04_TS_movementAnnotation/03_InterAgreement.html",
    "href": "04_TS_movementAnnotation/03_InterAgreement.html",
    "title": "Movement annotation III: Computing interrater agreement between manual and automatic annotation",
    "section": "",
    "text": "Overview\nIn this script, we prepare data to test the interrater agreement (IA) on movement annotation. To test the robustness, we compute interrater agreement between two human annotators (AC, GR) and between each human annotator and the automatic annotations created in the previous script. We compute IA for each tier separately.\nWe use EasyDIAG (Holle and Rein (2015)) to compute the IA, but document the results here in the table.\n\n\nCode to prepare the environment\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport xml.etree.ElementTree as ET\n\ncurfolder = os.getcwd()\n\n# Here we store our merged processed files\nprocessedfolder = os.path.join(curfolder + '\\\\..\\\\03_TS_processing\\\\TS_merged\\\\')\nprocessedfiles = glob.glob(processedfolder + '*.csv')\n\n# Here we store annotations from the logreg model\nannotatedfolder = os.path.join(curfolder + '\\\\TS_annotated_logreg\\\\')\nfolders = glob.glob(annotatedfolder + '*\\\\')\n\nfolders60 = [x for x in folders if '0_6' in x] #60percent confidence\nfolders80 = [x for x in folders if '0_8' in x] #80percent confidence\n\n# Here we store manual annotations from R1 (AC)\nmanualfolder1 = os.path.join(curfolder + '\\\\ManualAnno\\\\R1\\\\')\nmanualfiles1 = glob.glob(manualfolder1 + '*.eaf')\nmanualfiles1 = [x for x in manualfiles1 if 'ELAN_tiers' in x]\n\n# Here we store manual annotations from R2 (GR)\nmanualfolder2 = os.path.join(curfolder + '\\\\ManualAnno\\\\R3\\\\')\nmanualfiles2 = glob.glob(manualfolder2 + '*.eaf')\nmanualfiles2 = [x for x in manualfiles2 if 'ELAN_tiers' in x]\n\n# Here we store the txt files we need for EasyDIAG\ninterfolder = curfolder + '\\\\InterAg\\\\'\n\n\n\n\nPreprocessing annotations\nNow we need to get both manual and automatic annotations into format that EasyDIAG requires - so simple .txt files with timestamps and annotation values. For annotations that have been created by human annotators, we need to extract the timestamps and values from the .eaf files.\n\n\nCustom functions\n# Function to parse ELAN file\ndef parse_eaf_file(eaf_file, rel_tiers):\n    tree = ET.parse(eaf_file)\n    root = tree.getroot()\n\n    time_order = root.find('TIME_ORDER')\n    time_slots = {time_slot.attrib['TIME_SLOT_ID']: time_slot.attrib['TIME_VALUE'] for time_slot in time_order}\n\n    annotations = []\n    relevant_tiers = {rel_tiers}\n    for tier in root.findall('TIER'):\n        tier_id = tier.attrib['TIER_ID']\n        if tier_id in relevant_tiers:\n            for annotation in tier.findall('ANNOTATION/ALIGNABLE_ANNOTATION'):\n                # Ensure required attributes are present\n                if 'TIME_SLOT_REF1' in annotation.attrib and 'TIME_SLOT_REF2' in annotation.attrib:\n                    ts_ref1 = annotation.attrib['TIME_SLOT_REF1']\n                    ts_ref2 = annotation.attrib['TIME_SLOT_REF2']\n                    # Get annotation ID if it exists, otherwise set to None\n                    ann_id = annotation.attrib.get('ANNOTATION_ID', None)\n                    annotation_value = annotation.find('ANNOTATION_VALUE').text.strip()\n                    annotations.append({\n                        'tier_id': tier_id,\n                        'annotation_id': ann_id,\n                        'start_time': time_slots[ts_ref1],\n                        'end_time': time_slots[ts_ref2],\n                        'annotation_value': annotation_value\n                    })\n\n    return annotations\n\n# Function to write ELAN into txt file\ndef ELAN_into_txt(txtfile, raterID, foi, tier):\n    with open(txtfile, 'w') as f:\n        for file in foi:\n            print('working on ' + file)\n            # Filename\n            filename = file.split('\\\\')[-1]\n            # Parse ELAN file\n            annotations = parse_eaf_file(file, tier)\n            # Write annotations into txt file\n            for annotation in annotations:\n                f.write(f\"Anno_{raterID}\\t{annotation['start_time']}\\t{annotation['end_time']}\\t{annotation['annotation_value']}\\t{filename}\\n\")\n\n\n\nfoi = manualfiles2  # here we store manual annotations that we want to convert into txt files\nraterIDfile = 'R3'  # this is the rater as we name it in the txt files\nraterID = 'R2'      # this is the ID we need for EasyDIAG (the software always needs R1 and R2)\n\n# These are the files we want to create\ntxtfile_head = interfolder + raterIDfile + '_Manual_head.txt'\ntxtfile_upper = interfolder + raterIDfile + '_Manual_upper.txt'       # we add _2 for files where manual annotator 1 is R1, because we also want to compare with manual annotator 2 (R3)\ntxtfile_lower = interfolder + raterIDfile + '_Manual_lower.txt'\ntxtfile_arms = interfolder + raterIDfile + '_Manual_arms.txt'\n\n# For each tier, extract the annotations from ELAN file and save them in a txt file\nELAN_into_txt(txtfile_head, raterID, foi, 'head_mov')\nELAN_into_txt(txtfile_upper, raterID, foi, 'upper_body')\nELAN_into_txt(txtfile_lower, raterID, foi, 'lower_body')\nELAN_into_txt(txtfile_arms, raterID, foi, 'arms')\n\nThis is how the files look like\n\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nAnno_R1\n0\n3116\nnomovement\n0_1_11_p1_ELAN_tiers.eaf\n\n\n1\nAnno_R1\n0\n3629\nnomovement\n0_1_12_p1_ELAN_tiers.eaf\n\n\n2\nAnno_R1\n0\n3388\nnomovement\n0_1_13_p1_ELAN_tiers.eaf\n\n\n3\nAnno_R1\n0\n5120\nnomovement\n0_1_14_p1_ELAN_tiers.eaf\n\n\n4\nAnno_R1\n0\n3978\nnomovement\n0_1_15_p1_ELAN_tiers.eaf\n\n\n5\nAnno_R1\n1620\n1730\nmovement\n0_1_16_p1_ELAN_tiers.eaf\n\n\n6\nAnno_R1\n0\n1620\nnomovement\n0_1_16_p1_ELAN_tiers.eaf\n\n\n7\nAnno_R1\n1730\n3524\nnomovement\n0_1_16_p1_ELAN_tiers.eaf\n\n\n8\nAnno_R1\n1650\n3610\nmovement\n0_1_17_p1_ELAN_tiers.eaf\n\n\n9\nAnno_R1\n0\n1650\nnomovement\n0_1_17_p1_ELAN_tiers.eaf\n\n\n10\nAnno_R1\n3610\n4263\nnomovement\n0_1_17_p1_ELAN_tiers.eaf\n\n\n11\nAnno_R1\n930\n3450\nmovement\n0_1_20_p0_ELAN_tiers.eaf\n\n\n12\nAnno_R1\n0\n930\nnomovement\n0_1_20_p0_ELAN_tiers.eaf\n\n\n13\nAnno_R1\n3450\n3881\nnomovement\n0_1_20_p0_ELAN_tiers.eaf\n\n\n14\nAnno_R1\n0\n3595\nnomovement\n0_1_21_p0_ELAN_tiers.eaf\n\n\n\n\n\n\n\n\nFor automatic annotations, we need to extract the timestamps and values from the .csv files. Before doing that, we need to handle two issues that stem from the the fact that the classifier can create flickering annotations, as the confidence values continuously vary throughout each trial.\nSimilarly to Pouw et al. (2021), we apply two rules to handle this flickering: - Rule 1: If there is a nomovement event between two movement events that is shorter than 200 ms, this is considered as part of the movement event. - Rule 2: If there is a movement event between two nomovement events that is shorter than 200 ms, this is considered as part of the nomovement event.\nAfterwards, we take the first movement event and the very last movement event, and consider everything in between as a movement.\n\n\nCustom functions\n# Function to get chunks of annotations\ndef get_chunks(anno_df):\n    anno_df['chunk'] = (anno_df['anno_values'] != anno_df['anno_values'].shift()).cumsum()\n    anno_df['idx'] = anno_df.index\n\n    # Calculate start and end of each chunk, grouped by anno_values, save also the first and last index\n    chunks = anno_df.groupby(['anno_values', 'chunk']).agg(\n        time_ms_min=('time_ms', 'first'),\n        time_ms_max=('time_ms', 'last'),\n        idx_min=('idx', 'first'),\n        idx_max=('idx', 'last')\n    ).reset_index()\n\n    # Order the chunks\n    chunks = chunks.sort_values('idx_min').reset_index(drop=True)\n\n    return chunks\n\n\n\nfoi = folders80 # set which folder (threshold) you want to process\nthreshold = '80' # set the threshold\n\nfor folder in foi:\n    # get tierID\n    tier = folder.split('\\\\')[-2].split('_')[0]\n\n    if tier == 'head':\n        tier = 'head'\n    elif tier == 'upperBody':\n        tier = 'upper'\n    elif tier == 'lowerBody':\n        tier = 'lower'\n\n    # This is the file we want to create\n    txtfile = interfolder + 'AutoAnno_' + tier + '_' + threshold + '.txt'\n\n    # List all files in the folder\n    files = glob.glob(folder + '*.csv')\n\n    for file in files:\n        print('processing: ' + file)\n\n        # Filename\n        filename = file.split('\\\\')[-1].split('.')[0]\n        filename = filename.split('_')[2:6]\n        filename = '_'.join(filename)\n\n        # Check if we have manual file matching to this file, otherwise skip\n        manualfile = [x for x in manualfiles1 if filename in x]\n        if len(manualfile) == 0:\n            continue\n\n        # Now we process the annotations made by the logreg model\n        anno_df = pd.read_csv(file)\n\n        # Chunk the df to see unique annotated chunks\n        chunks = get_chunks(anno_df)\n\n        # Check for fake pauses (i.e., nomovement annotation that last for less than 200ms)\n        for i in range(1, len(chunks)-1):\n            if chunks.loc[i, 'anno_values'] == 'no movement' and chunks.loc[i-1, 'anno_values'] == 'movement' and chunks.loc[i+1, 'anno_values'] == 'movement':\n                if chunks.loc[i, 'time_ms_max'] - chunks.loc[i, 'time_ms_min'] &lt; 200:\n                    print('found a chunk of no movement between two movement chunks that is shorter than 200 ms')\n                    # Change the chunk into movement\n                    anno_df.loc[chunks.loc[i, 'idx_min']:chunks.loc[i, 'idx_max'], 'anno_values'] = 'movement'\n\n        # Calculate new chunks\n        chunks = get_chunks(anno_df)\n\n        # Now check for fake movement (i.e., movement chunk that is shorter than 200ms)\n        for i in range(1, len(chunks)-1):\n            if chunks.loc[i, 'anno_values'] == 'movement' and chunks.loc[i-1, 'anno_values'] == 'no movement' and chunks.loc[i+1, 'anno_values'] == 'no movement':\n                if chunks.loc[i, 'time_ms_max'] - chunks.loc[i, 'time_ms_min'] &lt; 200:\n                    print('found a chunk of movement between two no movement chunks that is shorter than 250 ms')\n                    # change the chunk to no movement in the original df\n                    anno_df.loc[chunks.loc[i, 'idx_min']:chunks.loc[i, 'idx_max'], 'anno_values'] = 'no movement'\n\n        \n        # Now, similarly to our human annotators, we consider movement anything from the very first movement to the very last movement\n        if 'movement' in anno_df['anno_values'].unique():\n            # Get the first and last index of movement\n            first_idx = anno_df[anno_df['anno_values'] == 'movement'].index[0]\n            last_idx = anno_df[anno_df['anno_values'] == 'movement'].index[-1]\n            # Change all between to movement\n            anno_df.loc[first_idx:last_idx, 'anno_values'] = 'movement'\n\n        # Calculate new chunks\n        chunks = get_chunks(anno_df)\n\n        # Rewrite \"no movement\" in anno_values to \"nomovement\" (to match the manual annotations)\n        chunks['anno_values'] = chunks['anno_values'].apply(\n            lambda x: 'nomovement' if x == 'no movement' else x\n        )\n\n        # Add elanID to chunks (to match the manual annotations in EasyDIAG)\n        chunks['elanID']  = str(filename + '_ELAN_tiers.eaf')\n\n        # Write to the text file\n        with open(txtfile, 'a') as f:\n            for _, row in chunks.iterrows():\n                f.write(\n                    f\"Anno_R1\\t{row['time_ms_min']}\\t{row['time_ms_max']}\\t{row['anno_values']}\\t{row['elanID']}\\n\"\n                )\n\n\n\nCreating txt files for EasyDIAG\nEasyDIAG requires a txt file that contains all annotations of a tier from both annotators we wish to compare. We therefore need to merge the files we have created above into one file for each tier.\n(Note that it is better to delete old files rather than let them overwrite because that can lead to some bugs in the files for which the agreement will be messy)\n\n# eval: false\n\n# These tiers we want to compare\ntoi = ['arms', 'head', 'upper', 'lower'] \n\n# We want to compare\n## auto60 with R1  \n## auto80 with R1 \n## auto60 with R3  \n## auto80 with R3 \n# r1_2 with r3\n\n# For us R1 is the manual annotator, R3 is second manual annotator, R2 is the automatic annotator\n# But note that manual annotator is in the txt files always as R2, and automatic annotator is always R1 \n\ncomp1 = 'R3'    # change here who you want to compare\ncomp2 = 'R1'    # with whom\n\n# Add adding if necessary\nadding = 'manual'\n\nfor tier in toi:\n    print('working on ' + tier)\n\n    txtfile_auto60 = interfolder + 'AutoAnno_' + tier + '_60.txt'        # this is the automatic annotator with threshold 60\n    txtfile_auto80 = interfolder + 'AutoAnno_' + tier + '_80.txt'        # this is the automatic annotator with threshold 80\n    txtfile_manual_r1 = interfolder + 'R1_Manual_' + tier + '.txt'  # this is manual annotator (AC) as R2\n    txtfile_manual_r3 = interfolder + 'R3_Manual_' + tier + '.txt'  # this is manual annotator (GR) as R2\n    txtfile_manual_r1_2 = interfolder + 'R1_Manual_' + tier + '_2.txt'  # this is manual annotator (AC) as R1\n\n    # Read in the files we want to compare\n    r1_anno = pd.read_csv(txtfile_manual_r3, sep='\\t', header=None)         # change here who you want to compare\n    r2_anno = pd.read_csv(txtfile_manual_r1_2, sep='\\t', header=None)    # with whom\n \n    # Check that both files have the same number of files (EasyDIAG will ignore this mismatch and lower the agreement)\n    files_to_check_r1 = r1_anno[4].unique()\n    files_to_check_r2 = r2_anno[4].unique()\n    files_to_check = list(set(files_to_check_r1) & set(files_to_check_r2))\n\n    # Adapt both\n    rows_auto = r1_anno[r1_anno[4].isin(files_to_check)]\n    rows_manual = r2_anno[r2_anno[4].isin(files_to_check)]\n\n    # And concatenate\n    concat_rows = pd.concat([r1_anno, r2_anno])\n\n    # Save as new file\n    txtfile_IA = interfolder + 'IA_' + comp1 + '_' + comp2 + '_' + tier + '_' + adding + '.txt' # adapt the threshold based on what you work with\n\n    with open(txtfile_IA, 'w') as f:\n        for index, row in concat_rows.iterrows():\n            f.write(f\"{row[0]}\\t{row[1]}\\t{row[2]}\\t{row[3]}\\t{row[4]}\\n\")\n\n\n\nInterrater agreement: results\nHere we report the raw agreements together with kappa coefficients for interrater agreement between manual annotators (R1, R3) and automatic annotations (with threshold 60 and 80). Interrater agreement was computed using EasyDIAG (Holle and Rein (2015)), and the results have been saved in txt files. We will now extract relevant information to report in the table. The overlap was kept at default value of 60% for all tiers.\n\ndef extract_ia(lines):\n    # Extracting values\n    linked_units = None\n    raw_agreement = None\n    kappa = None\n\n    inside_section_2 = False  # Flag to track section 2\n\n    for line in lines:\n        if \"Percentage of linked units:\" in line:\n            inside_section_2 = False  # Ensure we don't mistakenly extract from other parts\n        elif \"linked\" in line and \"=\" in line:\n            linked_units = float(line.split(\"=\")[-1].strip().replace(\"%\", \"\"))  # Extract linked %\n\n        elif \"2) Overall agreement indicies (including no match):\" in line:\n            inside_section_2 = True  # Activate flag when entering section 2\n\n        elif inside_section_2:\n            if \"Raw agreement\" in line and \"=\" in line:\n                raw_agreement = float(line.split(\"=\")[-1].strip())  # Extract correct raw agreement\n            elif \"kappa \" in line and \"=\" in line:\n                kappa = float(line.split(\"=\")[-1].strip())  # Extract correct kappa\n            elif \"3)\" in line:\n                inside_section_2 = False  # Stop when reaching section 3\n\n    return linked_units, raw_agreement, kappa\n\n\nfiles = glob.glob(interfolder + '*EasyDIAG*')\n\nIA_df = pd.DataFrame()\n\nfor txtfile in files:\n    with open(txtfile, 'r') as f:\n        lines = f.readlines()\n    comparison = txtfile.split('\\\\')[-1].split('.')[0].split('_')[1]+'_'+txtfile.split('\\\\')[-1].split('_')[2]\n    if 'Auto' in txtfile:\n        tier = txtfile.split('\\\\')[-1].split('.')[0].split('_')[-2]+'_'+txtfile.split('\\\\')[-1].split('.')[0].split('_')[-1]\n    else:\n        tier = txtfile.split('\\\\')[-1].split('.')[0].split('_')[-1]\n\n\n    linked_units, raw_agreement, kappa = extract_ia(lines)\n\n    IA_row = pd.DataFrame({\n        'comparison': comparison,\n        'tier': tier,\n        'linked_units': linked_units,\n        'raw_agreement': raw_agreement,\n        'kappa': kappa\n    }, index=[0])\n\n    IA_df = pd.concat([IA_df, IA_row])\n\n\n\n\n\n\n\n\n\n\n\ncomparison\ntier\nlinked_units\nraw_agreement\nkappa\n\n\n\n\n0\nR1_Auto\narms_60\n0.81\n0.81\n0.65\n\n\n0\nR1_Auto\narms_80\n0.81\n0.81\n0.64\n\n\n0\nR1_Auto\nhead_60\n0.60\n0.56\n0.32\n\n\n0\nR1_Auto\nhead_80\n0.63\n0.58\n0.34\n\n\n0\nR1_Auto\nlower_60\n0.80\n0.79\n0.59\n\n\n0\nR1_Auto\nlower_80\n0.75\n0.73\n0.50\n\n\n0\nR1_Auto\nupper_60\n0.67\n0.66\n0.44\n\n\n0\nR1_Auto\nupper_80\n0.67\n0.67\n0.43\n\n\n0\nR3_Auto\narms_60\n0.75\n0.75\n0.58\n\n\n0\nR3_Auto\narms_80\n0.73\n0.72\n0.53\n\n\n0\nR3_Auto\nhead_60\n0.62\n0.60\n0.39\n\n\n0\nR3_Auto\nhead_80\n0.64\n0.61\n0.39\n\n\n0\nR3_Auto\nlower_60\n0.66\n0.64\n0.38\n\n\n0\nR3_Auto\nlower_80\n0.70\n0.68\n0.41\n\n\n0\nR3_Auto\nupper_60\n0.67\n0.66\n0.44\n\n\n0\nR3_Auto\nupper_80\n0.63\n0.61\n0.37\n\n\n0\nR1_R3\narms\n0.85\n0.84\n0.70\n\n\n0\nR1_R3\nhead\n0.67\n0.62\n0.38\n\n\n0\nR1_R3\nlower\n0.74\n0.71\n0.46\n\n\n0\nR1_R3\nupper\n0.70\n0.68\n0.44\n\n\n\n\n\n\n\n\nFrom the table, we can observe few things: - 60% and 80% thresholds for automatic annotations are yielding similar results both in terms of raw agreement and kappa coefficient. - for arms, interrater agreement between automatic annotation and manual annotator R1 results in kappa coefficient 0.65, which is considered substantial agreement (e.g., by Landis and Koch (1977)). - for upper body and lower body - the kappa signifies moderate agreement, but the same drop we can see in the interrater agreement between manual annotators. - for head, we see only fair agreement both between manual annotators and between automatic annotation and manual annotators.\nGenerally, interrater agreement between manual annotator R1 and automatic annotation is comparable to the agreement between the two human annotators across all tiers. This suggests that the automatic annotation is a reliable tool for movement annotation, especially for arms. It seems like head is the most difficult tier to annotate, which is also reflected in the interrater agreement between manual annotators.\nTo improve its predictions for head, upper body, and lower body, and to avoid the risk of overfitting the model on a specific type of behaviour generated by individuals in dyad 0, we will extend the training data by annotating 10% of behaviour per participant per dyad before the final analysis. If kappa does not result in minimum substantial agreement (k = 0.61), we will annotate a larger portion of the data\nIn the next script, we will work with 60% threshold to annotate all the data.\n\n\nReferences\n\n\n\n\nHolle, Henning, and Robert Rein. 2015. “EasyDIAg: A Tool for Easy Determination of Interrater Agreement.” Behavior Research Methods 47 (3): 837–47. https://doi.org/10.3758/s13428-014-0506-7.\n\n\nLandis, J. R., and G. G. Koch. 1977. “The Measurement of Observer Agreement for Categorical Data.” Biometrics 33 (1). https://pubmed.ncbi.nlm.nih.gov/843571/.\n\n\nPouw, Wim, Jan de Wit, Sara Bögels, Marlou Rasenberg, Branka Milivojevic, and Asli Ozyurek. 2021. “Semantically Related Gestures Move Alike: Towards a Distributional Semantics of Gesture Kinematics.” In Digital Human Modeling and Applications in Health, Safety, Ergonomics and Risk Management. Human Body, Motion and Behavior, edited by Vincent G. Duffy, 269–87. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-77817-0_20.",
    "crumbs": [
      "Time Series Annotation",
      "Movement annotation III: Computing interrater agreement between manual and automatic annotation"
    ]
  },
  {
    "objectID": "06_ConceptSimilarity/ConceptNet_similarity.html",
    "href": "06_ConceptSimilarity/ConceptNet_similarity.html",
    "title": "Computing concept similarity using ConceptNet word embeddings",
    "section": "",
    "text": "Overview\nOur second hypothesis tests the effect of degree of misunderstanding on the magnitude of effort.\nWe operationalize degree of misunderstanding as a conceptual similarity between target concept and answer offered by a guesser.\nTo have a reproducible measure of conceptual similarity, we use the ConceptNet (Speer, Chin, and Havasi (2018)) to extract embeddings for concepts used in our study, and calculate cosine similarity between the target concept and guessed answer.\nTo verify the utility of the cosine similarity, we have collected data from 14 Dutch-native peoplewho were asked to rate the similarity between each pair of words in online anonymous rating study. We then compare the ‘perceived similarity’ with cosine similarity computed from ConceptNet embeddings, to validate the use of ConceptNet embeddings as a measure of conceptual similarity.\n\n\nCode to load packages and prepare environment\nimport numpy as np\nimport os\nimport glob\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pingouin\nimport openpyxl\n\ncurfolder = os.getcwd()\nrawdata = curfolder + '\\\\..\\\\00_RAWDATA\\\\'\nanswerfiles = glob.glob(rawdata + '*\\\\*.csv', recursive=True)\ndatafolder = curfolder + '\\\\data\\\\'\n\n# Load all files that have '_1_results' in the name \nanswerfiles_1 = [f for f in answerfiles if '_1_results' in f]\n# Loop over list and add it into one big df\ndf_all1 = pd.DataFrame()\nfor file in answerfiles_1:\n    df = pd.read_csv(file)\n    df_all1 = pd.concat([df_all1, df], ignore_index=True)\n\ndf_all1['exp'] = 1 \n\n# Load all files that have '_2_results' in the name\nanswerfiles_2 = [f for f in answerfiles if '_2_results' in f]\n# Loop over list and add it into one big df\ndf_all2 = pd.DataFrame()\nfor file in answerfiles_2:\n    df = pd.read_csv(file)\n    df_all2 = pd.concat([df_all2, df], ignore_index=True)\n\ndf_all2['exp'] = 2\n\n# Merge\ndf_all = pd.concat([df_all1, df_all2], ignore_index=True)\n\n# Keep only columns word and answer\ndf = df_all[['word', 'answer', 'exp']]\n\n\nFirst we need to do some data-wrangling to get all in the right format for the embedding extraction and comparison\n\n# concept list\ndf_concepts = pd.read_excel(rawdata + '/conceptlist_info.xlsx')\n\n# in df_concepts, keep only English and Dutch\ndf_concepts = df_concepts[['English', 'Dutch']]\n\n# rename Dutch to word\ndf_concepts = df_concepts.rename(columns={'Dutch': 'word'})\n\n# merge df and df_concepts on word\ndf = pd.merge(df, df_concepts, on='word', how='left')\n\n# show rows where English is NaN\ndf[df['English'].isnull()]\n\n# add translations manually for each (these are practice trials)\ndf.loc[df['word'] == 'bloem', 'English'] = 'flower'\ndf.loc[df['word'] == 'dansen', 'English'] = 'to dance'\ndf.loc[df['word'] == 'auto', 'English'] = 'car'\ndf.loc[df['word'] == 'olifant', 'English'] = 'elephant'\ndf.loc[df['word'] == 'comfortabel', 'English'] = 'comfortable'\ndf.loc[df['word'] == 'bal', 'English'] = 'ball'\ndf.loc[df['word'] == 'haasten', 'English'] = 'to hurry'\ndf.loc[df['word'] == 'gek', 'English'] = 'crazy'\ndf.loc[df['word'] == 'snijden', 'English'] = 'to cut'\ndf.loc[df['word'] == 'koken', 'English'] = 'to cook'\ndf.loc[df['word'] == 'juichen', 'English'] = 'to cheer'\ndf.loc[df['word'] == 'zingen', 'English'] = 'to sing'\ndf.loc[df['word'] == 'glimlach', 'English'] = 'smile'\ndf.loc[df['word'] == 'klok', 'English'] = 'clock'\ndf.loc[df['word'] == 'fiets', 'English'] = 'bicycle'\ndf.loc[df['word'] == 'vliegtuig', 'English'] = 'airplane'\ndf.loc[df['word'] == 'geheim', 'English'] = 'secret'\ndf.loc[df['word'] == 'telefoon', 'English'] = 'telephone'\ndf.loc[df['word'] == 'zwaaien', 'English'] = 'to wave'\ndf.loc[df['word'] == 'sneeuw', 'English'] = 'snow'\n\n# make a list of English answers\nanswers_en = ['party', 'to cheer', 'tasty', 'to shoot', 'to breathe', 'zombie', 'bee', 'sea', 'dirty', 'tasty', 'car', 'to eat', 'to eat', 'to blow', 'hose', 'hose', 'to annoy', 'to make noise', 'to make noise', 'to run away', 'elephant', 'to cry', 'cold', 'outfit', 'silence', 'to ski', 'wrong', 'to play basketball', 'to search', 'disturbed', 'to run', 'to lick', 'to lift', 'lightning', 'to think', 'to jump', 'to fall', 'to write', 'to dance', 'shoulder height', 'horn', 'dirty', 'boring', 'to drink', 'strong', 'elderly', 'to mix', 'fish', 'fish', 'dirty', 'wrong', 'smart', 'to box', 'to box', 'dog', 'to catch', 'to cheer', 'to sing', 'pregnant', 'hair', 'to shower', 'pain', 'burnt', 'hot', 'I', 'to chew', 'bird', 'airplane', 'to fly', 'to think', 'to choose', 'to doubt', 'graffiti', 'fireworks', 'bomb', 'to smile', 'to laugh', 'smile', 'clock', 'to wonder', 'height', 'big', 'height', 'space', 'to misjudge', 'to wait', 'satisfied', 'happy', 'fish', 'to smell', 'wind', 'pain', 'to burn', 'hot', 'to cycle', 'to fly', 'airplane', 'bird', 'to crawl', 'to drink', 'waterfall', 'water', 'fire', 'top', 'good', 'to hear', 'to point', 'distance', 'there', 'to whisper', 'quiet', 'to be silent', 'telephone', 'to blow', 'to distribute', 'to give', 'cat', 'to laugh', 'tasty', 'to eat', 'yummy', 'to sleep', 'mountain', 'dirty', 'to vomit', 'to be disgusted', 'to greet', 'hello', 'goodbye', 'to smell', 'nose', 'odor', 'to fly', 'fireworks', 'to blow', 'to cut', 'pain', 'hot', 'to slurp', 'to throw', 'to fall', 'to fall', 'whistle', 'heartbeat', 'mouse', 'to hit', 'to catch', 'to grab', 'to throw', 'to fall', 'to shoot', 'circus', 'trunk', 'to fall', 'to fight', 'pain', 'to push open', 'to growl', 'to cut', 'to eat', 'knife', 'to slurp', 'to drink', 'drink', 'to eat', 'delicious', 'tasty', 'to cough', 'sick', 'to cry', 'to cry']\n\n# replace skien with skiën in the df\ndf['answer'] = df['answer'].str.replace('skien', 'skiën')\n\n# get rid of English 'to beat'\ndf_final = df[df['English'] != 'to beat']\n# and to weep\ndf_final = df[df['English'] != 'to weep']\n# and noisy\ndf_final = df[df['English'] != 'noisy']\n\n# add those to df as answers_en\ndf['answer_en'] = answers_en\n\n# make a list of English targets\nmeanings_en = list(df['English'])\n# Dutch targets\nmeanings_nl = list(df['word'])\n# Dutch answers\nanswers_nl = list(df['answer'])\n\n# Save it\ndf.to_csv(datafolder + 'concept_answer_withoutcossim.csv', index=False)\n\nThis is how the dataframe looks like\n\n\n\n\n\n\n\n\n\n\nword\nanswer\nexp\nEnglish\nanswer_en\n\n\n\n\n0\nbloem\nfeest\n1\nflower\nparty\n\n\n1\ndansen\njuichen\n1\nto dance\nto cheer\n\n\n2\nbitter\nlekker\n1\nbitter\ntasty\n\n\n3\nvechten\nschieten\n1\nto fight\nto shoot\n\n\n4\nademen\nademen\n1\nto breathe\nto breathe\n\n\n5\nbijten\nzombie\n1\nto bite\nzombie\n\n\n6\nzoemen\nbij\n1\nbuzz\nbee\n\n\n7\nfluisteren\nzee\n1\nto whisper\nsea\n\n\n8\nwalgen\nvies\n1\ndisgusted\ndirty\n\n\n9\nlangzaam\nlekker\n1\nslow\ntasty\n\n\n10\nauto\nauto\n1\ncar\ncar\n\n\n11\neten\neten\n1\nto eat\nto eat\n\n\n12\nei\neten\n1\negg\nto eat\n\n\n13\nzwemmen\nwaaien\n1\nto swim\nto blow\n\n\n14\nsnel\nwaterslang\n1\nfast\nhose\n\n\n\n\n\n\n\n\n\n\nCalculating cosine similarity\nNow we will load in ConceptNet numberbatch (version 19.08) and compute cosine similarity for each pair\n\n\nCustom functions\n# Load embeddings from a file\ndef load_embeddings(file_path):\n    embeddings = {}\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.array(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n\n# Cosine similarity\ndef cosine_similarity(vec1, vec2):\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    return dot_product / (norm_vec1 * norm_vec2)\n\n\nWe will use multilingual numberbatch to extract words in the original language of experiment - Dutch. While English has better representation in ConceptNet, the English numberbatch does not make distinction between nouns and verbs (so ‘a drink’ and ‘to drink’ have common representation - drink). Because this is important distinction for us, we opt for Dutch embeddings to avoid this problem\n\n# load embeddings\nembeddings = load_embeddings('numberbatch\\\\numberbatch.txt')\n\nThis is how a single concept is represented (here skiën, engl. skiing)\n\n\n[ 3.410e-02 -4.640e-02  5.490e-02  1.544e-01  1.800e-02 -5.050e-02\n -6.660e-02 -2.300e-02  5.320e-02  1.104e-01  2.770e-02  5.040e-02\n -2.010e-02  5.900e-03 -1.133e-01 -9.370e-02 -7.890e-02  3.540e-02\n  3.780e-02  8.400e-02 -3.880e-02  7.680e-02 -8.010e-02  6.540e-02\n -1.493e-01 -1.036e-01  8.490e-02  1.040e-02 -6.890e-02  6.890e-02\n  1.226e-01 -1.850e-02  1.520e-02  2.810e-02 -5.660e-02 -2.670e-02\n -5.700e-02 -4.480e-02  1.924e-01  5.800e-02 -7.800e-02 -7.700e-03\n  1.132e-01  6.350e-02 -4.310e-02  1.900e-03 -4.820e-02  1.047e-01\n  6.900e-02  7.150e-02  1.660e-02  2.730e-02  4.340e-02  1.130e-02\n -1.427e-01 -9.200e-03 -8.000e-04  2.310e-02  1.234e-01 -1.452e-01\n -1.710e-02 -1.094e-01 -1.518e-01  4.820e-02  1.400e-02 -1.460e-02\n  1.023e-01  5.220e-02  1.362e-01  3.190e-02 -2.590e-02  1.220e-01\n  1.750e-02  8.810e-02 -9.200e-02 -1.226e-01 -5.560e-02 -6.600e-03\n  3.180e-02 -1.113e-01  6.130e-02 -1.202e-01 -2.480e-02 -8.300e-03\n -1.710e-02  3.410e-02  1.550e-02 -8.000e-02 -6.390e-02  1.170e-01\n -1.578e-01  2.660e-02 -1.360e-02 -6.790e-02 -1.030e-02 -3.320e-02\n  1.040e-02  1.600e-03 -1.300e-03 -4.420e-02  4.060e-02 -7.470e-02\n -2.800e-03  1.900e-02  1.098e-01 -6.180e-02 -5.690e-02  7.700e-02\n -2.000e-03 -1.050e-02 -9.610e-02  7.000e-03  5.310e-02  2.720e-02\n  5.850e-02  5.000e-02  4.400e-03 -3.210e-02  3.000e-02 -6.020e-02\n  1.190e-02  3.760e-02  3.650e-02  1.452e-01  1.900e-03  3.870e-02\n  5.210e-02  4.470e-02  5.570e-02  1.460e-02 -8.330e-02 -3.660e-02\n -8.020e-02 -6.520e-02  3.660e-02  2.260e-02 -7.050e-02 -9.100e-03\n  2.190e-02 -2.630e-02 -1.410e-02  6.200e-03 -2.130e-02 -3.750e-02\n  1.960e-02 -1.250e-02  2.740e-02  8.450e-02 -8.750e-02 -2.700e-02\n  8.500e-03  1.260e-02 -2.550e-02  1.640e-02  2.850e-02 -5.570e-02\n -1.110e-02  9.400e-02  3.280e-02 -4.830e-02  1.080e-02  1.170e-02\n  8.600e-03  6.620e-02  1.151e-01  2.960e-02  1.670e-02  1.180e-02\n -1.101e-01 -5.720e-02 -2.910e-02 -2.450e-02 -3.700e-03 -4.650e-02\n  7.500e-03 -4.350e-02 -3.150e-02 -4.980e-02 -3.750e-02  6.250e-02\n  6.600e-03 -9.910e-02  3.180e-02 -4.350e-02 -2.830e-02 -9.260e-02\n -6.110e-02 -1.280e-02 -7.960e-02  3.280e-02  8.670e-02 -1.080e-02\n -1.430e-02  1.350e-02 -6.210e-02 -2.220e-02 -6.010e-02 -4.440e-02\n  5.820e-02  9.090e-02  9.300e-03 -4.130e-02 -3.040e-02 -6.410e-02\n  4.760e-02 -6.620e-02 -1.000e-04  5.300e-02 -3.650e-02  7.500e-03\n  3.460e-02 -5.350e-02  8.900e-03 -9.140e-02 -2.830e-02 -4.290e-02\n  9.000e-03 -7.400e-02  1.014e-01 -8.760e-02  3.200e-03 -4.420e-02\n -2.990e-02  1.000e-04  3.900e-02 -6.200e-03  7.740e-02  2.840e-02\n  1.170e-02 -3.680e-02  2.790e-02  6.890e-02  1.800e-03  3.000e-03\n -3.070e-02 -5.000e-04  1.000e-03 -2.200e-03 -2.480e-02  1.117e-01\n -2.340e-02  3.780e-02  3.350e-02 -4.950e-02  9.500e-03  1.720e-02\n  4.720e-02 -6.040e-02 -3.220e-02 -1.570e-02 -3.860e-02  1.140e-02\n -4.240e-02 -1.710e-02 -8.000e-04  3.900e-02  1.410e-02 -8.580e-02\n -3.250e-02 -1.650e-02 -2.520e-02  1.140e-02  7.050e-02  6.000e-04\n  3.570e-02  4.750e-02 -7.050e-02  2.500e-02 -2.940e-02  1.830e-02\n  9.600e-03  1.600e-02  3.910e-02  3.970e-02 -1.540e-02  8.070e-02\n -1.000e-02 -6.400e-03 -4.050e-02  3.030e-02 -6.060e-02  1.690e-02\n  4.370e-02 -6.500e-03  2.920e-02 -2.900e-03 -1.017e-01  7.060e-02\n -1.053e-01 -1.500e-03  9.000e-03  9.500e-03  4.020e-02  2.420e-02\n  1.800e-03  4.270e-02  3.140e-02 -2.540e-02  2.780e-02 -6.000e-03]\n\n\nNow we take the list of target-answer pairs, transform them into embedding format and perform cosine similarity.\n\n# get the embeddings for the words in the list meanings_en\nword_embeddings_t = {}\nfor word in meanings_nl:\n    word_embed = '/c/nl/' + word\n    if word_embed in embeddings:\n        word_embeddings_t[word] = embeddings[word_embed]\n\n# get the embeddings for the words in the list answers_en\nword_embeddings_ans = {}\nfor word in answers_nl:\n    word_embed = '/c/nl/' + word\n    if word_embed in embeddings:\n        word_embeddings_ans[word] = embeddings[word_embed]\n\n# calculate the similarity between the first word in the list meanings_en and first word in answers_en, second word in meanings_en and second word in answers_en, etc.\ncosine_similarities = []\n\nfor i in range(len(meanings_nl)):\n    word1 = meanings_nl[i]\n    word2 = answers_nl[i]\n    vec1 = word_embeddings_t.get(word1)\n    vec2 = word_embeddings_ans.get(word2)\n    if vec1 is not None and vec2 is not None:\n        cosine_sim = cosine_similarity(vec1, vec2)\n        cosine_similarities.append(cosine_sim)\n    else:\n        # print which concepts could not be found\n        if vec1 is None:\n            print(f\"Concept not found: {word1}\")\n        if vec2 is None:\n            print(f\"Concept not found: {word2}\")\n        cosine_similarities.append(None)\n\ndf['cosine_similarity'] = cosine_similarities\n\n# Save it\ndf.to_csv(datafolder + 'conceptnet_clean.csv', index=False)\n\nConcept not found: lawaai maken\nConcept not found: lawaai maken\nConcept not found: schouderhoogte\nConcept not found: openduwen\n\n\nWhen running the code, we will see that some target or answered concepts are not represented in numberbatch (e.g., if the answer has more than one word).\nBecause we verified that cosine similarity and perceived similarity are highly correlated (see below), we will collect the missing data through new online rating study.\n\n\nComparing cosine similarity against perceived similarity\nTo validate the use of ConceptNet embeddings as a measure of conceptual similarity, we compare the cosine similarity computed from ConceptNet embeddings with the ‘perceived similarity’ ratings collected in the online anonymous rating study.\nThe rating study has been introduced to the participants in a way that closely relates to the experiment. The instructions go as follows:\nBelow is a list of 171 pairs of words. Your task is to go through them and rate on the scale from 0 to 10 how similar they are/feel for you. You can for example imagine that you are playing a game where you need to explain the first word from the pair (e.g., to dance), and someone answers the second word in the pair. In such a situation, how close is the guesser from the intended word? If they answer ‘to dance’, then the two words are completely identical. But if they answer ‘a car’ it is not similar at all. Rate it according to your intuition, there is no incorrect answer. Note that the survey is completely anonymous and we are not collecting any of your personal data, only the ratings.\nThis is how the survey results look like:\n\n\n\n\n\n\n\n\n\n\nbloem - feest\ndansen - juichen\nbitter - lekker\nvechten - schieten\nademen - ademen\nbijten - zombie\nzoemen - bij\nfluisteren - zee\nwalgen - vies\nlangzaam - lekker\n...\nzout - mes\nzuigen - slurpen\nzuigen - drinken\nzuigen - drink\ndik - eten\ndik - heerlijk\ndik - lekker\nziek - hoesten\nziek - ziek\nhuilen - huilen\n\n\n\n\n0\n1\n3\n1\n7\n10\n5\n8\n1\n8\n0\n...\n0\n8\n6\n5\n6\n5\n5\n8\n10\n10\n\n\n1\n3\n6\n1\n6\n10\n4\n8\n0\n8\n1\n...\n2\n8\n8\n8\n7\n4\n5\n7\n10\n10\n\n\n2\n5\n8\n5\n8\n10\n7\n7\n2\n8\n5\n...\n2\n7\n7\n7\n7\n5\n6\n8\n10\n10\n\n\n3\n4\n6\n4\n8\n10\n7\n9\n7\n7\n0\n...\n2\n7\n7\n8\n6\n2\n2\n8\n10\n10\n\n\n4\n6\n0\n5\n6\n10\n7\n8\n0\n6\n0\n...\n0\n7\n8\n7\n4\n0\n0\n8\n10\n10\n\n\n5\n0\n4\n2\n3\n10\n6\n5\n2\n8\n0\n...\n0\n6\n6\n4\n0\n0\n0\n3\n10\n10\n\n\n6\n0\n1\n1\n6\n10\n4\n8\n0\n9\n0\n...\n0\n5\n6\n8\n2\n0\n0\n7\n10\n10\n\n\n7\n1\n5\n1\n2\n10\n3\n6\n0\n4\n0\n...\n1\n6\n5\n4\n6\n3\n4\n6\n10\n10\n\n\n8\n2\n3\n3\n4\n10\n0\n7\n0\n8\n1\n...\n0\n7\n0\n2\n6\n4\n2\n8\n10\n10\n\n\n9\n0\n2\n0\n2\n10\n5\n0\n0\n8\n0\n...\n0\n0\n3\n2\n0\n0\n2\n5\n10\n10\n\n\n10\n3\n4\n4\n7\n10\n4\n6\n3\n8\n3\n...\n4\n7\n4\n6\n6\n5\n6\n7\n10\n10\n\n\n11\n0\n2\n0\n5\n10\n8\n7\n3\n9\n0\n...\n0\n5\n2\n2\n5\n2\n1\n8\n10\n10\n\n\n12\n0\n0\n4\n0\n10\n0\n2\n0\n2\n0\n...\n0\n1\n1\n3\n3\n5\n0\n2\n10\n10\n\n\n13\n0\n1\n2\n1\n10\n0\n0\n0\n0\n1\n...\n0\n2\n1\n0\n0\n0\n0\n0\n10\n10\n\n\n\n\n14 rows × 166 columns\n\n\n\n\nNow we have to calculate mean rating for each pair\n\n# for each column, calculate the mean and save it to a df\ndf_survey_means = pd.DataFrame(df_survey.mean()).reset_index()\n\n# separate the index, the first part is English, the second part is the answer_en\ndf_survey_means['word'] = df_survey_means['index'].str.split(' - ').str[0]\ndf_survey_means['answer'] = df_survey_means['index'].str.split(' - ').str[1]\n\n# get rid of the index column\ndf_survey_means = df_survey_means.drop(columns='index')\n\n# rename the column 0 to mean_similarity\ndf_survey_means = df_survey_means.rename(columns={0: 'mean_similarity'})\n\n##### some corrections ####\n# get rid of all invisible spaces in answer\ndf_survey_means['answer'] = df_survey_means['answer'].str.strip()\n# where word is vangen and answer vagen, change answer to vangen, and add similarity to 10\ndf_survey_means.loc[(df_survey_means['word'] == 'vagen') & (df_survey_means['answer'] == 'vangen'), 'word'] = 'vangen'\ndf_survey_means.loc[(df_survey_means['word'] == 'vangen') & (df_survey_means['answer'] == 'vangen'), 'mean_similarity'] = 10\n# where word is lopen and answer skien, change answer to skiën\ndf_survey_means.loc[(df_survey_means['word'] == 'lopen') & (df_survey_means['answer'] == 'skien'), 'answer'] = 'skiën'\n# add one missing pair vallen-vallen with mean_similarity 10\nmissing_row = pd.DataFrame({'word': ['vallen'], 'answer': ['vallen'], 'mean_similarity': [10]})\ndf_survey_means = pd.concat([df_survey_means, missing_row], ignore_index=True)\n\n# display\ndf_survey_means.head(15)\n\n\n\n\n\n\n\n\n\nmean_similarity\nword\nanswer\n\n\n\n\n0\n1.785714\nbloem\nfeest\n\n\n1\n3.214286\ndansen\njuichen\n\n\n2\n2.357143\nbitter\nlekker\n\n\n3\n4.642857\nvechten\nschieten\n\n\n4\n10.000000\nademen\nademen\n\n\n5\n4.285714\nbijten\nzombie\n\n\n6\n5.785714\nzoemen\nbij\n\n\n7\n1.285714\nfluisteren\nzee\n\n\n8\n6.642857\nwalgen\nvies\n\n\n9\n0.785714\nlangzaam\nlekker\n\n\n10\n10.000000\nauto\nauto\n\n\n11\n10.000000\neten\neten\n\n\n12\n5.285714\nei\neten\n\n\n13\n1.142857\nzwemmen\nwaaien\n\n\n14\n0.500000\nsnel\nwaterslang\n\n\n\n\n\n\n\n\nNow we can merge it with the cosine similarity dataframe\n\n# load in similarity\ndf_similarity = pd.read_csv(datafolder + 'conceptnet_clean.csv')\n\n# merge df_survey_means with df on English and answer_en\ndf_final = pd.merge(df_similarity, df_survey_means, on=['word', 'answer'], how='left')\n\n# get rid of English 'to beat'\ndf_final = df_final[df_final['English'] != 'beat']\n# and to weep\ndf_final = df_final[df_final['English'] != 'weep']\n\n# save it\ndf_final.to_csv(datafolder + '/df_final_conceptnet.csv', index=False)\n\n# Display\ndf_final.head(15)\n\n\n\n\n\n\n\n\n\nword\nanswer\nexp\nEnglish\nanswer_en\ncosine_similarity\nmean_similarity\n\n\n\n\n0\nbloem\nfeest\n1\nflower\nparty\n0.135571\n1.785714\n\n\n1\ndansen\njuichen\n1\nto dance\nto cheer\n0.177888\n3.214286\n\n\n2\nbitter\nlekker\n1\nbitter\ntasty\n0.257505\n2.357143\n\n\n3\nvechten\nschieten\n1\nto fight\nto shoot\n0.205791\n4.642857\n\n\n4\nademen\nademen\n1\nto breathe\nto breathe\n1.000000\n10.000000\n\n\n5\nbijten\nzombie\n1\nto bite\nzombie\n0.068596\n4.285714\n\n\n6\nzoemen\nbij\n1\nbuzz\nbee\n0.164508\n5.785714\n\n\n7\nfluisteren\nzee\n1\nto whisper\nsea\n0.072605\n1.285714\n\n\n8\nwalgen\nvies\n1\ndisgusted\ndirty\n0.353700\n6.642857\n\n\n9\nlangzaam\nlekker\n1\nslow\ntasty\n0.077073\n0.785714\n\n\n10\nauto\nauto\n1\ncar\ncar\n1.000000\n10.000000\n\n\n11\neten\neten\n1\nto eat\nto eat\n1.000000\n10.000000\n\n\n12\nei\neten\n1\negg\nto eat\n0.233187\n5.285714\n\n\n13\nzwemmen\nwaaien\n1\nto swim\nto blow\n0.065727\n1.142857\n\n\n14\nsnel\nwaterslang\n1\nfast\nhose\n0.081750\n0.500000\n\n\n\n\n\n\n\n\nNow we can finally run correlation\n\n# get rid of all lines where mean_similarity is 10.0 - otherwise we will drag the correlation up\ndf_corr = df_final[df_final['mean_similarity'] != 10.0]\n\nfeature1 = \"cosine_similarity\"\nfeature2 = \"mean_similarity\"\n\n# create a sub-dataframe with the selected features, dropping missing values\nsubdf = df_corr[[feature1, feature2]].dropna()\n\n# compute the correlation coefficient, with Bayes factor\ncorr_with_bf = pingouin.pairwise_corr(subdf, columns=['cosine_similarity', 'mean_similarity'], method='pearson', alternative='two-sided')\n\n# display\nprint(corr_with_bf)\n\n                   X                Y   method alternative    n         r  \\\n0  cosine_similarity  mean_similarity  pearson   two-sided  122  0.730051   \n\n         CI95%         p-unc       BF10  power  \n0  [0.63, 0.8]  1.430306e-21  3.728e+18    1.0  \n\n\nAnd here we see the relationship visually\n\n\n\n\n\n\n\n\n\nThe strong correlation (r=0.73) validates the use of ConceptNet embeddings as a measure of conceptual similarity. In the next script, we will load it in together with our effort features.\n\n\nReferences\n\n\n\n\nSpeer, Robyn, Joshua Chin, and Catherine Havasi. 2018. “ConceptNet 5.5: An Open Multilingual Graph of General Knowledge.” December 11, 2018. https://doi.org/10.48550/arXiv.1612.03975.",
    "crumbs": [
      "Analysis",
      "Computing concept similarity using ConceptNet word embeddings"
    ]
  },
  {
    "objectID": "08_Analysis_XGBoost/01_PCA_featureDimensions.html",
    "href": "08_Analysis_XGBoost/01_PCA_featureDimensions.html",
    "title": "Exploratory Analysis I: Using PCA to identify effort dimensions",
    "section": "",
    "text": "Overview\nIn this notebook, we will use Principal Component Analysis (PCA) to identify the most relevant planes (components) of effort among the features in the dataset(s) we created in the previous script. We do this paralel to eXtreme Gradient Boosting (XGBoost, see script here) because unlike PCA, XGBoost does not prevent from cummulating most relevant features that are correlated, i.e., they likely explain similar dimension of effort. To increase interpretative power of our analysis, we will combine these two methods to identify the most relevant features within the most relevant dimensions (i.e., components) of effort.\nNote that the current version of the script is used with data only from dyad 0. Since this is not sufficient amount of data for any meaningful conclusions, this script serves for building the workflow. We will use identical pipeline with the full dataset, and any deviations from this script will be reported.\n\n\nCode to prepare the environment\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\ncurfolder = os.getcwd()\n\n# This is where our features live\nfeatures = curfolder + '\\\\..\\\\07_TS_featureExtraction\\\\Datasets\\\\'\ndfs = glob.glob(features + '*.csv') \n\n\nBecause within the three distinct modalities - gesture, vocalization, combined - a set of different components could be decisive to characterize effort, we will perform PCA on each modality separately.\n\n# This is gesture data\nges = [x for x in dfs if 'gesture' in x]\ndata_ges = pd.read_csv(ges[0])\n\n# This is vocalization data\nvoc = [x for x in dfs if 'vocal' in x]\ndata_voc = pd.read_csv(voc[0])\n\n# This is multimodal data\nmulti = [x for x in dfs if 'combination' in x]\ndata_multi = pd.read_csv(multi[0])\n\n\n\nPCA: Gesture\nLet’s start by cleaning the dataframe. In gesture modality, some of the features are not relevant to the current analysis - those are mainly the ones that are related to acoustics and concept-related information. We will remove them from the dataframe before performing PCA.\n\n\nCustom functions\n# Function to clean the data\ndef clean_df(df, colstodel):\n\n    # Delete all desired columns\n    df = df.loc[:,~df.columns.str.contains('|'.join(colstodel))]\n\n    # Fill NaNs with 0\n    df = df.fillna(0)   # FLAGGED: this we might change, maybe not the best method (alternative: MICE)\n\n    # Save values from correction_info\n    correction_info = df['correction_info']\n\n    # Leave only numerical cols, except correction_info\n    df = df.select_dtypes(include=['float64','int64'])\n\n    # Add back correction_info\n    df['correction_info'] = correction_info\n    \n    return df\n\n\n\n# These are answer related columns\nconceptcols = ['answer', 'expressibility', 'response']\n\n# These are vocalization related columns\nvoccols = ['envelope', 'audio', 'f0', 'f1', 'f2', 'f3', 'env_', 'duration_voc', 'CoG']\n\n# Concatenate both lists\ncolstodel = conceptcols + voccols\n\n# Clean the df\nges_clean = clean_df(data_ges, colstodel)\n\nges_clean.head(15)\n\n\n\n\n\n\n\n\n\narm_duration\narm_inter_Kin\narm_inter_IK\narm_bbmv\nlowerbody_duration\nlowerbody_inter_Kin\nlowerbody_inter_IK\nlowerbody_bbmv\nleg_duration\nleg_inter_Kin\n...\narm_asymmetry\narm_moment_sum_pospeak_mean\narm_power_pospeak_std\npelvis_moment_sum_change_pospeak_std\npelvis_moment_sum_pospeak_std\narm_moment_sum_pospeak_std\nlowerbody_moment_sum_pospeak_std\nlowerbody_power_pospeak_std\nleg_moment_sum_change_pospeak_std\ncorrection_info\n\n\n\n\n0\n3816.0\n28.001\n29.471\n10.348\n3244.0\n27.815\n30.057\n9.607\n3244.0\n27.788\n...\n-2481.688\n-8.489\n1.030\n0.000\n0.646\n4.579\n0.303\n0.000\n0.000\nc0_only\n\n\n1\n5280.0\n28.098\n30.942\n13.350\n5862.0\n28.648\n31.235\n17.882\n5862.0\n29.740\n...\n-2803.359\n-0.430\n1.682\n0.213\n0.907\n1.201\n0.409\n0.000\n0.513\nc0_only\n\n\n2\n4302.0\n28.401\n31.164\n10.572\n4206.0\n28.373\n31.096\n8.402\n4206.0\n27.451\n...\n-6703.803\n-6.312\n0.348\n0.000\n1.403\n3.175\n1.227\n0.000\n0.000\nc0\n\n\n3\n4474.0\n27.316\n30.376\n10.229\n4398.0\n28.390\n32.196\n9.064\n4398.0\n27.665\n...\n-2827.356\n-5.724\n1.266\n0.000\n1.717\n5.321\n0.124\n0.000\n0.000\nc1\n\n\n4\n4388.0\n27.872\n31.225\n9.748\n3782.0\n27.458\n30.479\n8.428\n3782.0\n27.529\n...\n-3711.104\n-1.685\n0.000\n0.012\n1.045\n3.724\n0.270\n0.000\n0.000\nc2\n\n\n5\n5852.0\n29.417\n32.048\n10.675\n5404.0\n29.153\n30.545\n10.334\n5404.0\n29.056\n...\n-8208.505\n-8.680\n2.553\n0.018\n0.958\n2.077\n0.145\n0.000\n0.000\nc0_only\n\n\n6\n2736.0\n26.324\n27.656\n6.547\n0.0\n0.000\n0.000\n0.000\n0.0\n0.000\n...\n-3005.988\n0.645\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc0\n\n\n7\n4256.0\n28.675\n29.776\n8.148\n0.0\n0.000\n0.000\n0.000\n0.0\n0.000\n...\n-3988.156\n-5.168\n0.000\n0.000\n1.373\n8.906\n0.000\n0.000\n0.000\nc1\n\n\n8\n2516.0\n26.234\n27.815\n7.774\n884.0\n23.300\n25.890\n6.598\n884.0\n23.423\n...\n-6191.330\n-2.486\n0.000\n0.000\n0.909\n4.152\n0.000\n0.000\n0.000\nc0_only\n\n\n9\n4504.0\n28.604\n30.892\n9.939\n4266.0\n27.764\n29.475\n11.045\n4266.0\n28.404\n...\n-9892.382\n-5.280\n1.533\n0.000\n0.688\n1.350\n0.390\n0.356\n0.000\nc0\n\n\n10\n5946.0\n28.473\n30.925\n9.779\n5136.0\n28.712\n30.358\n9.084\n5136.0\n28.117\n...\n-8094.630\n-14.464\n0.000\n0.142\n0.512\n0.984\n0.865\n0.000\n0.039\nc1\n\n\n11\n6100.0\n29.300\n31.243\n10.521\n6030.0\n29.592\n30.960\n9.672\n6030.0\n30.037\n...\n-6914.443\n-4.261\n0.850\n0.000\n0.552\n0.636\n0.283\n0.000\n0.000\nc2\n\n\n12\n3004.0\n26.079\n26.368\n7.253\n2686.0\n28.219\n27.971\n4.870\n2686.0\n27.750\n...\n-1630.817\n0.000\n0.000\n0.000\n0.000\n0.000\n0.085\n0.000\n0.000\nc0_only\n\n\n13\n3918.0\n28.358\n28.936\n8.394\n3366.0\n28.669\n28.833\n5.086\n3366.0\n28.617\n...\n-2136.578\n-4.907\n0.184\n0.460\n0.851\n1.980\n0.855\n0.406\n1.368\nc0\n\n\n14\n3918.0\n28.120\n29.766\n9.343\n1692.0\n26.370\n26.244\n4.733\n1692.0\n24.816\n...\n-1165.345\n-11.533\n0.299\n0.202\n1.278\n2.726\n0.000\n0.140\n0.000\nc1\n\n\n\n\n15 rows × 325 columns\n\n\n\n\nNow, we first standardize the data and apply PCA to extract principal components. We use custom function PCA_biplot (adapted from here) to visualize the first two principal components. Data points are color-coded on the target variable (correction info) and red arrows represent the contributions of selected variables to the PC.\n\n\nCustom functions\n# Function to plot PCA results\ndef PCA_biplot(score, coeff, labels=None, selected_vars=None):\n    xs = score[:, 0]\n    ys = score[:, 1]\n    n = coeff.shape[0]\n    scalex = 1.0 / (xs.max() - xs.min())\n    scaley = 1.0 / (ys.max() - ys.min())\n\n    # Ensure all arrays have the same length\n    min_length = min(len(xs), len(ys), len(y))\n\n    # Trim all arrays to the smallest length\n    xs_trimmed = xs[:min_length]\n    ys_trimmed = ys[:min_length]\n    y_trimmed = y[:min_length]  # Adjust 'c' values to match\n\n    \n    plt.figure(figsize=(12, 8))  # Increase figure size\n    # Now plot safely\n    plt.scatter(xs_trimmed * scalex, ys_trimmed * scaley, c=y_trimmed, cmap=\"viridis\", alpha=0.7)\n    \n    # If selected_vars is provided, only plot these variables\n    if selected_vars is not None:\n        for i in selected_vars:\n            plt.arrow(0, 0, coeff[i, 0], coeff[i, 1], color='r', alpha=0.5)\n            if labels is None:\n                plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, \"Var\" + str(i + 1), \n                         color='g', ha='center', va='center', fontsize=9)\n            else:\n                plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, labels[i], \n                         color='g', ha='center', va='center', fontsize=9)\n    else:\n        for i in range(n):\n            plt.arrow(0, 0, coeff[i, 0], coeff[i, 1], color='r', alpha=0.5)\n            if labels is None:\n                plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, \"Var\" + str(i + 1), \n                         color='g', ha='center', va='center', fontsize=9)\n            else:\n                plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, labels[i], \n                         color='g', ha='center', va='center', fontsize=9)\n\n    # Zoom into the plot by narrowing the axis limits\n    plt.xlim(-0.5, 0.5)  # Adjust the range as needed\n    plt.ylim(-0.5, 0.5)  # Adjust the range as needed\n    \n    plt.xlabel(\"PC1\", fontsize=14)\n    plt.ylabel(\"PC2\", fontsize=14)\n    plt.grid()\n    plt.title(\"PCA Biplot\", fontsize=16)\n    plt.show()\n\n\n\n# Prepare data\nX = ges_clean.iloc[:, :-1].values  # All columns except the last as features\ny = ges_clean.iloc[:, -1].values   # Last column as target variable\n\n# Convert categorical target to numeric if necessary\nif y.dtype == 'object' or y.dtype.name == 'category':\n    le = LabelEncoder()\n    y = le.fit_transform(y)  # Converts categorical labels into numeric labels\n\n# Scale the data\nscaler = StandardScaler()\nX = scaler.fit_transform(X)    \n\n# PCA transformation\npca = PCA()\nx_new = pca.fit_transform(X)\n\n# For intelligibility, let's select only plot some variables\nselected_vars = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  \n\n# Call the function. Use only the 2 PCs.\nPCA_biplot(x_new[:, 0:2], np.transpose(pca.components_[0:2, :]), selected_vars=selected_vars)\n\n\n\n\n\n\n\n\nWhat amount of variances each PC explains?\n\n\narray([3.84511094e-01, 1.33517247e-01, 1.13255439e-01, 7.31860616e-02,\n       4.06776991e-02, 3.60648304e-02, 3.37781522e-02, 2.58262475e-02,\n       2.25049502e-02, 2.11170841e-02, 1.88455453e-02, 1.63285979e-02,\n       1.42102152e-02, 1.37325090e-02, 1.14525593e-02, 1.05874031e-02,\n       9.39941422e-03, 8.83676344e-03, 6.69106898e-03, 5.47711790e-03,\n       8.22273514e-33])\n\n\nSo we have really few data, therefore even the first principal component explains only 38% of the variance, second 13% and third 11%. But we will be focusing on the first three principal components, as they together explain at least 50% of the variance\nNow we can check the most important features. The larger the absolute value of the Eigenvalue, the more important the feature is for the principal component.\n\n\n[[5.38681564e-02 5.19802363e-02 6.69687848e-02 ... 3.65783055e-02\n  7.72739752e-03 1.33552992e-02]\n [4.86443290e-02 6.05250242e-02 6.33776285e-02 ... 3.84745526e-02\n  3.50366562e-02 9.39677427e-02]\n [1.18950177e-02 3.18850132e-02 1.07001005e-02 ... 8.76759978e-02\n  6.23252471e-02 3.70224151e-02]\n ...\n [6.91904948e-02 1.05956755e-01 5.99632890e-02 ... 2.86900649e-02\n  2.45013548e-02 1.30355450e-02]\n [8.55970837e-03 1.99497041e-02 6.52238928e-02 ... 7.38606597e-02\n  1.22245602e-01 2.97327756e-02]\n [2.14046104e-01 7.13652577e-01 1.35550979e-01 ... 5.87310753e-04\n  8.63642051e-03 2.48636333e-03]]\n\n\n\n# Number of principal components\nn_pcs = 3\n\n# Feature names (excluding target column)\nfeature_names = ges_clean.columns[:-1]  \n\n# Create storage for the ordered feature names and loadings\nresults_dict_ges = {}\n\nfor i in range(n_pcs):\n    # Get all features sorted by absolute loading values\n    sorted_indices = np.abs(pca.components_[i]).argsort()[::-1]\n    sorted_features = feature_names[sorted_indices]  # Feature names\n    sorted_loadings = pca.components_[i, sorted_indices]  # Loadings\n\n    # Store in dictionary\n    results_dict_ges[f'PC{i+1}'] = sorted_features.values\n    results_dict_ges[f'PC{i+1}_Loading'] = sorted_loadings\n\n# Convert dictionary to DataFrame\nresults_df_ges = pd.DataFrame(results_dict_ges)\nresults_df_ges.head(20)\n\n\n\n\n\n\n\n\n\nPC1\nPC1_Loading\nPC2\nPC2_Loading\nPC3\nPC3_Loading\n\n\n\n\n0\nbbmv_total\n0.088552\nleg_moment_sum_change_integral\n0.122350\nlowerbody_angAcc_sum_Gstd\n-0.113851\n\n\n1\nlowerbody_bbmv\n0.088311\nlowerbody_moment_sum_change_integral\n0.119478\npelvis_moment_sum_integral\n0.110988\n\n\n2\nleg_bbmv\n0.088311\nleg_moment_sum_range\n0.118334\nlowerbody_power_pospeak_n\n-0.110353\n\n\n3\nleg_angSpeed_sum_range\n0.086581\nleg_moment_sum_Gstd\n0.117526\nnumofArt\n0.106303\n\n\n4\narm_bbmv\n0.086062\nleg_moment_sum_pospeak_std\n0.117384\nlowerbody_power_range\n-0.105257\n\n\n5\nlowerbody_accKin_sum_pospeak_n\n0.085862\nleg_moment_sum_change_Gmean\n0.111512\nleg_angAcc_sum_integral\n0.104674\n\n\n6\nlowerbody_speedKin_sum_pospeak_n\n0.085862\nleg_moment_sum_change_pospeak_n\n0.110876\nlowerbody_power_Gstd\n-0.104521\n\n\n7\nleg_angSpeed_sum_Gstd\n0.084579\narm_angSpeed_sum_Gstd\n-0.106664\nlowerbody_angAcc_sum_range\n-0.104089\n\n\n8\nhead_speedKin_sum_pospeak_std\n0.083995\nspine_moment_sum_integral\n-0.106053\nspine_moment_sum_change_integral\n-0.103468\n\n\n9\nhead_accKin_sum_pospeak_std\n0.083995\nlowerbody_moment_sum_change_range\n0.105053\nleg_angJerk_sum_integral\n0.103358\n\n\n10\nhead_bbmv\n0.083770\narm_angSpeed_sum_range\n-0.104518\nspine_moment_sum_Gmean\n-0.103014\n\n\n11\nleg_angJerk_sum_range\n0.083486\nhead_angSpeed_sum_pospeak_std\n0.102664\nlowerbody_angSpeed_sum_Gstd\n-0.102256\n\n\n12\nleg_speedKin_sum_range\n0.083428\nhead_angAcc_sum_pospeak_std\n0.102664\nleg_angSpeed_sum_integral\n0.101966\n\n\n13\nhead_accKin_sum_range\n0.082507\nspine_moment_sum_Gmean\n-0.102477\nspine_moment_sum_integral\n-0.101497\n\n\n14\nhead_jerkKin_sum_integral\n0.082302\nspine_moment_sum_change_Gstd\n0.101442\npelvis_moment_sum_Gmean\n0.100055\n\n\n15\nleg_duration\n0.082105\nlowerbody_moment_sum_Gstd\n0.101287\nleg_angJerk_sum_pospeak_mean\n0.099828\n\n\n16\nlowerbody_duration\n0.082105\nlowerbody_moment_sum_change_Gstd\n0.101006\nlowerbody_angSpeed_sum_range\n-0.099430\n\n\n17\nhead_accKin_sum_Gstd\n0.082004\nlowerbody_moment_sum_range\n0.100636\nspine_moment_sum_change_Gmean\n-0.099217\n\n\n18\nlowerbody_accKin_sum_range\n0.081567\nlowerbody_moment_sum_change_Gmean\n0.100220\npelvis_moment_sum_change_Gstd\n-0.098610\n\n\n19\nleg_angAcc_sum_range\n0.081196\nhead_moment_sum_change_Gmean\n0.099953\nleg_angJerk_sum_Gmean\n0.097263\n\n\n\n\n\n\n\n\nNow we have dataframe for gesture modality where each column represents a principal component (PC1-PC3) and each row represents a feature. The values in the dataframe are the loadings of the features on the principal components. The loadings are the correlation coefficients between the features and the principal components. The higher the absolute value of the loading, the more important the feature is for the principal component. The dataframe is sorted by the absolute value of the loadings in descending order.\nSave the top contributors as a file so that we can load it in for the XGBoost analysis. We will also save the clean data which we can use for XGBoost modeling too.\n\n# Save top contributors\nresults_df_ges.to_csv(curfolder + '\\\\datasets\\\\PCA_top_contributors_ges.csv', index=False)\n\n# Save clean data\nges_clean.to_csv(curfolder + '\\\\datasets\\\\ges_clean_df.csv', index=False)\n\n\n\nPCA: Vocalizations\nIn the following repetitions, we will use custom function pca_analysis which does all the steps we performed previously for gesture modality in one go\n\n\nCustom PCA function\ndef pca_analysis(df_clean):\n    # Prepare data\n    X = df_clean.iloc[:, :-1].values  # All columns except the last as features\n    y = df_clean.iloc[:, -1].values   # Last column as target variable\n\n    # Convert categorical target to numeric if necessary\n    if y.dtype == 'object' or y.dtype.name == 'category':\n        le = LabelEncoder()\n        y = le.fit_transform(y)  # Converts categorical labels into numeric labels\n\n    # Scale the data\n    scaler = StandardScaler()\n    X = scaler.fit_transform(X)    \n\n    # PCA transformation\n    pca = PCA()\n    x_new = pca.fit_transform(X)\n\n    # Select few variables\n    selected_vars = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  \n\n    print('Biplot for the first 2 PCs:')\n    PCA_biplot(x_new[:, 0:2], np.transpose(pca.components_[0:2, :]), selected_vars=selected_vars)\n\n    PC1explained = pca.explained_variance_ratio_[0]*100\n    PC2explained = pca.explained_variance_ratio_[1]*100\n    PC3explained = pca.explained_variance_ratio_[2]*100\n\n    print('PCs explained variance:')\n    print(f'PC1: {PC1explained:.2f}%')\n    print(f'PC2: {PC2explained:.2f}%')\n    print(f'PC3: {PC3explained:.2f}%')\n\n    # Getting most contributing features\n    n_pcs = 3\n\n    # Feature names (excluding target column)\n    feature_names = df_clean.columns[:-1]  \n\n    # Create storage for the ordered feature names and loadings\n    results_dict = {}\n\n    for i in range(n_pcs):\n        # Get all features sorted by absolute loading values\n        sorted_indices = np.abs(pca.components_[i]).argsort()[::-1]\n        sorted_features = feature_names[sorted_indices]  # Feature names\n        sorted_loadings = pca.components_[i, sorted_indices]  # Loadings\n\n        # Store in dictionary\n        results_dict[f'PC{i+1}'] = sorted_features.values\n        results_dict[f'PC{i+1}_Loading'] = sorted_loadings\n\n    # Convert dictionary to DataFrame\n    results_df = pd.DataFrame(results_dict)\n\n    return results_df\n\n\nBefore PCA, we need to clean the data such that only vocalization-relevant features are kept.\n\n# These are answer related columns\nconceptcols = ['answer', 'expressibility', 'response']\n\n# These are vocalization related columns\nvoccols = ['envelope', 'audio', 'f0', 'f1', 'f2', 'f3', 'env_', 'duration_voc', 'CoG', 'correction_info']\n\n# Concatenate both lists\ncolstodel = conceptcols \n\n# Clean the df\nvoc_clean = clean_df(data_voc, colstodel)\n\n# Keep only those cols that have some in name - at least partially - words from voccols\ncolstokeep = [col for col in voc_clean.columns if any(word in col for word in voccols)]\n\n# Keep only those columns\nvoc_clean = voc_clean[colstokeep]\n\nvoc_clean.head(15)\n\n\n\n\n\n\n\n\n\nenvelope_Gmean\nenvelope_Gstd\nenvelope_pospeak_mean\nenvelope_pospeak_std\nenvelope_pospeak_n\nenvelope_integral\nenvelope_range\nenvelope_change_Gmean\nenvelope_change_Gstd\nenvelope_change_pospeak_mean\n...\nf2_clean_pospeak_std\nf1_clean_pospeak_std\nf1_clean_vel_pospeak_std\nCoG_Gmean\nCoG_Gstd\nCoG_pospeak_mean\nCoG_pospeak_std\nCoG_integral\nCoG_range\ncorrection_info\n\n\n\n\n0\n0.288\n1.438\n0.297\n2.393\n6.0\n337.541\n7.888\n0.016\n0.788\n0.042\n...\n0.000\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc0\n\n\n1\n0.805\n2.106\n-0.586\n0.231\n5.0\n1417.739\n7.888\n-0.258\n0.367\n-0.336\n...\n0.685\n0.11\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc1\n\n\n2\n0.281\n1.843\n-0.584\n0.196\n3.0\n357.523\n7.889\n-0.243\n0.505\n-0.542\n...\n0.000\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc2\n\n\n3\n0.446\n1.907\n-0.641\n0.118\n3.0\n359.095\n7.858\n-0.122\n0.714\n-0.572\n...\n0.000\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc0\n\n\n4\n0.804\n1.927\n0.003\n0.728\n7.0\n916.490\n7.888\n0.037\n0.890\n-0.266\n...\n1.624\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc1\n\n\n5\n0.779\n2.046\n1.788\n4.266\n3.0\n1116.787\n7.853\n-0.149\n0.674\n-0.237\n...\n0.000\n0.00\n1.777\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc2\n\n\n6\n0.317\n1.751\n-0.500\n0.001\n5.0\n421.491\n7.713\n-0.144\n0.857\n-0.571\n...\n0.000\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc0\n\n\n7\n2.471\n2.336\n-0.592\n0.000\n2.0\n781.091\n7.686\n0.463\n0.957\n-0.572\n...\n0.000\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc1\n\n\n8\n1.174\n2.096\n3.063\n5.167\n2.0\n766.878\n7.815\n-0.077\n0.772\n-0.572\n...\n0.000\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc2\n\n\n9\n0.263\n1.589\n-0.472\n0.430\n8.0\n447.570\n7.889\n0.098\n0.896\n-0.191\n...\n0.000\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc0\n\n\n10\n0.994\n1.856\n-0.492\n0.000\n8.0\n1208.357\n7.887\n0.481\n1.182\n-0.572\n...\n0.463\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc1\n\n\n11\n1.233\n2.069\n-0.435\n0.000\n6.0\n1144.241\n7.884\n0.564\n1.231\n-0.572\n...\n0.000\n0.00\n0.000\n-0.561\n1.464\n-0.270\n0.232\n-518.310\n5.346\nc2\n\n\n12\n0.000\n0.000\n0.000\n0.000\n0.0\n0.000\n0.000\n0.000\n0.000\n0.000\n...\n0.000\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc0_only\n\n\n13\n2.322\n2.804\n0.395\n1.654\n6.0\n2303.726\n7.888\n0.751\n1.304\n-0.238\n...\n1.347\n0.00\n0.005\n-3.272\n0.375\n-0.528\n2.491\n-3242.007\n2.236\nc0\n\n\n14\n3.156\n2.213\n2.712\n3.700\n4.0\n3206.970\n7.889\n-0.137\n0.681\n-0.466\n...\n0.372\n0.00\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\nc1\n\n\n\n\n15 rows × 71 columns\n\n\n\n\nNow we can use the function to perform the same PCA analysis but on vocal features\n\n# Perform PCA analysis\nresults_df_voc = pca_analysis(voc_clean)\n\n# Display\nresults_df_voc.head(20)\n\nBiplot for the first 2 PCs:\nPCs explained variance:\nPC1: 23.02%\nPC2: 15.73%\nPC3: 12.03%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC1_Loading\nPC2\nPC2_Loading\nPC3\nPC3_Loading\n\n\n\n\n0\nf3_clean_vel_pospeak_n\n-0.227090\nf3_clean_vel_pospeak_mean\n-0.249509\nCoG_integral\n-0.294560\n\n\n1\nf3_clean_pospeak_n\n-0.225100\nf0_Gstd\n0.245291\nCoG_pospeak_std\n0.279256\n\n\n2\nf2_clean_vel_pospeak_n\n-0.223553\nf0_pospeak_n\n0.244768\nCoG_pospeak_n\n0.274855\n\n\n3\nf2_clean_vel_range\n-0.209338\nf0_range\n0.240540\nenvelope_change_Gmean\n0.254844\n\n\n4\nf2_clean_Gstd\n-0.207581\nf3_clean_vel_pospeak_std\n0.205998\nCoG_Gmean\n-0.250294\n\n\n5\nf1_clean_vel_range\n-0.204405\nf3_clean_pospeak_mean\n0.202094\nCoG_range\n0.248384\n\n\n6\nf1_clean_range\n-0.203867\nenvelope_integral\n-0.172313\nf1_clean_pospeak_mean\n-0.226109\n\n\n7\nf2_clean_range\n-0.201218\nf2_clean_vel_pospeak_mean\n-0.169850\nenvelope_change_integral\n0.219129\n\n\n8\nf2_clean_pospeak_n\n-0.199212\nf1_clean_vel_integral\n-0.165138\nCoG_Gstd\n0.218615\n\n\n9\nVSA_f1f2\n-0.198116\nf2_clean_integral\n0.165138\nf2_clean_vel_pospeak_std\n0.212810\n\n\n10\nf3_clean_range\n-0.173889\nf3_clean_vel_integral\n-0.165138\nenvelope_change_Gstd\n0.203972\n\n\n11\nf1_clean_vel_pospeak_n\n-0.171451\nf2_clean_vel_integral\n-0.165138\nenvelope_Gstd\n0.170814\n\n\n12\nf2_clean_vel_Gstd\n-0.170596\nf1_clean_integral\n0.165138\nf2_clean_pospeak_mean\n-0.147427\n\n\n13\nf3_clean_vel_range\n-0.164741\nf3_clean_integral\n0.165138\nf2_clean_pospeak_std\n0.142625\n\n\n14\nf1_clean_pospeak_n\n-0.163461\nf0_Gmean\n-0.164620\nenvelope_Gmean\n0.140345\n\n\n15\nf1_clean_Gstd\n-0.160931\nenvelope_change_integral\n0.163419\nenvelope_change_pospeak_n\n0.131470\n\n\n16\nf1_clean_vel_Gstd\n-0.143258\nf1_clean_vel_pospeak_n\n0.163373\nf1_clean_pospeak_n\n-0.128745\n\n\n17\nenvelope_change_range\n-0.141037\nf3_clean_Gmean\n0.149983\nf1_clean_Gstd\n-0.127111\n\n\n18\nf3_clean_Gstd\n-0.138584\nenvelope_Gstd\n-0.148065\nf3_clean_pospeak_mean\n-0.126233\n\n\n19\nf3_clean_vel_pospeak_std\n-0.135056\nenvelope_Gmean\n-0.148007\nenvelope_change_range\n0.124729\n\n\n\n\n\n\n\n\nSave contributors as a file\n\n# Save top contributors\nresults_df_voc.to_csv(curfolder + '\\\\datasets\\\\PCA_top_contributors_voc.csv', index=False)\n\n# Save clean data\nvoc_clean.to_csv(curfolder + '\\\\datasets\\\\voc_clean_df.csv', index=False)\n\n\n\nPCA: Combined\nNow we do the same for combined condition\n\n# These are answer related columns\nconceptcols = ['answer', 'expressibility', 'response']\n\n# Concatenate both lists\ncolstodel = conceptcols \n\n# Clean the df\nmulti_clean = clean_df(data_multi, colstodel)\n\nmulti_clean.head(15)\n\n\n\n\n\n\n\n\n\narm_duration\narm_inter_Kin\narm_inter_IK\narm_bbmv\nlowerbody_duration\nlowerbody_inter_Kin\nlowerbody_inter_IK\nlowerbody_bbmv\nleg_duration\nleg_inter_Kin\n...\nf2_clean_pospeak_std\nf1_clean_pospeak_std\nf1_clean_vel_pospeak_std\nCoG_Gmean\nCoG_Gstd\nCoG_pospeak_mean\nCoG_pospeak_std\nCoG_integral\nCoG_range\ncorrection_info\n\n\n\n\n0\n3988.0\n26.197\n26.728\n8.472\n2020.0\n26.580\n26.181\n5.400\n2020.0\n26.759\n...\n0.000\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc0\n\n\n1\n3868.0\n26.674\n28.122\n8.148\n2870.0\n28.319\n28.291\n6.487\n2870.0\n27.096\n...\n0.000\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc1\n\n\n2\n4014.0\n26.449\n27.565\n8.465\n874.0\n23.434\n23.040\n2.861\n874.0\n22.945\n...\n0.000\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc2\n\n\n3\n4046.0\n26.207\n28.558\n8.482\n3750.0\n28.421\n28.796\n6.124\n3750.0\n27.973\n...\n0.000\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc0\n\n\n4\n4708.0\n26.502\n28.717\n8.309\n4508.0\n29.760\n28.762\n6.005\n4508.0\n28.679\n...\n0.000\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc1\n\n\n5\n4004.0\n26.978\n28.554\n8.699\n3598.0\n28.801\n28.616\n5.723\n3598.0\n28.179\n...\n0.000\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc2\n\n\n6\n0.0\n0.000\n0.000\n0.000\n784.0\n23.222\n22.753\n2.313\n784.0\n22.135\n...\n0.867\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc0\n\n\n7\n5248.0\n28.973\n31.658\n10.919\n4930.0\n28.905\n29.800\n14.893\n4930.0\n28.250\n...\n0.000\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc0\n\n\n8\n1784.0\n25.011\n24.190\n6.604\n0.0\n0.000\n0.000\n0.000\n0.0\n0.000\n...\n0.000\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc1\n\n\n9\n1284.0\n24.278\n24.516\n5.054\n1334.0\n25.381\n25.483\n4.317\n1334.0\n24.478\n...\n0.000\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc2\n\n\n10\n1494.0\n24.248\n24.502\n6.483\n2944.0\n29.247\n28.630\n5.378\n2944.0\n26.524\n...\n0.000\n0.018\n0.812\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc0_only\n\n\n11\n2486.0\n25.718\n26.035\n6.420\n780.0\n25.181\n21.982\n2.271\n780.0\n22.482\n...\n0.000\n0.000\n0.000\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc0_only\n\n\n12\n2968.0\n25.494\n28.758\n8.040\n2750.0\n28.377\n27.107\n4.885\n2750.0\n26.672\n...\n0.618\n0.000\n0.552\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc0_only\n\n\n13\n3850.0\n27.666\n27.348\n7.419\n3366.0\n28.208\n28.519\n5.033\n3366.0\n28.230\n...\n0.223\n0.000\n0.502\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc0\n\n\n14\n3236.0\n26.551\n27.446\n6.411\n0.0\n0.000\n0.000\n0.000\n0.0\n0.000\n...\n0.197\n0.429\n0.347\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nc1\n\n\n\n\n15 rows × 395 columns\n\n\n\n\nNow we can use the function to perform the same PCA analysis but all (i.e., both vocal and gestural) features\n\n# Perform PCA analysis\nresults_df_multi = pca_analysis(multi_clean)\n\n# Display\nresults_df_multi.head(20)\n\nBiplot for the first 2 PCs:\nPCs explained variance:\nPC1: 32.33%\nPC2: 12.86%\nPC3: 8.72%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPC1\nPC1_Loading\nPC2\nPC2_Loading\nPC3\nPC3_Loading\n\n\n\n\n0\nlowerbody_speedKin_sum_range\n0.084947\npelvis_moment_sum_change_pospeak_n\n-0.108598\npelvis_moment_sum_Gmean\n-0.131078\n\n\n1\nlowerbody_speedKin_sum_Gstd\n0.084128\nlowerbody_moment_sum_change_pospeak_n\n-0.102210\nleg_moment_sum_change_pospeak_std\n0.126453\n\n\n2\nlowerbody_accKin_sum_range\n0.083025\nenvelope_change_range\n-0.095835\npelvis_moment_sum_change_Gmean\n0.125537\n\n\n3\nleg_angAcc_sum_range\n0.082578\nleg_moment_sum_pospeak_n\n-0.095587\npelvis_moment_sum_integral\n-0.125378\n\n\n4\nbbmv_total\n0.082215\nleg_angAcc_sum_pospeak_mean\n0.095287\nleg_moment_sum_change_pospeak_mean\n0.123384\n\n\n5\nleg_angJerk_sum_range\n0.082004\nleg_angSpeed_sum_pospeak_mean\n0.095287\nlowerbody_jerkKin_sum_pospeak_mean\n0.120110\n\n\n6\nlowerbody_accKin_sum_Gstd\n0.081860\nlowerbody_moment_sum_change_pospeak_std\n-0.090847\nleg_moment_sum_change_pospeak_n\n0.114781\n\n\n7\nleg_angSpeed_sum_range\n0.081268\nleg_moment_sum_pospeak_std\n-0.090727\nlowerbody_moment_sum_change_Gmean\n0.114695\n\n\n8\nhead_angJerk_sum_range\n0.081148\nenvelope_change_pospeak_n\n-0.090655\nleg_moment_sum_change_Gmean\n0.114310\n\n\n9\nhead_angJerk_sum_Gstd\n0.080572\nf0_range\n-0.090308\npelvis_moment_sum_pospeak_mean\n-0.113300\n\n\n10\npelvis_moment_sum_range\n0.080076\nf1_clean_vel_pospeak_n\n-0.090298\npelvis_moment_sum_change_integral\n0.111752\n\n\n11\nlowerbody_speedKin_sum_integral\n0.079815\nf3_clean_vel_pospeak_n\n-0.089649\nf1_clean_vel_pospeak_std\n0.107322\n\n\n12\nleg_angAcc_sum_Gstd\n0.079559\nleg_moment_sum_Gstd\n-0.089473\nlowerbody_moment_sum_integral\n-0.105503\n\n\n13\nhead_angJerk_sum_Gmean\n0.079513\nhead_jerkKin_sum_pospeak_mean\n0.088606\nlowerbody_moment_sum_change_integral\n0.105260\n\n\n14\narm_angJerk_sum_integral\n0.079455\nf0_Gstd\n-0.088475\nlowerbody_angAcc_sum_Gstd\n0.104757\n\n\n15\nleg_power_integral\n0.079208\nspine_moment_sum_change_pospeak_n\n-0.087931\nlowerbody_moment_sum_Gmean\n-0.103744\n\n\n16\nleg_angJerk_sum_Gstd\n0.079082\nlowerbody_moment_sum_change_Gstd\n-0.087358\nlowerbody_jerkKin_sum_pospeak_std\n0.102416\n\n\n17\nleg_angSpeed_sum_Gstd\n0.078558\nleg_jerkKin_sum_pospeak_mean\n0.087297\nleg_moment_sum_change_integral\n0.102272\n\n\n18\nhead_accKin_sum_integral\n0.078307\nlowerbody_power_pospeak_n\n-0.087118\nlowerbody_angAcc_sum_range\n0.098288\n\n\n19\narm_angSpeed_sum_pospeak_std\n0.078266\nf2_clean_vel_pospeak_n\n-0.086946\nlowerbody_moment_sum_change_range\n0.095950\n\n\n\n\n\n\n\n\n\n# Save top contributors\nresults_df_multi.to_csv(curfolder + '\\\\datasets\\\\PCA_top_contributors_multi.csv', index=False)\n\n# Save clean data\nmulti_clean.to_csv(curfolder + '\\\\datasets\\\\multi_clean_df.csv', index=False)\n\nIn the XGBoost script, we will combine the ranking from Principal Component Analysis with the ranking of cummulative importance to get most predictive features of effort from uncorrelated dimensions.",
    "crumbs": [
      "Analysis",
      "Exploratory Analysis I: Using PCA to identify effort dimensions"
    ]
  }
]