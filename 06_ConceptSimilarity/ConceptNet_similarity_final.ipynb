{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Concept Similarity\n",
        "format:\n",
        "  html:\n",
        "    code-overflow: wrap\n",
        "    code-width: 1200\n",
        "---"
      ],
      "id": "327af379"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Our second hypothesis tests the effect of degree of misunderstanding on the magnitude of effort. \n",
        "\n",
        "We operationalize degree of misunderstanding as a conceptual similarity between target concept and answer offered by a guesser. \n",
        "\n",
        "To have a reproducible measure of conceptual similarity, we use the ConceptNet (@speer_etal18) to extract embeddings for concepts used in our study, and calculate cosine similarity between the target concept and guessed answer. \n",
        "\n",
        "To verify the utility of the cosine similarity, we have collected data from 14 Dutch-native peoplewho were asked to rate the similarity between each pair of words in online anonymous rating study. We then compare the 'perceived similarity' with cosine similarity computed from ConceptNet embeddings, to validate the use of ConceptNet embeddings as a measure of conceptual similarity."
      ],
      "id": "864e0e50"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: Code to load packages and prepare environment\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pingouin\n",
        "import openpyxl\n",
        "\n",
        "curfolder = os.getcwd()\n",
        "rawdata = curfolder + '\\\\..\\\\00_RAWDATA\\\\'\n",
        "answerfiles = glob.glob(rawdata + '*\\\\*.csv', recursive=True)\n",
        "datafolder = curfolder + '\\\\data\\\\'\n",
        "\n",
        "# Load all files that have '_1_results' in the name \n",
        "answerfiles_1 = [f for f in answerfiles if '_1_results' in f]\n",
        "# Loop over list and add it into one big df\n",
        "df_all1 = pd.DataFrame()\n",
        "for file in answerfiles_1:\n",
        "    df = pd.read_csv(file)\n",
        "    df_all1 = pd.concat([df_all1, df], ignore_index=True)\n",
        "\n",
        "df_all1['exp'] = 1 \n",
        "\n",
        "# Load all files that have '_2_results' in the name\n",
        "answerfiles_2 = [f for f in answerfiles if '_2_results' in f]\n",
        "# Loop over list and add it into one big df\n",
        "df_all2 = pd.DataFrame()\n",
        "for file in answerfiles_2:\n",
        "    df = pd.read_csv(file)\n",
        "    df_all2 = pd.concat([df_all2, df], ignore_index=True)\n",
        "\n",
        "df_all2['exp'] = 2\n",
        "\n",
        "# Merge\n",
        "df_all = pd.concat([df_all1, df_all2], ignore_index=True)\n",
        "\n",
        "# Keep only columns word and answer\n",
        "df = df_all[['word', 'answer', 'exp']]"
      ],
      "id": "38aef8be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we need to do some data-wrangling to get all in the right format for the embedding extraction and comparison"
      ],
      "id": "2c6d9733"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "# concept list\n",
        "df_concepts = pd.read_excel(rawdata + '/conceptlist_info.xlsx')\n",
        "\n",
        "# in df_concepts, keep only English and Dutch\n",
        "df_concepts = df_concepts[['English', 'Dutch']]\n",
        "\n",
        "# rename Dutch to word\n",
        "df_concepts = df_concepts.rename(columns={'Dutch': 'word'})\n",
        "\n",
        "# merge df and df_concepts on word\n",
        "df = pd.merge(df, df_concepts, on='word', how='left')\n",
        "\n",
        "# show rows where English is NaN\n",
        "df[df['English'].isnull()]\n",
        "\n",
        "# add translations manually for each (these are practice trials)\n",
        "df.loc[df['word'] == 'bloem', 'English'] = 'flower'\n",
        "df.loc[df['word'] == 'dansen', 'English'] = 'to dance'\n",
        "df.loc[df['word'] == 'auto', 'English'] = 'car'\n",
        "df.loc[df['word'] == 'olifant', 'English'] = 'elephant'\n",
        "df.loc[df['word'] == 'comfortabel', 'English'] = 'comfortable'\n",
        "df.loc[df['word'] == 'bal', 'English'] = 'ball'\n",
        "df.loc[df['word'] == 'haasten', 'English'] = 'to hurry'\n",
        "df.loc[df['word'] == 'gek', 'English'] = 'crazy'\n",
        "df.loc[df['word'] == 'snijden', 'English'] = 'to cut'\n",
        "df.loc[df['word'] == 'koken', 'English'] = 'to cook'\n",
        "df.loc[df['word'] == 'juichen', 'English'] = 'to cheer'\n",
        "df.loc[df['word'] == 'zingen', 'English'] = 'to sing'\n",
        "df.loc[df['word'] == 'glimlach', 'English'] = 'smile'\n",
        "df.loc[df['word'] == 'klok', 'English'] = 'clock'\n",
        "df.loc[df['word'] == 'fiets', 'English'] = 'bicycle'\n",
        "df.loc[df['word'] == 'vliegtuig', 'English'] = 'airplane'\n",
        "df.loc[df['word'] == 'geheim', 'English'] = 'secret'\n",
        "df.loc[df['word'] == 'telefoon', 'English'] = 'telephone'\n",
        "df.loc[df['word'] == 'zwaaien', 'English'] = 'to wave'\n",
        "df.loc[df['word'] == 'sneeuw', 'English'] = 'snow'\n",
        "\n",
        "# make a list of English answers\n",
        "answers_en = ['party', 'to cheer', 'tasty', 'to shoot', 'to breathe', 'zombie', 'bee', 'sea', 'dirty', 'tasty', 'car', 'to eat', 'to eat', 'to blow', 'hose', 'hose', 'to annoy', 'to make noise', 'to make noise', 'to run away', 'elephant', 'to cry', 'cold', 'outfit', 'silence', 'to ski', 'wrong', 'to play basketball', 'to search', 'disturbed', 'to run', 'to lick', 'to lift', 'lightning', 'to think', 'to jump', 'to fall', 'to write', 'to dance', 'shoulder height', 'horn', 'dirty', 'boring', 'to drink', 'strong', 'elderly', 'to mix', 'fish', 'fish', 'dirty', 'wrong', 'smart', 'to box', 'to box', 'dog', 'to catch', 'to cheer', 'to sing', 'pregnant', 'hair', 'to shower', 'pain', 'burnt', 'hot', 'I', 'to chew', 'bird', 'airplane', 'to fly', 'to think', 'to choose', 'to doubt', 'graffiti', 'fireworks', 'bomb', 'to smile', 'to laugh', 'smile', 'clock', 'to wonder', 'height', 'big', 'height', 'space', 'to misjudge', 'to wait', 'satisfied', 'happy', 'fish', 'to smell', 'wind', 'pain', 'to burn', 'hot', 'to cycle', 'to fly', 'airplane', 'bird', 'to crawl', 'to drink', 'waterfall', 'water', 'fire', 'top', 'good', 'to hear', 'to point', 'distance', 'there', 'to whisper', 'quiet', 'to be silent', 'telephone', 'to blow', 'to distribute', 'to give', 'cat', 'to laugh', 'tasty', 'to eat', 'yummy', 'to sleep', 'mountain', 'dirty', 'to vomit', 'to be disgusted', 'to greet', 'hello', 'goodbye', 'to smell', 'nose', 'odor', 'to fly', 'fireworks', 'to blow', 'to cut', 'pain', 'hot', 'to slurp', 'to throw', 'to fall', 'to fall', 'whistle', 'heartbeat', 'mouse', 'to hit', 'to catch', 'to grab', 'to throw', 'to fall', 'to shoot', 'circus', 'trunk', 'to fall', 'to fight', 'pain', 'to push open', 'to growl', 'to cut', 'to eat', 'knife', 'to slurp', 'to drink', 'drink', 'to eat', 'delicious', 'tasty', 'to cough', 'sick', 'to cry', 'to cry']\n",
        "\n",
        "# replace skien with skiën in the df\n",
        "df['answer'] = df['answer'].str.replace('skien', 'skiën')\n",
        "\n",
        "# get rid of English 'to beat'\n",
        "df_final = df[df['English'] != 'to beat']\n",
        "# and to weep\n",
        "df_final = df[df['English'] != 'to weep']\n",
        "# and noisy\n",
        "df_final = df[df['English'] != 'noisy']\n",
        "\n",
        "# add those to df as answers_en\n",
        "df['answer_en'] = answers_en\n",
        "\n",
        "# make a list of English targets\n",
        "meanings_en = list(df['English'])\n",
        "# Dutch targets\n",
        "meanings_nl = list(df['word'])\n",
        "# Dutch answers\n",
        "answers_nl = list(df['answer'])\n",
        "\n",
        "# Save it\n",
        "df.to_csv(datafolder + 'concept_answer_withoutcossim.csv', index=False)"
      ],
      "id": "19a7cee2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is how the dataframe looks like"
      ],
      "id": "7eebe584"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# load in\n",
        "df = pd.read_csv(datafolder + 'concept_answer_withoutcossim.csv')\n",
        "\n",
        "# display\n",
        "df.head(15)"
      ],
      "id": "d971b6e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculating cosine similarity\n",
        "\n",
        "Now we will load in [ConceptNet numberbatch (version 19.08)](https://github.com/commonsense/conceptnet-numberbatch) and compute cosine similarity for each pair"
      ],
      "id": "2e352ce6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: Custom functions\n",
        "\n",
        "# Load embeddings from a file\n",
        "def load_embeddings(file_path):\n",
        "    embeddings = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)"
      ],
      "id": "4e8ce2c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use multilingual numberbatch to extract words in the original language of experiment - Dutch. While English has better representation in ConceptNet, the English numberbatch does not make distinction between nouns and verbs (so 'a drink' and 'to drink' have common representation - drink). Because this is important distinction for us, we opt for Dutch embeddings to avoid this problem"
      ],
      "id": "5dfa4213"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# load embeddings\n",
        "embeddings = load_embeddings('numberbatch\\\\numberbatch.txt')"
      ],
      "id": "b5b9401b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is how a single concept is represented (here skiën, engl. skiing)"
      ],
      "id": "4dbad040"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "vec_nl = embeddings.get('/c/nl/skiën')\n",
        "print(vec_nl)"
      ],
      "id": "6251363f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we take the list of target-answer pairs, transform them into embedding format and perform cosine similarity."
      ],
      "id": "baef41fe"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "\n",
        "# get the embeddings for the words in the list meanings_en\n",
        "word_embeddings_t = {}\n",
        "for word in meanings_nl:\n",
        "    word_embed = '/c/nl/' + word\n",
        "    if word_embed in embeddings:\n",
        "        word_embeddings_t[word] = embeddings[word_embed]\n",
        "\n",
        "# get the embeddings for the words in the list answers_en\n",
        "word_embeddings_ans = {}\n",
        "for word in answers_nl:\n",
        "    word_embed = '/c/nl/' + word\n",
        "    if word_embed in embeddings:\n",
        "        word_embeddings_ans[word] = embeddings[word_embed]\n",
        "\n",
        "# calculate the similarity between the first word in the list meanings_en and first word in answers_en, second word in meanings_en and second word in answers_en, etc.\n",
        "cosine_similarities = []\n",
        "\n",
        "for i in range(len(meanings_nl)):\n",
        "    word1 = meanings_nl[i]\n",
        "    word2 = answers_nl[i]\n",
        "    vec1 = word_embeddings_t.get(word1)\n",
        "    vec2 = word_embeddings_ans.get(word2)\n",
        "    if vec1 is not None and vec2 is not None:\n",
        "        cosine_sim = cosine_similarity(vec1, vec2)\n",
        "        cosine_similarities.append(cosine_sim)\n",
        "    else:\n",
        "        # print which concepts could not be found\n",
        "        if vec1 is None:\n",
        "            print(f\"Concept not found: {word1}\")\n",
        "        if vec2 is None:\n",
        "            print(f\"Concept not found: {word2}\")\n",
        "        cosine_similarities.append(None)\n",
        "\n",
        "df['cosine_similarity'] = cosine_similarities\n",
        "\n",
        "# Save it\n",
        "df.to_csv(datafolder + 'conceptnet_clean.csv', index=False)"
      ],
      "id": "20967152",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# load in the df\n",
        "df = pd.read_csv(datafolder + 'conceptnet_clean.csv')\n",
        "\n",
        "# display\n",
        "df.head(20)"
      ],
      "id": "c6558c38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When running the code, we will see that some target or answered concepts are not represented in numberbatch (e.g., if the answer has more than one word). \n",
        "\n",
        "Because we verified that cosine similarity and perceived similarity are highly correlated (see below), we will collect the missing data through new online rating study.\n",
        "\n",
        "# Comparing cosine similarity against perceived similarity\n",
        "\n",
        "To validate the use of ConceptNet embeddings as a measure of conceptual similarity, we compare the cosine similarity computed from ConceptNet embeddings with the 'perceived similarity' ratings collected in the online anonymous rating study. \n",
        "\n",
        "The rating study has been introduced to the participants in a way that closely relates to the experiment. The instructions go as follows:\n",
        "\n",
        "*Below is a list of 171 pairs of words. \n",
        "\n",
        "Your task is to go through them and rate on the scale from 0 to 10 how similar they are/feel for you. \n",
        "\n",
        "You can for example imagine that you are playing a game where you need to explain the first word from the pair (e.g., to dance), and someone answers the second word in the pair. In such a situation, how close is the guesser from the intended word? If they answer 'to dance', then the two words are completely identical. But if they answer 'a car' it is not similar at all. \n",
        "\n",
        "Rate it according to your intuition, there is no incorrect answer.\n",
        "\n",
        "Note that the survey is completely anonymous and we are not collecting any of your personal data, only the ratings.*\n",
        "\n",
        "This is how the survey looks"
      ],
      "id": "6429b9d1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# load in excel similarity_en_survey\n",
        "df_survey = pd.read_excel(datafolder + '\\\\similarity_nl_survey.xlsx')\n",
        "\n",
        "# get rid of Timestamp column\n",
        "df_survey = df_survey.drop(columns='Timestamp')\n",
        "\n",
        "# display\n",
        "df_survey.head(15)"
      ],
      "id": "cf74432a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have to calculate mean rating for each pair"
      ],
      "id": "50bb6b27"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# for each column, calculate the mean and save it to a df\n",
        "df_survey_means = pd.DataFrame(df_survey.mean()).reset_index()\n",
        "\n",
        "# separate the index, the first part is English, the second part is the answer_en\n",
        "df_survey_means['word'] = df_survey_means['index'].str.split(' - ').str[0]\n",
        "df_survey_means['answer'] = df_survey_means['index'].str.split(' - ').str[1]\n",
        "\n",
        "# get rid of the index column\n",
        "df_survey_means = df_survey_means.drop(columns='index')\n",
        "\n",
        "# rename the column 0 to mean_similarity\n",
        "df_survey_means = df_survey_means.rename(columns={0: 'mean_similarity'})\n",
        "\n",
        "##### some corrections ####\n",
        "# get rid of all invisible spaces in answer\n",
        "df_survey_means['answer'] = df_survey_means['answer'].str.strip()\n",
        "# where word is vangen and answer vagen, change answer to vangen, and add similarity to 10\n",
        "df_survey_means.loc[(df_survey_means['word'] == 'vagen') & (df_survey_means['answer'] == 'vangen'), 'word'] = 'vangen'\n",
        "df_survey_means.loc[(df_survey_means['word'] == 'vangen') & (df_survey_means['answer'] == 'vangen'), 'mean_similarity'] = 10\n",
        "# where word is lopen and answer skien, change answer to skiën\n",
        "df_survey_means.loc[(df_survey_means['word'] == 'lopen') & (df_survey_means['answer'] == 'skien'), 'answer'] = 'skiën'\n",
        "# add one missing pair vallen-vallen with mean_similarity 10\n",
        "missing_row = pd.DataFrame({'word': ['vallen'], 'answer': ['vallen'], 'mean_similarity': [10]})\n",
        "df_survey_means = pd.concat([df_survey_means, missing_row], ignore_index=True)\n",
        "\n",
        "# display\n",
        "df_survey_means.head(15)"
      ],
      "id": "2c8ee472",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can merge it with the cosine similarity dataframe"
      ],
      "id": "9fe4a068"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# load in similarity\n",
        "df_similarity = pd.read_csv(datafolder + 'conceptnet_clean.csv')\n",
        "\n",
        "# merge df_survey_means with df on English and answer_en\n",
        "df_final = pd.merge(df_similarity, df_survey_means, on=['word', 'answer'], how='left')\n",
        "\n",
        "# get rid of English 'to beat'\n",
        "df_final = df_final[df_final['English'] != 'beat']\n",
        "# and to weep\n",
        "df_final = df_final[df_final['English'] != 'weep']\n",
        "\n",
        "# save it\n",
        "df_final.to_csv(datafolder + '/df_final_conceptnet.csv', index=False)\n",
        "\n",
        "# Display\n",
        "df_final.head(15)"
      ],
      "id": "4f0e233c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can finally run correlation"
      ],
      "id": "6846a370"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get rid of all lines where mean_similarity is 10.0 - otherwise we will drag the correlation up\n",
        "df_corr = df_final[df_final['mean_similarity'] != 10.0]\n",
        "\n",
        "feature1 = \"cosine_similarity\"\n",
        "feature2 = \"mean_similarity\"\n",
        "\n",
        "# create a sub-dataframe with the selected features, dropping missing values\n",
        "subdf = df_corr[[feature1, feature2]].dropna()\n",
        "\n",
        "# compute the correlation coefficient, with Bayes factor\n",
        "corr_with_bf = pingouin.pairwise_corr(subdf, columns=['cosine_similarity', 'mean_similarity'], method='pearson', alternative='two-sided')\n",
        "\n",
        "print(corr_with_bf)"
      ],
      "id": "b6e7bf7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here we see the relationship visually"
      ],
      "id": "36fe4592"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# create a joint plot with scatter and marginal histograms\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# scatter plot with histograms\n",
        "g = sns.jointplot(x=subdf[feature1], y=subdf[feature2], kind='reg', height=8,\n",
        "                  scatter_kws={'s': 50, 'alpha': 0.7}, marginal_kws=dict(bins=20, fill=True))\n",
        "\n",
        "g.fig.subplots_adjust(top=0.93)  # Adjust the title position\n",
        "\n",
        "# show plot\n",
        "plt.show()\n",
        "\n",
        "# save with high dpi\n",
        "plot_name = f\"{feature1}_vs_{feature2}_jointplot.png\"\n",
        "g.savefig(plot_name, dpi=300)"
      ],
      "id": "f97c4e4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The strong correlation (r=0.73) validates the use of ConceptNet embeddings as a measure of conceptual similarity. In the next script @ADDREF, we will load it in together with our effort features.\n"
      ],
      "id": "07d4c217"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ],
      "id": "974d2a32",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "tsprocess",
      "language": "python",
      "display_name": "TSprocess"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}