{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing XX: Merging multimodal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "\n",
    "curfolder = os.getcwd()\n",
    "print(curfolder)\n",
    "\n",
    "# folders with processed data\n",
    "MTfolder_processed = curfolder + '\\\\TS_motiontracking\\\\'\n",
    "ACfolder_processed = curfolder + '\\\\TS_acoustics\\\\'\n",
    "# folder to save merged data\n",
    "TSmerged = curfolder + '\\\\TS_merged\\\\'\n",
    "\n",
    "# prepare all files\n",
    "bbfiles = glob.glob(MTfolder_processed + '/bb*.csv')\n",
    "idfiles = glob.glob(MTfolder_processed + '/id*.csv')\n",
    "ikfiles = glob.glob(MTfolder_processed + '/ik*.csv')\n",
    "mtfiles = glob.glob(MTfolder_processed + '/mt*.csv')\n",
    "envfiles = glob.glob(ACfolder_processed + '/env*.csv')\n",
    "f0files = glob.glob(ACfolder_processed + '/f0*.csv')\n",
    "soundgen = glob.glob(ACfolder_processed + '/soundgen*.csv')\n",
    "formants = glob.glob(ACfolder_processed + '/praat_formants*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now merge all the data we have processed in the previous notebooks into a single dataframe per trial. These data include:\n",
    "\n",
    "- Balance Board data\n",
    "- Kinematics\n",
    "- Joint angles\n",
    "- Joint moments\n",
    "- Amplitude envelope\n",
    "- f0\n",
    "- formants\n",
    "- other acoustic features (from soundgen)\n",
    "\n",
    "Because each timeseries is sampling at different frequency, we opt for 500 Hz as the final sampling rate we will merge on. That means that we will interpolate all missing data (using linear interpolation) to match this frequency.\n",
    "\n",
    "Additionally, we will adapt the formants such that we only consider values that are present within a range of an amplitude peak, or where f0 is present, or both. These two situations can be considered as yielding in the most reliable formant values.\n",
    "\n",
    "Finally, we will also use inverse kinematics and dynamics to calculate power (as joint moment * joint velocity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_1_p0.csv\n",
      "IndexError: 0_1_1_p0not found\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_10_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_18_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_2_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_8_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_9_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_111_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_112_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_0_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_3_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_4_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_5_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_6_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_7_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_11_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_12_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_13_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_14_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_15_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_16_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_17_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_19_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_20_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_21_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_22_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_23_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_24_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_25_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_26_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_27_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_28_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_29_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_30_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_31_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_32_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_33_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_34_p1.csv\n",
      "IndexError: 0_1_34_p1not found\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_35_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_36_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_37_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_38_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_39_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_40_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_41_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_42_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_43_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_44_p0.csv\n",
      "IndexError: 0_1_44_p0not found\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_45_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_46_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_47_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_48_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_49_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_50_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_51_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_52_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_1_53_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_0_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_1_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_2_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_3_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_4_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_5_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_6_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_7_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_8_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_9_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_10_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_11_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_12_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_13_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_14_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_15_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_16_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_17_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_18_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_19_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_20_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_21_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_22_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_23_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_24_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_25_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_26_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_27_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_28_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_29_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_30_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_31_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_32_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_33_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_34_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_35_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_36_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_37_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_38_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_39_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_40_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_41_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_42_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_43_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_44_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_45_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_46_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_47_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_48_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_49_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_50_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_51_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_52_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_53_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_54_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_55_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_56_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_57_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_58_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_59_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_60_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_61_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_62_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_63_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_64_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_65_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_66_p1.csv\n",
      "IndexError: 0_2_66_p1not found\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_67_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_68_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_69_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_70_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_71_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_72_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_73_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_74_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_75_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_76_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_77_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_78_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_79_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_80_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_81_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_82_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_83_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_84_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_85_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_86_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_87_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_88_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_89_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_90_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_91_p0.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_92_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_93_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_94_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_95_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_96_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_97_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_98_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_99_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_100_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_101_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_102_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_103_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_104_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_105_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_106_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_107_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_108_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_109_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_110_p1.csv\n",
      "working on e:\\FLESH_ContinuousBodilyEffort\\03_TS_processing\\TS_motiontracking\\bb_0_2_113_p1.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>left_back</th>\n",
       "      <th>right_forward</th>\n",
       "      <th>right_back</th>\n",
       "      <th>left_forward</th>\n",
       "      <th>COPXc</th>\n",
       "      <th>COPYc</th>\n",
       "      <th>COPc</th>\n",
       "      <th>TrialID</th>\n",
       "      <th>FileInfo</th>\n",
       "      <th>...</th>\n",
       "      <th>f1_clean</th>\n",
       "      <th>f2_clean</th>\n",
       "      <th>f3_clean</th>\n",
       "      <th>f1_clean_vel</th>\n",
       "      <th>f2_clean_vel</th>\n",
       "      <th>f3_clean_vel</th>\n",
       "      <th>lowerbody_power</th>\n",
       "      <th>leg_power</th>\n",
       "      <th>head_power</th>\n",
       "      <th>arm_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.121945</td>\n",
       "      <td>0.812051</td>\n",
       "      <td>1.480789</td>\n",
       "      <td>1.403169</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.538925</td>\n",
       "      <td>31.086670</td>\n",
       "      <td>8.121212</td>\n",
       "      <td>29.250154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.122196</td>\n",
       "      <td>0.812064</td>\n",
       "      <td>1.480782</td>\n",
       "      <td>1.403227</td>\n",
       "      <td>-0.000150</td>\n",
       "      <td>-0.000100</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.963377</td>\n",
       "      <td>30.812275</td>\n",
       "      <td>8.039370</td>\n",
       "      <td>28.757306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.122379</td>\n",
       "      <td>0.812091</td>\n",
       "      <td>1.480792</td>\n",
       "      <td>1.403254</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>-0.000083</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.275597</td>\n",
       "      <td>30.502219</td>\n",
       "      <td>7.949268</td>\n",
       "      <td>28.210476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.122505</td>\n",
       "      <td>0.812128</td>\n",
       "      <td>1.480816</td>\n",
       "      <td>1.403257</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000072</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.483383</td>\n",
       "      <td>30.160837</td>\n",
       "      <td>7.852231</td>\n",
       "      <td>27.617322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.122583</td>\n",
       "      <td>0.812174</td>\n",
       "      <td>1.480855</td>\n",
       "      <td>1.403241</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>129.594537</td>\n",
       "      <td>29.792463</td>\n",
       "      <td>7.749587</td>\n",
       "      <td>26.985505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.122625</td>\n",
       "      <td>0.812226</td>\n",
       "      <td>1.480908</td>\n",
       "      <td>1.403212</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>128.616857</td>\n",
       "      <td>29.401433</td>\n",
       "      <td>7.642659</td>\n",
       "      <td>26.322683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.122637</td>\n",
       "      <td>0.812281</td>\n",
       "      <td>1.480972</td>\n",
       "      <td>1.403172</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127.558144</td>\n",
       "      <td>28.992082</td>\n",
       "      <td>7.532775</td>\n",
       "      <td>25.636516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.122628</td>\n",
       "      <td>0.812338</td>\n",
       "      <td>1.481048</td>\n",
       "      <td>1.403127</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126.426198</td>\n",
       "      <td>28.568744</td>\n",
       "      <td>7.421261</td>\n",
       "      <td>24.934663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.122604</td>\n",
       "      <td>0.812396</td>\n",
       "      <td>1.481133</td>\n",
       "      <td>1.403078</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125.235815</td>\n",
       "      <td>28.143369</td>\n",
       "      <td>7.312152</td>\n",
       "      <td>24.239301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.122571</td>\n",
       "      <td>0.812453</td>\n",
       "      <td>1.481227</td>\n",
       "      <td>1.403030</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.979754</td>\n",
       "      <td>27.710952</td>\n",
       "      <td>7.203799</td>\n",
       "      <td>23.541250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.122533</td>\n",
       "      <td>0.812507</td>\n",
       "      <td>1.481328</td>\n",
       "      <td>1.402983</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>122.776611</td>\n",
       "      <td>27.303682</td>\n",
       "      <td>7.103220</td>\n",
       "      <td>22.887729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1.122494</td>\n",
       "      <td>0.812559</td>\n",
       "      <td>1.481435</td>\n",
       "      <td>1.402939</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>-0.000082</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.457629</td>\n",
       "      <td>26.873201</td>\n",
       "      <td>6.999544</td>\n",
       "      <td>22.206614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.122458</td>\n",
       "      <td>0.812608</td>\n",
       "      <td>1.481546</td>\n",
       "      <td>1.402900</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.041580</td>\n",
       "      <td>26.426734</td>\n",
       "      <td>6.894443</td>\n",
       "      <td>21.509692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1.122427</td>\n",
       "      <td>0.812652</td>\n",
       "      <td>1.481660</td>\n",
       "      <td>1.402866</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118.552122</td>\n",
       "      <td>25.971776</td>\n",
       "      <td>6.789482</td>\n",
       "      <td>20.808573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1.122403</td>\n",
       "      <td>0.812691</td>\n",
       "      <td>1.481776</td>\n",
       "      <td>1.402838</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0_2_113_p1</td>\n",
       "      <td>p1_huilen_geluiden_c0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.017648</td>\n",
       "      <td>25.516077</td>\n",
       "      <td>6.686119</td>\n",
       "      <td>20.114685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows Ã— 593 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    time  left_back  right_forward  right_back  left_forward     COPXc  \\\n",
       "0    0.0   1.121945       0.812051    1.480789      1.403169 -0.000252   \n",
       "1    2.0   1.122196       0.812064    1.480782      1.403227 -0.000150   \n",
       "2    4.0   1.122379       0.812091    1.480792      1.403254 -0.000064   \n",
       "3    6.0   1.122505       0.812128    1.480816      1.403257  0.000007   \n",
       "4    8.0   1.122583       0.812174    1.480855      1.403241  0.000065   \n",
       "5   10.0   1.122625       0.812226    1.480908      1.403212  0.000111   \n",
       "6   12.0   1.122637       0.812281    1.480972      1.403172  0.000146   \n",
       "7   14.0   1.122628       0.812338    1.481048      1.403127  0.000172   \n",
       "8   16.0   1.122604       0.812396    1.481133      1.403078  0.000190   \n",
       "9   18.0   1.122571       0.812453    1.481227      1.403030  0.000200   \n",
       "10  20.0   1.122533       0.812507    1.481328      1.402983  0.000205   \n",
       "11  22.0   1.122494       0.812559    1.481435      1.402939  0.000205   \n",
       "12  24.0   1.122458       0.812608    1.481546      1.402900  0.000201   \n",
       "13  26.0   1.122427       0.812652    1.481660      1.402866  0.000193   \n",
       "15  28.0   1.122403       0.812691    1.481776      1.402838  0.000182   \n",
       "\n",
       "       COPYc      COPc     TrialID               FileInfo  ...  f1_clean  \\\n",
       "0  -0.000123  0.000280  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "1  -0.000100  0.000180  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "2  -0.000083  0.000105  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "3  -0.000072  0.000072  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "4  -0.000065  0.000092  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "5  -0.000062  0.000126  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "6  -0.000061  0.000158  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "7  -0.000063  0.000183  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "8  -0.000067  0.000201  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "9  -0.000071  0.000213  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "10 -0.000076  0.000219  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "11 -0.000082  0.000221  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "12 -0.000087  0.000219  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "13 -0.000092  0.000214  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "15 -0.000096  0.000206  0_2_113_p1  p1_huilen_geluiden_c0  ...       NaN   \n",
       "\n",
       "    f2_clean  f3_clean  f1_clean_vel  f2_clean_vel  f3_clean_vel  \\\n",
       "0        NaN       NaN           NaN           NaN           NaN   \n",
       "1        NaN       NaN           NaN           NaN           NaN   \n",
       "2        NaN       NaN           NaN           NaN           NaN   \n",
       "3        NaN       NaN           NaN           NaN           NaN   \n",
       "4        NaN       NaN           NaN           NaN           NaN   \n",
       "5        NaN       NaN           NaN           NaN           NaN   \n",
       "6        NaN       NaN           NaN           NaN           NaN   \n",
       "7        NaN       NaN           NaN           NaN           NaN   \n",
       "8        NaN       NaN           NaN           NaN           NaN   \n",
       "9        NaN       NaN           NaN           NaN           NaN   \n",
       "10       NaN       NaN           NaN           NaN           NaN   \n",
       "11       NaN       NaN           NaN           NaN           NaN   \n",
       "12       NaN       NaN           NaN           NaN           NaN   \n",
       "13       NaN       NaN           NaN           NaN           NaN   \n",
       "15       NaN       NaN           NaN           NaN           NaN   \n",
       "\n",
       "    lowerbody_power  leg_power  head_power  arm_power  \n",
       "0        132.538925  31.086670    8.121212  29.250154  \n",
       "1        131.963377  30.812275    8.039370  28.757306  \n",
       "2        131.275597  30.502219    7.949268  28.210476  \n",
       "3        130.483383  30.160837    7.852231  27.617322  \n",
       "4        129.594537  29.792463    7.749587  26.985505  \n",
       "5        128.616857  29.401433    7.642659  26.322683  \n",
       "6        127.558144  28.992082    7.532775  25.636516  \n",
       "7        126.426198  28.568744    7.421261  24.934663  \n",
       "8        125.235815  28.143369    7.312152  24.239301  \n",
       "9        123.979754  27.710952    7.203799  23.541250  \n",
       "10       122.776611  27.303682    7.103220  22.887729  \n",
       "11       121.457629  26.873201    6.999544  22.206614  \n",
       "12       120.041580  26.426734    6.894443  21.509692  \n",
       "13       118.552122  25.971776    6.789482  20.808573  \n",
       "15       117.017648  25.516077    6.686119  20.114685  \n",
       "\n",
       "[15 rows x 593 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_sr = 0.5    # this is the sr we are going to merge on (in Hz/sec)\n",
    "\n",
    "error_log = []\n",
    "\n",
    "for file in bbfiles:\n",
    "    print('working on ' + file)\n",
    "    bb_df = pd.read_csv(file)\n",
    "    # get trial id\n",
    "    trialid = bb_df['TrialID'][0]\n",
    "    # find the same trialid in idfiles\n",
    "    id_file = [x for x in idfiles if trialid in x]\n",
    "    try:\n",
    "        id_df = pd.read_csv(id_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        errormessage = 'IndexError: ' + trialid + 'not found for ID'\n",
    "        error_log.append(errormessage)\n",
    "        continue\n",
    "    \n",
    "    # find the same trialid in mtfiles\n",
    "    mt_file = [x for x in mtfiles if trialid in x]\n",
    "    try:\n",
    "        mt_df = pd.read_csv(mt_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        errormessage = 'IndexError: ' + trialid + 'not found for MT'\n",
    "        error_log.append(errormessage)\n",
    "        continue\n",
    "    # rename Time to time\n",
    "    mt_df.rename(columns={'Time': 'time'}, inplace=True)\n",
    "\n",
    "    # find the same trialid in envfiles\n",
    "    env_file = [x for x in envfiles if trialid in x]\n",
    "    try:\n",
    "        env_df = pd.read_csv(env_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        errormessage = 'IndexError: ' + trialid + 'not found for ENV'\n",
    "        error_log.append(errormessage)\n",
    "        continue\n",
    "    # rename trialID to TrialID\n",
    "    env_df.rename(columns={'trialID': 'TrialID'}, inplace=True)\n",
    "\n",
    "    # find the same trialid in f0files\n",
    "    f0_file = [x for x in f0files if trialid in x]\n",
    "    try:\n",
    "        f0_df = pd.read_csv(f0_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        errormessage = 'IndexError: ' + trialid + 'not found for F0'\n",
    "        error_log.append(errormessage)\n",
    "        continue\n",
    "\n",
    "    # rename time_ms to time\n",
    "    f0_df.rename(columns={'time_ms': 'time'}, inplace=True)\n",
    "    # rename ID to TrialID\n",
    "    f0_df.rename(columns={'ID': 'TrialID'}, inplace=True)\n",
    "\n",
    "    # find the same trialid in ikfiles\n",
    "    ik_file = [x for x in ikfiles if trialid in x]\n",
    "    try:\n",
    "        ik_df = pd.read_csv(ik_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        errormessage = 'IndexError: ' + trialid + 'not found for IK'\n",
    "        error_log.append(errormessage)\n",
    "        continue\n",
    "\n",
    "    # find the same trialid in soundgen\n",
    "    soundgen_file = [x for x in soundgen if trialid in x]\n",
    "    try:\n",
    "        soundgen_df = pd.read_csv(soundgen_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        errormessage = 'IndexError: ' + trialid + 'not found for soundgen'\n",
    "        error_log.append(errormessage)\n",
    "        continue\n",
    "    \n",
    "    # adapt the File column such that it has only last element separated by /           ##FLAGGED. DO THIS IN RMARKDOWN\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.split('/')[-1])\n",
    "    # get rid of .wav ending\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.split('.')[0])\n",
    "    # replace Mic_nominal_srate48000_ by ''\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.replace('Mic_nominal_srate48000_', ''))\n",
    "    # replace pr or trial by ''\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.replace('pr_', ''))\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.replace('trial_', ''))\n",
    "    # if there is corrected, replace by ''\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.replace('_corrected', ''))\n",
    "    # leave the first 4 elements\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: '_'.join(x.split('_')[:4]))\n",
    "\n",
    "    # rename the col to TrialID\n",
    "    soundgen_df.rename(columns={'File': 'TrialID'}, inplace=True)\n",
    "\n",
    "    # leave only columns harmEnergy, f1_freq, f2_freq, loudness, novelty, roughness\n",
    "    soundgen_df = soundgen_df[['time', 'TrialID', 'harmEnergy', 'loudness', 'novelty', 'roughness', 'flux']] ### FLAGGED PERHAPS COG add\n",
    "\n",
    "    # find the same trialid in formants\n",
    "    formants_file = [x for x in formants if trialid in x]\n",
    "    try:\n",
    "        formants_df = pd.read_csv(formants_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        errormessage = 'IndexError: ' + trialid + 'not found for formants'\n",
    "        error_log.append(errormessage)\n",
    "        continue\n",
    "\n",
    "    # rename triald to TrialID\n",
    "    formants_df.rename(columns={'trialid': 'TrialID'}, inplace=True)\n",
    "    formants_df = formants_df[['time', 'f1', 'f2', 'f3', 'TrialID']]\n",
    "    formants_df['time'] = formants_df['time'] * 1000\n",
    "\n",
    "    # write error log\n",
    "    with open(TSmerged + '/error_log.txt', 'w') as f:\n",
    "        for item in error_log:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "    ############## MERGING ########################\n",
    "\n",
    "    #### regularize sr in bb\n",
    "    time_new = np.arange(0, max(bb_df['time']), 1/desired_sr)\n",
    "    bb_interp = pd.DataFrame({'time': time_new})\n",
    "    \n",
    "    # interpolate all columns in samplebb \n",
    "    colstoint = bb_df.columns\n",
    "    colstoint = [x for x in colstoint if 'time' not in x]\n",
    "    colstoint = [x for x in colstoint if 'TrialID' not in x]\n",
    "    colstoint = [x for x in colstoint if 'FileInfo' not in x]\n",
    "\n",
    "    for col in colstoint:\n",
    "        bb_interp[col] = bb_df[col].interpolate(method='linear', x = bb_interp['time'])\n",
    "\n",
    "    # add trialid and time\n",
    "    bb_interp['TrialID'] = trialid\n",
    "    bb_interp['FileInfo'] = bb_df['FileInfo'][0]\n",
    "    \n",
    "    ########### merge the bb_interp with env\n",
    "    # merge the two dataframes\n",
    "    merge1 = pd.merge(bb_interp, env_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # interpolate missing values of envelope and audio\n",
    "    colstoint = merge1.columns\n",
    "    colstoint = [x for x in colstoint if 'audio' in x or 'envelope' in x]\n",
    "\n",
    "    for col in colstoint: \n",
    "        merge1[col] = merge1[col].interpolate(method='linear', x = merge1['time'])\n",
    "\n",
    "    # now we can kick out all vlaues where COPc is NaN\n",
    "    merge1 = merge1[~np.isnan(merge1['COPc'])]\n",
    "\n",
    "    ########### merge with ID\n",
    "    # merge the two dataframes\n",
    "    merge2 = pd.merge(merge1, id_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # get cols of sampleid\n",
    "    colstoint = id_df.columns\n",
    "    colstoint = [x for x in colstoint if 'time' not in x]\n",
    "    colstoint = [x for x in colstoint if 'TrialID' not in x]\n",
    "\n",
    "    # interpolate \n",
    "    for col in colstoint:\n",
    "        merge2[col] = merge2[col].interpolate(method='linear', x = merge2['time'])\n",
    "\n",
    "    # now we can kick out all values where COPc is NaN to get sampling rate back to 500\t\n",
    "    merge2 = merge2[~np.isnan(merge2['COPc'])]\n",
    "\n",
    "    ########### merge with MT\n",
    "    # merge the two dataframes\n",
    "    merge3 = pd.merge(merge2, mt_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # get cols of samplemt\n",
    "    colstoint = mt_df.columns\n",
    "    colstoint = [x for x in colstoint if 'time' not in x]\n",
    "    colstoint = [x for x in colstoint if 'TrialID' not in x]\n",
    "\n",
    "    # interpolate missing values of from mt\n",
    "    for col in colstoint:\n",
    "        merge3[col] = merge3[col].interpolate(method='linear', x = merge3['time'])\n",
    "\n",
    "    # now we can kick out all values where COPc is NaN\n",
    "    merge3 = merge3[~np.isnan(merge3['COPc'])]\n",
    "\n",
    "    ########### merge with F0\n",
    "    # for interpolation, we need to again parse f0 into chunks of non-NaN values\n",
    "    f0_df['chunk'] = None\n",
    "\n",
    "    # annotate chunks of non-NaN values\n",
    "    chunk = 0\n",
    "    for index, row in f0_df.iterrows():\n",
    "        if np.isnan(row['f0']):\n",
    "            continue\n",
    "        else:\n",
    "            f0_df.loc[index, 'chunk'] = chunk\n",
    "            # if the next value is NaN or this is the last row, increase the chunk\n",
    "            if index == len(f0_df)-1:\n",
    "                continue\n",
    "            elif np.isnan(f0_df.loc[index+1, 'f0']):\n",
    "                chunk += 1\n",
    "\n",
    "    chunks = f0_df['chunk'].unique()\n",
    "\n",
    "    # skip if chunks are empty (that means that there is no f0 trace)\n",
    "    if len(chunks) > 1:\n",
    "        # ignore the first chunk (None)\n",
    "        chunks = chunks[1:]\n",
    "\n",
    "    # now we can merge\n",
    "    merge4 = pd.merge(merge3, f0_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # we ignore the None chunk above, so if there is some trace, None should not be within chunks\n",
    "    if None not in chunks:\n",
    "        for chunk in chunks:\n",
    "            # get the first and last row of the chunk\n",
    "            firstrow = merge4[merge4['chunk'] == chunk].index[0]\n",
    "            lastrow = merge4[merge4['chunk'] == chunk].index[-1]\n",
    "            # fill all inbetween with the chunk number\n",
    "            merge4.loc[firstrow:lastrow, 'chunk'] = chunk\n",
    "            # get the rows of the chunk\n",
    "            chunkrows = merge4[merge4['chunk'] == chunk].copy()\n",
    "            # interpolate\n",
    "            chunkrows['f0'] = chunkrows['f0'].interpolate(method='linear', x = chunkrows['time'])\n",
    "            # put the interpolated chunk back to the merge4\n",
    "            merge4.loc[merge4['chunk'] == chunk, 'f0'] = chunkrows['f0']\n",
    "\n",
    "    # get rid of the chunk column\n",
    "    merge4.drop('chunk', axis=1, inplace=True)\n",
    "\n",
    "    # now we can drop all rows where COPc is NaN\n",
    "    merge4 = merge4[~np.isnan(merge4['COPc'])]\n",
    "\n",
    "    ########### merge with IK\n",
    "    merge5 = pd.merge(merge4, ik_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # get cols of sampleik\n",
    "    colstoint = ik_df.columns\n",
    "    colstoint = [x for x in colstoint if 'time' not in x]\n",
    "    colstoint = [x for x in colstoint if 'TrialID' not in x]\n",
    "\n",
    "    # interpolate missing values of from ik\n",
    "    for col in colstoint:\n",
    "        merge5[col] = merge5[col].interpolate(method='linear', x = merge5['time'])\n",
    "\n",
    "    # now we can kick out all values where COPc is NaN\n",
    "    merge5 = merge5[~np.isnan(merge5['COPc'])]\n",
    "\n",
    "    ########### merge with soundgen\n",
    "    # first we need to parse all cols on chunks\n",
    "\n",
    "    # cols to chunk\n",
    "    colstochunk = soundgen_df.columns\n",
    "    colstochunk = [x for x in colstochunk if 'TrialID' not in x]\n",
    "    colstochunk = [x for x in colstochunk if 'time' not in x]\n",
    "\n",
    "    for col in colstochunk:\n",
    "        soundgen_df[col + '_chunk'] = None\n",
    "\n",
    "        chunk = 0\n",
    "        for index, row in soundgen_df.iterrows():\n",
    "            if np.isnan(row[col]):\n",
    "                continue\n",
    "            else:\n",
    "                soundgen_df.loc[index, col + '_chunk'] = chunk\n",
    "                # if the next value is NaN or this is the last row, increase the chunk\n",
    "                if index == len(soundgen_df)-1:\n",
    "                    continue\n",
    "                elif np.isnan(soundgen_df.loc[index+1, col]):\n",
    "                    chunk += 1\n",
    "\n",
    "    # merge\n",
    "    merge6 = pd.merge(merge5, soundgen_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    for col in colstochunk:\n",
    "        # get the chunks and ignore nan and None\n",
    "        chunks = merge6[col + '_chunk'].dropna().unique()\n",
    " \n",
    "        # interpolate\n",
    "        for chunk in chunks:\n",
    "            # get the first and last row of the chunk\n",
    "            firstrow = merge6[merge6[col + '_chunk'] == chunk].index[0]\n",
    "            lastrow = merge6[merge6[col + '_chunk'] == chunk].index[-1]\n",
    "            # fill all inbetween with the chunk number\n",
    "            merge6.loc[firstrow:lastrow, col + '_chunk'] = chunk\n",
    "            # get the rows of the chunk\n",
    "            chunkrows = merge6[merge6[col + '_chunk'] == chunk].copy()\n",
    "            # interpolate , not putting any limits\n",
    "            chunkrows[col] = chunkrows[col].interpolate(method='linear', x = chunkrows['time'])\n",
    "            # put the interpolated chunk back to the merge6\n",
    "            merge6.loc[merge6[col + '_chunk'] == chunk, col] = chunkrows[col]\n",
    "\n",
    "\n",
    "    # get rid of the chunk columns\n",
    "    merge6 = merge6.drop([x for x in merge6.columns if 'chunk' in x], axis=1)\n",
    "\n",
    "    # now we can kick out all values where COPc is NaN\n",
    "    merge6 = merge6[~np.isnan(merge6['COPc'])]\n",
    "\n",
    "    ########### merge with formants\n",
    "    merge7 = pd.merge(merge6, formants_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # get cols of sampleformants\n",
    "    colstoint = formants_df.columns\n",
    "    colstoint = [x for x in colstoint if 'time' not in x]\n",
    "    colstoint = [x for x in colstoint if 'TrialID' not in x]\n",
    "\n",
    "    # interpolate missing values of from formants\n",
    "    for col in colstoint:\n",
    "        merge7[col] = merge7[col].interpolate(method='linear', x = merge7['time'])\n",
    "\n",
    "    # now we can kick out all values where COPc is NaN\n",
    "    merge7 = merge7[~np.isnan(merge7['COPc'])]\n",
    "\n",
    "    # this is final df\n",
    "    merge_final = merge7     \n",
    "\n",
    "\n",
    "    ############## FORMANT ADAPTATION ########################\n",
    "\n",
    "    # find peaks in envelope, with min=mean\n",
    "    peaks, _ = scipy.signal.find_peaks(merge_final['envelope'], height=np.mean(merge_final['envelope']))\n",
    "    # get widths of the peaks\n",
    "    widths = scipy.signal.peak_widths(merge_final['envelope'], peaks, rel_height=0.95)\n",
    "    # peak width df with starts and ends\n",
    "    peak_widths = pd.DataFrame({'start': widths[2], 'end': widths[3]})\n",
    "\n",
    "    # now create a new column env_weak_width, and put 0s everywhere, and 1s in the intervals of the width\n",
    "    merge_final['env_peak_width'] = 0\n",
    "    for index, row in peak_widths.iterrows():\n",
    "        merge_final.loc[int(row['start']):int(row['end']), 'env_peak_width'] = 1\n",
    "\n",
    "    # now we will create formant columns, where we will keep only formants in the intervals of env_pak_width OR where f0 is not NaN\n",
    "    merge_final['f1_clean_f0'] = merge_final['f1']\n",
    "    merge_final['f2_clean_f0'] = merge_final['f2']\n",
    "    merge_final['f3_clean_f0'] = merge_final['f3']\n",
    "\n",
    "    # where f0 is NaN, we will put NaN - these are formants during f0 only\n",
    "    merge_final.loc[np.isnan(merge_final['f0']), 'f1_clean_f0'] = np.nan\n",
    "    merge_final.loc[np.isnan(merge_final['f0']), 'f2_clean_f0'] = np.nan\n",
    "    merge_final.loc[np.isnan(merge_final['f0']), 'f3_clean_f0'] = np.nan\n",
    "\n",
    "    # we will also create formants, where we will keep only those in the intervals of env_pak_width\n",
    "    merge_final['f1_clean_env'] = merge_final['f1']\n",
    "    merge_final['f2_clean_env'] = merge_final['f2']\n",
    "    merge_final['f3_clean_env'] = merge_final['f3']\n",
    "\n",
    "    # where env_peak_width is 0, we will put NaN - these are formants during envelope peaks only\n",
    "    merge_final.loc[merge_final['env_peak_width'] == 0, 'f1_clean_env'] = np.nan\n",
    "    merge_final.loc[merge_final['env_peak_width'] == 0, 'f2_clean_env'] = np.nan\n",
    "    merge_final.loc[merge_final['env_peak_width'] == 0, 'f3_clean_env'] = np.nan\n",
    "\n",
    "    ## now we create formants where we copy values from clean_env and clean_f0\n",
    "    merge_final['f1_clean'] = merge_final['f1_clean_env']\n",
    "    merge_final['f2_clean'] = merge_final['f2_clean_env']\n",
    "    merge_final['f3_clean'] = merge_final['f3_clean_env']\n",
    "\n",
    "    # where formant is now NaN, copy values from f_clean_f0 in case there is a value\n",
    "    merge_final.loc[np.isnan(merge_final['f1_clean']), 'f1_clean'] = merge_final['f1_clean_f0']\n",
    "    merge_final.loc[np.isnan(merge_final['f2_clean']), 'f2_clean'] = merge_final['f2_clean_f0']\n",
    "    merge_final.loc[np.isnan(merge_final['f3_clean']), 'f3_clean'] = merge_final['f3_clean_f0']\n",
    "\n",
    "    # now calculate formant velocities (but only for the f_clean)\n",
    "    merge_final['f1_clean_vel'] = np.insert(np.diff(merge_final['f1_clean']), 0, 0)\n",
    "    merge_final['f2_clean_vel'] = np.insert(np.diff(merge_final['f2_clean']), 0, 0)\n",
    "    merge_final['f3_clean_vel'] = np.insert(np.diff(merge_final['f3_clean']), 0, 0)\n",
    "\n",
    "    # smooth\n",
    "    merge_final['f1_clean_vel'] = scipy.signal.savgol_filter(merge_final['f1_clean_vel'], 5, 3)\n",
    "    merge_final['f2_clean_vel'] = scipy.signal.savgol_filter(merge_final['f2_clean_vel'], 5, 3)\n",
    "    merge_final['f3_clean_vel'] = scipy.signal.savgol_filter(merge_final['f3_clean_vel'], 5, 3)\n",
    "\n",
    "    ########## POWER ####################\n",
    "    groups = ['lowerbody', 'leg', 'head', 'arm']\n",
    "\n",
    "    for group in groups:\n",
    "        # get all columns that contain group\n",
    "        cols = [x for x in merge_final.columns if group in x]\n",
    "\n",
    "        # get all columns that contain 'moment_sum'\n",
    "        torque = [x for x in cols if 'moment_sum' in x]\n",
    "        # but not change\n",
    "        torque = [x for x in torque if 'change' not in x][0]\n",
    "\n",
    "        # get all columns that contain 'angSpeed_sum'\n",
    "        angSpeed = [x for x in cols if 'angSpeed_sum' in x][0]\n",
    "\n",
    "        # get power which is moment * angSpeed\n",
    "        merge_final[group + '_power'] = merge_final[torque] * merge_final[angSpeed]\n",
    "        # smooth\n",
    "        merge_final[group + '_power'] = scipy.signal.savgol_filter(merge_final[group + '_power'], 15,3)\n",
    "    \n",
    "    #write to csv\n",
    "    merge_final.to_csv(TSmerged + '/merged_' + trialid + '.csv', index=False)  \n",
    "    #break\n",
    "\n",
    "merge_final.head(15)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### add here plot with all timeseries'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TSPROCESS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
