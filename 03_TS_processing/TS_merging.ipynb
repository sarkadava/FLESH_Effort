{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing - merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "\n",
    "curfolder = os.getcwd()\n",
    "print(curfolder)\n",
    "\n",
    "# files to work with\n",
    "MTfolder = 'C:\\\\Users\\\\kadava\\\\Documents\\\\Github\\\\FLESH_3Dtracking_new\\\\projectdata\\\\' ## FLAGGED CHANGE\n",
    "ACfolder = 'E:\\\\charade_experiment_WORKSPACE\\\\xdf_procedure\\\\data\\\\Data_processed\\\\Data_trials\\\\Audio_48\\\\' ## FLAGGED CHANGE\n",
    "BBfolder = 'E:\\\\charade_experiment_WORKSPACE\\\\xdf_procedure\\\\data\\\\Data_processed\\\\Data_trials\\\\'\n",
    "\n",
    "# folders to save the processed data\n",
    "MTfolder_processed = curfolder + '\\\\TS_motiontracking\\\\'\n",
    "ACfolder_processed = curfolder + '\\\\TS_acoustics\\\\'\n",
    "TSmerged = curfolder + '\\\\TS_merged\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "# prepare all files\n",
    "bbfiles = glob.glob(MTfolder_processed + '/bb*.csv')\n",
    "idfiles = glob.glob(MTfolder_processed + '/id*.csv')\n",
    "ikfiles = glob.glob(MTfolder_processed + '/ik*.csv')\n",
    "mtfiles = glob.glob(MTfolder_processed + '/mt*.csv')\n",
    "mtfiles = [x for x in mtfiles if 'centered' not in x]\n",
    "envfiles = glob.glob(ACfolder_processed + '/env*.csv')\n",
    "f0files = glob.glob(ACfolder_processed + '/f0*.csv')\n",
    "soundgen = glob.glob(ACfolder_processed + '/soundgen*.csv')\n",
    "formants = glob.glob(ACfolder_processed + '/praat_formants*.csv')\n",
    "\n",
    "desired_sr = 0.5    # this is the sr we are going to merge on (in Hz/sec)\n",
    "\n",
    "\n",
    "for file in bbfiles:\n",
    "    print('working on ' + file)\n",
    "    bb_df = pd.read_csv(file)\n",
    "    # get trial id\n",
    "    trialid = bb_df['TrialID'][0]\n",
    "    # find the same trialid in idfiles\n",
    "    id_file = [x for x in idfiles if trialid in x]\n",
    "    try:\n",
    "        id_df = pd.read_csv(id_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        continue\n",
    "    \n",
    "    # find the same trialid in mtfiles\n",
    "    mt_file = [x for x in mtfiles if trialid in x]\n",
    "    try:\n",
    "        mt_df = pd.read_csv(mt_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        continue\n",
    "    # rename Time to time\n",
    "    mt_df.rename(columns={'Time': 'time'}, inplace=True)\n",
    "\n",
    "    # find the same trialid in envfiles\n",
    "    env_file = [x for x in envfiles if trialid in x]\n",
    "    try:\n",
    "        env_df = pd.read_csv(env_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        continue\n",
    "    # rename trialID to TrialID\n",
    "    env_df.rename(columns={'trialID': 'TrialID'}, inplace=True)\n",
    "\n",
    "    # find the same trialid in f0files\n",
    "    f0_file = [x for x in f0files if trialid in x]\n",
    "    try:\n",
    "        f0_df = pd.read_csv(f0_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        continue\n",
    "\n",
    "    # rename time_ms to time\n",
    "    f0_df.rename(columns={'time_ms': 'time'}, inplace=True)\n",
    "    # rename ID to TrialID\n",
    "    f0_df.rename(columns={'ID': 'TrialID'}, inplace=True)\n",
    "\n",
    "    # find the same trialid in ikfiles\n",
    "    ik_file = [x for x in ikfiles if trialid in x]\n",
    "    try:\n",
    "        ik_df = pd.read_csv(ik_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        continue\n",
    "\n",
    "    # find the same trialid in soundgen\n",
    "    soundgen_file = [x for x in soundgen if trialid in x]\n",
    "    try:\n",
    "        soundgen_df = pd.read_csv(soundgen_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        continue\n",
    "    \n",
    "    # adapt the File column such that it has only last element separated by /\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.split('/')[-1])\n",
    "    # get rid of .wav ending\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.split('.')[0])\n",
    "    # replace Mic_nominal_srate48000_ by ''\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.replace('Mic_nominal_srate48000_', ''))\n",
    "    # replace pr or trial by ''\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.replace('pr_', ''))\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.replace('trial_', ''))\n",
    "    # if there is corrected, replace by ''\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: x.replace('_corrected', ''))\n",
    "    # leave the first 4 elements\n",
    "    soundgen_df['File'] = soundgen_df['File'].apply(lambda x: '_'.join(x.split('_')[:4]))\n",
    "\n",
    "    # rename the col to TrialID\n",
    "    soundgen_df.rename(columns={'File': 'TrialID'}, inplace=True)\n",
    "\n",
    "    # leave only columns harmEnergy, f1_freq, f2_freq, loudness, novelty, roughness\n",
    "    soundgen_df = soundgen_df[['time', 'TrialID', 'harmEnergy', 'loudness', 'novelty', 'roughness', 'flux']]\n",
    "\n",
    "    # find the same trialid in formants\n",
    "    formants_file = [x for x in formants if trialid in x]\n",
    "    try:\n",
    "        formants_df = pd.read_csv(formants_file[0])\n",
    "    except IndexError:\n",
    "        print('IndexError: ' + trialid + 'not found')\n",
    "        continue\n",
    "\n",
    "    # rename triald to TrialID\n",
    "    formants_df.rename(columns={'trialid': 'TrialID'}, inplace=True)\n",
    "\n",
    "    # keep only time, f1, f2, TrialID\n",
    "    formants_df = formants_df[['time', 'f1', 'f2', 'f3', 'TrialID']]\n",
    "\n",
    "    # convert time to ms\n",
    "    formants_df['time'] = formants_df['time'] * 1000\n",
    "\n",
    "    \n",
    "    ######################################\n",
    "\n",
    "    #### regularize sr in bb\n",
    "    time_new = np.arange(0, max(bb_df['time']), 1/desired_sr)\n",
    "    bb_interp = pd.DataFrame({'time': time_new})\n",
    "    \n",
    "    # interpolate all columns in samplebb except of the first and last\n",
    "    colstoint = bb_df.columns\n",
    "    # get rid of time and trialid\n",
    "    colstoint = [x for x in colstoint if 'time' not in x]\n",
    "    colstoint = [x for x in colstoint if 'TrialID' not in x]\n",
    "    colstoint = [x for x in colstoint if 'FileInfo' not in x]\n",
    "\n",
    "    for col in colstoint:\n",
    "        bb_interp[col] = bb_df[col].interpolate(method='linear', x = bb_interp['time'])\n",
    "\n",
    "    # add trialid and time\n",
    "    bb_interp['TrialID'] = trialid\n",
    "    bb_interp['FileInfo'] = bb_df['FileInfo'][0]\n",
    "    \n",
    "    ##### merge the bb_interp with env\n",
    "    # merge the two dataframes\n",
    "    merge1 = pd.merge(bb_interp, env_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # interpolate missing values of envelope and audio\n",
    "    colstoint = merge1.columns\n",
    "    colstoint = [x for x in colstoint if 'audio' in x or 'envelope' in x]\n",
    "\n",
    "    for col in colstoint: \n",
    "        merge1[col] = merge1[col].interpolate(method='linear', x = merge1['time'])\n",
    "\n",
    "    # now we can kick out all vlaues where COPc is NaN\n",
    "    merge1 = merge1[~np.isnan(merge1['COPc'])]\n",
    "\n",
    "    #### merge with ID\n",
    "    # merge the two dataframes\n",
    "    merge2 = pd.merge(merge1, id_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # get cols of sampleid\n",
    "    colstoint = id_df.columns\n",
    "    colstoint = [x for x in colstoint if 'time' not in x]\n",
    "    colstoint = [x for x in colstoint if 'TrialID' not in x]\n",
    "\n",
    "    # interpolate \n",
    "    for col in colstoint:\n",
    "        merge2[col] = merge2[col].interpolate(method='linear', x = merge2['time'])\n",
    "\n",
    "    # now we can kick out all values where COPc is NaN to get sampling rate back to 500\t\n",
    "    merge2 = merge2[~np.isnan(merge2['COPc'])]\n",
    "\n",
    "    #### merge with MT\n",
    "    # merge the two dataframes\n",
    "    merge3 = pd.merge(merge2, mt_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # get cols of samplemt\n",
    "    colstoint = mt_df.columns\n",
    "    colstoint = [x for x in colstoint if 'time' not in x]\n",
    "    colstoint = [x for x in colstoint if 'TrialID' not in x]\n",
    "\n",
    "    # interpolate missing values of from mt\n",
    "    for col in colstoint:\n",
    "        # interpolate\n",
    "        merge3[col] = merge3[col].interpolate(method='linear', x = merge3['time'])\n",
    "\n",
    "    # now we can kick out all values where COPc is NaN\n",
    "    merge3 = merge3[~np.isnan(merge3['COPc'])]\n",
    "\n",
    "    #### merge with F0\n",
    "    # for interpolation, we need to again parse f0 into chunks of non-NaN values\n",
    "    f0_df['chunk'] = None\n",
    "\n",
    "    # annotate chunks of non-NaN values\n",
    "    chunk = 0\n",
    "    for index, row in f0_df.iterrows():\n",
    "        if np.isnan(row['f0']):\n",
    "            continue\n",
    "        else:\n",
    "            f0_df.loc[index, 'chunk'] = chunk\n",
    "            # if the next value is NaN or this is the last row, increase the chunk\n",
    "            if index == len(f0_df)-1:\n",
    "                continue\n",
    "            elif np.isnan(f0_df.loc[index+1, 'f0']):\n",
    "                chunk += 1\n",
    "\n",
    "    chunks = f0_df['chunk'].unique()\n",
    "\n",
    "    # skip if chunks are empty (that means that there is no f0 trace)\n",
    "    if len(chunks) > 1:\n",
    "        # ignore the first chunk (None)\n",
    "        chunks = chunks[1:]\n",
    "\n",
    "    #now we can merge\n",
    "    merge4 = pd.merge(merge3, f0_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # we ignore the None chunk above, so if there is some trace, None should not be within chunks\n",
    "    if None not in chunks:\n",
    "        for chunk in chunks:\n",
    "            # get the first and last row of the chunk\n",
    "            firstrow = merge4[merge4['chunk'] == chunk].index[0]\n",
    "            lastrow = merge4[merge4['chunk'] == chunk].index[-1]\n",
    "            # fill all inbetween with the chunk number\n",
    "            merge4.loc[firstrow:lastrow, 'chunk'] = chunk\n",
    "            # get the rows of the chunk\n",
    "            chunkrows = merge4[merge4['chunk'] == chunk]\n",
    "            # interpolate\n",
    "            chunkrows['f0'] = chunkrows['f0'].interpolate(method='linear', x = chunkrows['time'])\n",
    "            # put the interpolated chunk back to the merge4\n",
    "            merge4.loc[merge4['chunk'] == chunk, 'f0'] = chunkrows['f0']\n",
    "\n",
    "    # get rid of the chunk column\n",
    "    merge4.drop('chunk', axis=1, inplace=True)\n",
    "\n",
    "    # now we can drop all rows where COPc is NaN\n",
    "    merge4 = merge4[~np.isnan(merge4['COPc'])]\n",
    "\n",
    "    ### merge with IK\n",
    "    merge5 = pd.merge(merge4, ik_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # get cols of sampleik\n",
    "    colstoint = ik_df.columns\n",
    "    colstoint = [x for x in colstoint if 'time' not in x]\n",
    "    colstoint = [x for x in colstoint if 'TrialID' not in x]\n",
    "\n",
    "    # interpolate missing values of from ik\n",
    "    for col in colstoint:\n",
    "        # interpolate\n",
    "        merge5[col] = merge5[col].interpolate(method='linear', x = merge5['time'])\n",
    "\n",
    "    # now we can kick out all values where COPc is NaN\n",
    "    merge5 = merge5[~np.isnan(merge5['COPc'])]\n",
    "\n",
    "    ### merge with soundgen\n",
    "    # first we need to parse all cols on chunks\n",
    "\n",
    "    # cols to chunk\n",
    "    colstochunk = soundgen_df.columns\n",
    "    colstochunk = [x for x in colstochunk if 'TrialID' not in x]\n",
    "    colstochunk = [x for x in colstochunk if 'time' not in x]\n",
    "\n",
    "    for col in colstochunk:\n",
    "        soundgen_df[col + '_chunk'] = None\n",
    "\n",
    "        chunk = 0\n",
    "        for index, row in soundgen_df.iterrows():\n",
    "            if np.isnan(row[col]):\n",
    "                continue\n",
    "            else:\n",
    "                soundgen_df.loc[index, col + '_chunk'] = chunk\n",
    "                # if the next value is NaN or this is the last row, increase the chunk\n",
    "                if index == len(soundgen_df)-1:\n",
    "                    continue\n",
    "                elif np.isnan(soundgen_df.loc[index+1, col]):\n",
    "                    chunk += 1\n",
    "\n",
    "    # merge\n",
    "    merge6 = pd.merge(merge5, soundgen_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    for col in colstochunk:\n",
    "        # get the chunks\n",
    "        chunks = merge6[col + '_chunk'].unique()\n",
    "        # ignore chunk nan and None\n",
    "        chunks = [x for x in chunks if x is not None]\n",
    "        chunks = [x for x in chunks if x is not np.nan]\n",
    " \n",
    "        # interpolate\n",
    "        for chunk in chunks:\n",
    "            # get the first and last row of the chunk\n",
    "            firstrow = merge6[merge6[col + '_chunk'] == chunk].index[0]\n",
    "            lastrow = merge6[merge6[col + '_chunk'] == chunk].index[-1]\n",
    "            # fill all inbetween with the chunk number\n",
    "            merge6.loc[firstrow:lastrow, col + '_chunk'] = chunk\n",
    "            # get the rows of the chunk\n",
    "            chunkrows = merge6[merge6[col + '_chunk'] == chunk]\n",
    "            # interpolate , not putting any limits\n",
    "            chunkrows[col] = chunkrows[col].interpolate(method='linear', x = chunkrows['time'])\n",
    "            # put the interpolated chunk back to the merge6\n",
    "            merge6.loc[merge6[col + '_chunk'] == chunk, col] = chunkrows[col]\n",
    "\n",
    "\n",
    "    # get rid of the chunk columns\n",
    "    merge6 = merge6.drop([x for x in merge6.columns if 'chunk' in x], axis=1)\n",
    "\n",
    "    # now we can kick out all values where COPc is NaN\n",
    "    merge6 = merge6[~np.isnan(merge6['COPc'])]\n",
    "\n",
    "    ### merge with formants\n",
    "    merge7 = pd.merge(merge6, formants_df, on=['time', 'TrialID'], how='outer')\n",
    "\n",
    "    # get cols of sampleformants\n",
    "    colstoint = formants_df.columns\n",
    "    colstoint = [x for x in colstoint if 'time' not in x]\n",
    "    colstoint = [x for x in colstoint if 'TrialID' not in x]\n",
    "\n",
    "    # interpolate missing values of from formants\n",
    "    for col in colstoint:\n",
    "        # interpolate\n",
    "        merge7[col] = merge7[col].interpolate(method='linear', x = merge7['time'])\n",
    "\n",
    "    # now we can kick out all values where COPc is NaN\n",
    "    merge7 = merge7[~np.isnan(merge7['COPc'])]\n",
    "\n",
    "    # this is final df\n",
    "    merge_final = merge7    \n",
    "\n",
    "    ##### now we need to keep keep formants only in some intervals\n",
    "    ### for that we first need to find out, where are peak widths for envelope peaks\n",
    "    # find peaks in envelope, with min=mean\n",
    "    peaks, _ = scipy.signal.find_peaks(merge_final['envelope'], height=np.mean(merge_final['envelope']))\n",
    "    # get widths of the peaks\n",
    "    widths = scipy.signal.peak_widths(merge_final['envelope'], peaks, rel_height=0.95)\n",
    "    # peak width df with starts and ends\n",
    "    peak_widths = pd.DataFrame({'start': widths[2], 'end': widths[3]})\n",
    "\n",
    "    # now create a new column env_weak_width, and put 0s everywhere, and 1s in the intervals of the width\n",
    "    merge_final['env_peak_width'] = 0\n",
    "    for index, row in peak_widths.iterrows():\n",
    "        merge_final.loc[int(row['start']):int(row['end']), 'env_peak_width'] = 1\n",
    "\n",
    "    #### now we will create f1_clean and f2_clean, where we will keep only formants in the intervals of env_pak_width OR where f0 is not NaN\n",
    "    merge_final['f1_clean_f0'] = merge_final['f1']\n",
    "    merge_final['f2_clean_f0'] = merge_final['f2']\n",
    "    merge_final['f3_clean_f0'] = merge_final['f3']\n",
    "\n",
    "    # where f0 is NaN, we will put NaN\n",
    "    merge_final.loc[np.isnan(merge_final['f0']), 'f1_clean_f0'] = np.nan\n",
    "    merge_final.loc[np.isnan(merge_final['f0']), 'f2_clean_f0'] = np.nan\n",
    "    merge_final.loc[np.isnan(merge_final['f0']), 'f3_clean_f0'] = np.nan\n",
    "\n",
    "    #### we will also create f1_clean_env and f2_clean_env, where we will keep only formants in the intervals of env_pak_width\n",
    "    merge_final['f1_clean_env'] = merge_final['f1']\n",
    "    merge_final['f2_clean_env'] = merge_final['f2']\n",
    "    merge_final['f3_clean_env'] = merge_final['f3']\n",
    "\n",
    "    # where env_peak_width is 0, we will put NaN\n",
    "    merge_final.loc[merge_final['env_peak_width'] == 0, 'f1_clean_env'] = np.nan\n",
    "    merge_final.loc[merge_final['env_peak_width'] == 0, 'f2_clean_env'] = np.nan\n",
    "    merge_final.loc[merge_final['env_peak_width'] == 0, 'f3_clean_env'] = np.nan\n",
    "\n",
    "    ## now create f1_clean and f2_clean, where we copy values from clean_env and clean_f0\n",
    "    merge_final['f1_clean'] = merge_final['f1_clean_env']\n",
    "    merge_final['f2_clean'] = merge_final['f2_clean_env']\n",
    "    merge_final['f3_clean'] = merge_final['f3_clean_env']\n",
    "\n",
    "    # copy also f1_clean_f0 and f2_clean_f0\n",
    "    merge_final.loc[np.isnan(merge_final['f1_clean']), 'f1_clean'] = merge_final['f1_clean_f0']\n",
    "    merge_final.loc[np.isnan(merge_final['f2_clean']), 'f2_clean'] = merge_final['f2_clean_f0']\n",
    "    merge_final.loc[np.isnan(merge_final['f3_clean']), 'f3_clean'] = merge_final['f3_clean_f0']\n",
    "\n",
    "    # now for f1_clean and f2_clean, calculate velocity (and smooth\n",
    "    merge_final['f1_clean_vel'] = np.insert(np.diff(merge_final['f1_clean']), 0, 0)\n",
    "    merge_final['f2_clean_vel'] = np.insert(np.diff(merge_final['f2_clean']), 0, 0)\n",
    "    merge_final['f3_clean_vel'] = np.insert(np.diff(merge_final['f3_clean']), 0, 0)\n",
    "\n",
    "    # smooth\n",
    "    merge_final['f1_clean_vel'] = scipy.signal.savgol_filter(merge_final['f1_clean_vel'], 5, 3)\n",
    "    merge_final['f2_clean_vel'] = scipy.signal.savgol_filter(merge_final['f2_clean_vel'], 5, 3)\n",
    "    merge_final['f3_clean_vel'] = scipy.signal.savgol_filter(merge_final['f3_clean_vel'], 5, 3)\n",
    "\n",
    "    ####### calculate power\n",
    "    # get all columns that contain 'torque_sum'\n",
    "    torquestosum = [x for x in merge_final.columns if 'torque_sum' in x]\n",
    "    # but not change\n",
    "    torquestosum = [x for x in torquestosum if 'change' not in x]\n",
    "\n",
    "    # get all columns that contain 'angSpeed_sum'\n",
    "    angSpeedtosum = [x for x in merge_final.columns if 'angSpeed_sum' in x]\n",
    "\n",
    "    groups = ['lowerbody', 'leg', 'head', 'arm']\n",
    "\n",
    "    for group in groups:\n",
    "        # get all columns that contain group\n",
    "        cols = [x for x in merge_final.columns if group in x]\n",
    "\n",
    "        # get all columns that contain 'torque_sum'\n",
    "        torque = [x for x in cols if 'torque_sum' in x]\n",
    "        # but not change\n",
    "        torque = [x for x in torquestosum if 'change' not in x][0]\n",
    "\n",
    "        # get all columns that contain 'angSpeed_sum'\n",
    "        angSpeed = [x for x in cols if 'angSpeed_sum' in x][0]\n",
    "\n",
    "        # get power which is torque * angSpeed\n",
    "        merge_final[group + '_power'] = merge_final[torque] * merge_final[angSpeed]\n",
    "        # smooth\n",
    "        merge_final[group + '_power'] = scipy.signal.savgol_filter(merge_final[group + '_power'], 15,3)\n",
    "    \n",
    "    #print(merge_final)\n",
    "    #break\n",
    "    #write to csv\n",
    "    merge_final.to_csv(TSmerged + '/merged_' + trialid + '.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergedfiles\n",
    "mergedfiles = glob.glob(TSmerged + '/merged*.csv')\n",
    "\n",
    "# load random file\n",
    "samplemerged = pd.read_csv(mergedfiles[20])\n",
    "samplemerged['TrialID']\n",
    "\n",
    "# plot to check\n",
    "plt.plot(samplemerged['time'], samplemerged['f2_clean'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
