---
title: 'Indicators of effort: XGBoost'
author: "Aleksandra Ä†wiek (original code), Sarka Kadava (adaptation)"
output: html_document
date: "2024-08-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data preparation

## Source setup

```{r source setup, echo = TRUE, message=FALSE, warning = FALSE}

########## folders ##########
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())

datasets      <- paste0(parentfolder, '/08_Analysis_XGBoost/datasets/')
models        <- paste0(parentfolder, '/08_Analysis_XGBoost/models/')
plots         <- paste0(parentfolder, '/08_Analysis_XGBoost/plots/')

########## source file ##########

#source(paste0(scripts, "adjectives-preparation.R"))

#################### packages ####################
# Data Manipulation
library(tibble)
library(stringr)
library(tidyverse) # includes readr, tidyr, dplyr, ggplot2
library(data.table)

# Plotting
library(ggforce)
library(ggpubr)
library(gridExtra)

# Random Forests and XGBoost
library(rpart)
library(rpart.plot)
library(ranger)
library(tuneRanger)
library(caret)
library(xgboost)
library(parallel)
library(mice)
library(doParallel)
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())

colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                       "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

```

In the previous script, we have already separated trials into modalities and clean the resulting dataframes of superfluous columns so now we can directly load them and continue modelling.


#######################
# Gesture #############
#######################

First, for gesture modality, we need to remove columns with features that are not present in gesture modality overal. These are all vocal features. They should be anyway mostly filled with NaNs as in gesture modality, any vocalizations were 'forbidden'. Additionally, we need to make sure that df does not contain any NAs. 

```{r}

data_ges <- read_csv(paste0(datasets, "ges_clean_df.csv"))

```

## Random forests

```{r}

# prepare predictors
predictors <- setdiff(names(data_ges), "correction_info")

formula_str <- paste("correction_info ~", paste(predictors, collapse = " + "))

# Convert the formula string to a formula object
gesTree_formula <- as.formula(formula_str)

# Now use the formula in rpart
gesTree <- rpart(formula = gesTree_formula, data = data_ges, 
                method='class', # Specify that it's a classification tree
                control = rpart.control(maxdepth = 5)  # Control parameters for the 'rpart' function
)

prp(
  gesTree,         # The decision tree object to be visualized
  extra = 1,      # Show extra information (like node statistics) in the plot
  varlen = 0,     # Length of variable names (0 means auto-determined)
  faclen = 0     # Length of factor levels displayed on the plot (increase as needed)
)
```

We will build a random forest first.

```{r set seed, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(998) # Set a seed for reproducibility
```

Split the data
```{r}


# Split the data into training and testing subsets
sample_indices <- sample(1:nrow(data_ges), 0.8*nrow(data_ges)) # 80% training, 20% testing
train_data <- data_ges[sample_indices, ]
test_data <- data_ges[-sample_indices, ]

```

Building the untuned model.

```{r untuned ges, echo=TRUE, message=FALSE, warning=FALSE}
# Untuned Model with importance (permutation) option set
gesUntuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:163],
  num.trees = 500,
  importance = "permutation"
)

predictions <- predict(gesUntuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(gesUntuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)
```

Set the parameters for the random forest.

```{r settings for RF ges, echo=TRUE, message=FALSE, warning=FALSE}
# Define the number of CPU cores to use
num_cores <- detectCores()

# Create a cluster with specified number of cores
cl <- makeCluster(num_cores)
```

Tuning the random forest.

```{r tuning RF ges, message=FALSE, warning=FALSE}
tuneGes <- makeClassifTask(data = data_ges[,0:164],
                           target = "correction_info")

tuneGes <- tuneRanger(tuneGes,
                      measure = list(multiclass.brier),
                      num.trees = 500)

# Return hyperparameter values
tuneGes

# Recommended parameter settings: 
# > tuneGes[["recommended.pars"]][["mtry"]]
# [1] 67
# > 
# > tuneGes[["recommended.pars"]][["min.node.size"]]
# [1] 2
# > tuneGes[["recommended.pars"]][["sample.fraction"]]
# [1] 0.8896767
# > tuneGes[["recommended.pars"]][["multiclass.brier"]]
# [1] 0.6112816
# > tuneGes[["recommended.pars"]][["exec.time"]]
# [1] 0.194

gesTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:163], 
  num.trees = 5000, 
  mtry = 67, # Set the recommended mtry value (number of features).
  min.node.size = 2, # Set the recommended min.node.size value (number of samples before a node terminates).
  sample.fraction = 0.8896767, # Set the recommended sample fraction value.(% of data for bagging).
  importance = "permutation" # Permutation is a computationally intensive test.
)

predictions <- predict(gesTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(gesTuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)


# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```


Create a tuned model only.

```{r tuned model imputed ges, echo=TRUE, message=FALSE, warning=FALSE}
# Create a classification task for tuning
tuneGes <- makeClassifTask(data = train_data[, 0:164], target = "correction_info")

# Tune the model
tuneGes <- tuneRanger(tuneGes, measure = list(multiclass.brier), num.trees = 500)

# Return hyperparameter values
tuneGes

# Recommended parameter settings: 
#   mtry min.node.size sample.fraction multiclass.brier exec.time
# 1    3             3       0.7533427        0.7109598     0.168

# Fit the tuned model on the training data
gesTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[, 0:163],
  num.trees = 5000,
  mtry = 3,
  min.node.size = 3,
  sample.fraction = 0.7533427,
  importance = "permutation"
)

# Predict on the test data
predictions <- predict(gesTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(gesTuned, num.threads = 1, type = 1)

# Convert to data frame
feature_importance_df <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance_df$Feature <- rownames(feature_importance_df)
colnames(feature_importance_df) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance_df[order(-feature_importance_df$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)

# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```


Save  data frame.

```{r save imputed data, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}

write.csv(data_ges, file = paste0(datasets, "gesDataXGB.csv"), row.names = FALSE)
```

# XGBoost

Ensure parallel processing.

```{r parallel, echo=TRUE, message=FALSE, warning=FALSE}
# Detect the number of available cores
cores <- detectCores() #- 1  # Leave one core free

# Create a cluster with the detected number of cores
cl <- makeCluster(cores)

# Register the parallel backend
registerDoParallel(cl)
```

Define the grid and estimate runtime.

```{r grid, echo=TRUE, message=FALSE, warning=FALSE}
grid_tune <- expand.grid(
  nrounds = c(5000, 10000), 
  max_depth = c(3, 6), 
  eta = c(0.05, 0.1), 
  gamma = c(0.1), 
  colsample_bytree = c(0.6, 0.8), 
  min_child_weight = c(1), 
  subsample = c(0.75, 1.0)
)

# Calculate total combinations
total_combinations <- nrow(grid_tune)

# Estimate single model run time (assume 1 minute per run)
single_model_time <- 10 # minute

# Total runs for cross-validation
folds <- 5
total_runs <- total_combinations * folds

# Total time estimation without parallel processing
total_time <- total_runs * single_model_time # in minutes

# Convert to hours
total_time_hours <- total_time / 60

# Output estimated time without parallel processing
print(paste("Estimated time for grid search without parallel processing:", total_time_hours, "hours"))

# Parallel processing with 4 cores
cores <- 24
total_time_parallel <- total_time / cores # in minutes

# Convert to hours
total_time_parallel_hours <- total_time_parallel / 60

# Output estimated time with parallel processing
print(paste("Estimated time for grid search with", cores, "cores:", total_time_parallel_hours, "hours"))

rm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)

```

#### K-fold cross-validation

Create subsets to train and test data (80/20).


```{r}

# Set seed for reproducibility
set.seed(998)

# Define the number of subsets
numSubsets <- 5

# Load MICE-imputed data (using placeholder 'data_ges' as the input dataset)
gesDataXGB <- data_ges

# Ensure 'correction_info' is a factor
gesDataXGB$correction_info <- as.factor(gesDataXGB$correction_info)

# Remove rows with only NA values
gesDataXGB <- gesDataXGB[rowSums(is.na(gesDataXGB)) < ncol(gesDataXGB), ]

# Split data by levels of 'correction_info'
correction_levels <- levels(gesDataXGB$correction_info)
split_data <- split(gesDataXGB, gesDataXGB$correction_info)

# Initialize a list to store subsets
gesSubsets <- vector("list", length = numSubsets)

# Distribute rows for each level equally across subsets
for (level in correction_levels) {
  level_data <- split_data[[level]]
  subset_sizes <- rep(floor(nrow(level_data) / numSubsets), numSubsets)
  remainder <- nrow(level_data) %% numSubsets
  
  # Distribute remainder rows randomly
  if (remainder > 0) {
    subset_sizes[seq_len(remainder)] <- subset_sizes[seq_len(remainder)] + 1
  }
  
  # Shuffle rows of the level and assign to subsets
  shuffled_data <- level_data[sample(nrow(level_data)), ]
  indices <- cumsum(c(0, subset_sizes))
  
  for (i in 1:numSubsets) {
    if (is.null(gesSubsets[[i]])) {
      gesSubsets[[i]] <- shuffled_data[(indices[i] + 1):indices[i + 1], ]
    } else {
      gesSubsets[[i]] <- rbind(gesSubsets[[i]], shuffled_data[(indices[i] + 1):indices[i + 1], ])
    }
  }
}

# Naming the subsets
names(gesSubsets) <- paste0("gesData", 1:numSubsets)

# Verify balance in subsets
for (i in 1:numSubsets) {
  cat("Subset", i, "contains rows:", nrow(gesSubsets[[i]]), "and levels:\n")
  print(table(gesSubsets[[i]]$correction_info))
}

# Remove any rows with only NAs from subsets just to ensure cleanliness
gesSubsets <- lapply(gesSubsets, function(subset) {
  subset[rowSums(is.na(subset)) < ncol(subset), ]
})

# Access the subsets
gesData1 <- gesSubsets$gesData1
gesData2 <- gesSubsets$gesData2
gesData3 <- gesSubsets$gesData3
gesData4 <- gesSubsets$gesData4
gesData5 <- gesSubsets$gesData5

# Combine subsets into 80% groups
gesData1234 <- rbind(gesData1, gesData2, gesData3, gesData4)
gesData1235 <- rbind(gesData1, gesData2, gesData3, gesData5)
gesData1245 <- rbind(gesData1, gesData2, gesData4, gesData5)
gesData1345 <- rbind(gesData1, gesData3, gesData4, gesData5)
gesData2345 <- rbind(gesData2, gesData3, gesData4, gesData5)

# Final verification of all levels in the combined datasets
combined_sets <- list(gesData1234, gesData1235, gesData1245, gesData1345, gesData2345)
names(combined_sets) <- c("gesData1234", "gesData1235", "gesData1245", "gesData1345", "gesData2345")

for (set_name in names(combined_sets)) {
  cat("Dataset", set_name, "contains rows:", nrow(combined_sets[[set_name]]), "and levels:\n")
  print(table(combined_sets[[set_name]]$correction_info))
}

```

```{r}

# Set seed for reproducibility
set.seed(998)

# Set up train control
train_control <- trainControl(
  method = "cv",        # Cross-validation
  number = 5,           # 5-fold cross-validation
  allowParallel = TRUE  # Enable parallel processing
)

# Define the number of subsets
numSubsets <- 5

# Create an empty list to store subsets
gesSubsets <- vector("list", length = numSubsets)

# load MICE imputed data
#gerDataXGB <- read_csv(paste0(datasets, "gerDataXGB.csv"))
gesDataXGB <- data_ges
# ensure percProm is factor
gesDataXGB$correction_info <- as.factor(gesDataXGB$correction_info)
levels(gesDataXGB$correction_info)
# only keep the columns of output and predictor variables
#gerDataXGB <- gerDataXGB[,13:52] 

# Calculate the number of samples in each subset
subsetSize <- nrow(gesDataXGB) %/% numSubsets

# Randomly assign samples to subsets
for (i in 1:numSubsets) {
  if (i < numSubsets) {
    gesSubsets[[i]] <- gesDataXGB[sample((1:nrow(gesDataXGB)), size = subsetSize), ]
  } else {
    gesSubsets[[i]] <- gesDataXGB[sample((1:nrow(gesDataXGB)), size = subsetSize + (nrow(gesDataXGB) %% numSubsets)), ]
  }
}

# Naming the subsets
names(gesSubsets) <- paste0("gesData", 1:numSubsets)

# Verify that all subsets contain all levels
for (i in 1:numSubsets) {
  cat("Subset", i, "contains levels:", levels(gesSubsets[[i]]$correction_info), "\n")
}


# Access the subsets (e.g., gerData1, gerData2, etc.)
gesData1 <- gesSubsets$gesData1
gesData2 <- gesSubsets$gesData2
gesData3 <- gesSubsets$gesData3
gesData4 <- gesSubsets$gesData4
gesData5 <- gesSubsets$gesData5

# Combine subsets into 80% groups.
gesData1234 <- rbind(gesData1, gesData2, gesData3, gesData4)
gesData1235 <- rbind(gesData1, gesData2, gesData3, gesData5)
gesData1245 <- rbind(gesData1, gesData2, gesData4, gesData5)
gesData1345 <- rbind(gesData1, gesData3, gesData4, gesData5)
gesData2345 <- rbind(gesData2, gesData3, gesData4, gesData5)
```


```{r k-fold subset, echo=TRUE, message=FALSE, warning=FALSE}
# Set seed for reproducibility
set.seed(998)

# Set up train control
train_control <- trainControl(
  method = "cv",        # Cross-validation
  number = 5,           # 5-fold cross-validation
  allowParallel = TRUE  # Enable parallel processing
)

# Define the number of subsets
numSubsets <- 5

# Create an empty list to store subsets
gesSubsets <- vector("list", length = numSubsets)

# Ensure 'correction_info' is a factor
gesDataXGB$correction_info <- as.factor(gesDataXGB$correction_info)

# Perform stratified sampling
for (level in levels(gesDataXGB$correction_info)) {
  # Subset the data by level
  level_data <- gesDataXGB[gesDataXGB$correction_info == level, ]
  
  # Calculate subset size for each level
  subsetSize <- nrow(level_data) %/% numSubsets
  extraSamples <- nrow(level_data) %% numSubsets
  
  # Randomly assign samples of each level to subsets
  shuffled <- sample(nrow(level_data)) # Randomly shuffle row indices
  for (i in 1:numSubsets) {
    start_idx <- (i - 1) * subsetSize + 1
    end_idx <- ifelse(i < numSubsets, i * subsetSize, nrow(level_data))
    if (i == numSubsets && extraSamples > 0) {
      end_idx <- end_idx + extraSamples
    }
    if (is.null(gesSubsets[[i]])) {
      gesSubsets[[i]] <- level_data[shuffled[start_idx:end_idx], ]
    } else {
      gesSubsets[[i]] <- rbind(gesSubsets[[i]], level_data[shuffled[start_idx:end_idx], ])
    }
  }
}



# load MICE imputed data
# #gerDataXGB <- read_csv(paste0(datasets, "gerDataXGB.csv"))
# gesDataXGB <- data_ges
# # ensure percProm is factor
# gesDataXGB$correction_info <- as.factor(gesDataXGB$correction_info)
# levels(gesDataXGB$correction_info)
# # only keep the columns of output and predictor variables
# #gerDataXGB <- gerDataXGB[,13:52] 
# 
# # Calculate the number of samples in each subset
# subsetSize <- nrow(gesDataXGB) %/% numSubsets
# extraSamples <- nrow(level_data) %% numSubsets
# 

# 
# 
# 
# # Randomly assign samples to subsets
# for (i in 1:numSubsets) {
#   if (i < numSubsets) {
#     gesSubsets[[i]] <- gesDataXGB[sample((1:nrow(gesDataXGB)), size = subsetSize), ]
#   } else {
#     gesSubsets[[i]] <- gesDataXGB[sample((1:nrow(gesDataXGB)), size = subsetSize + (nrow(gesDataXGB) %% numSubsets)), ]
#   }
# }

# Naming the subsets
names(gesSubsets) <- paste0("gesData", 1:numSubsets)

# Verify that all subsets contain all levels
for (i in 1:numSubsets) {
  cat("Subset", i, "contains levels:", levels(gesSubsets[[i]]$correction_info), "\n")
}


# Access the subsets (e.g., gerData1, gerData2, etc.)
gesData1 <- gesSubsets$gesData1
gesData2 <- gesSubsets$gesData2
gesData3 <- gesSubsets$gesData3
gesData4 <- gesSubsets$gesData4
gesData5 <- gesSubsets$gesData5

# Combine subsets into 80% groups.
gesData1234 <- rbind(gesData1, gesData2, gesData3, gesData4)
gesData1235 <- rbind(gesData1, gesData2, gesData3, gesData5)
gesData1245 <- rbind(gesData1, gesData2, gesData4, gesData5)
gesData1345 <- rbind(gesData1, gesData3, gesData4, gesData5)
gesData2345 <- rbind(gesData2, gesData3, gesData4, gesData5)

```

#### Models

Only run the models one time and then readRDS.

##### Model 1

```{r ges model1, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel1 <- caret::train(
  correction_info ~ .,              
  data = gesData1234,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel1, file = paste0(models, "gesModel1.rds"), compress = TRUE)
```

##### Model 2

```{r ges model2, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel2 <- caret::train(
  correction_info ~ .,              
  data = gesData1235,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel2, file = paste0(models, "gesModel2.rds"), compress = TRUE)
```

##### Model 3

```{r ges model3, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel3 <- caret::train(
  correction_info ~ .,              
  data = gesData1245,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel3, file = paste0(models, "gesModel3.rds"), compress = TRUE)
```

##### Model 4

```{r ges model4, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel4 <- caret::train(
  correction_info ~ .,              
  data = gesData1345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)
saveRDS(gesModel4, file = paste0(models, "gesModel4.rds"), compress = TRUE)
```

##### Model 5

```{r ges model5, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel5 <- caret::train(
  correction_info ~ .,              
  data = gesData2345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel5, file = paste0(models, "gesModel5.rds"), compress = TRUE)
```

##### Load models

Load all models after running, if necessary.

```{r loas models ges, echo=TRUE, message=FALSE, warning=FALSE}
gesModel1 <- readRDS(paste0(models, "gesModel1.rds"))
gesModel2 <- readRDS(paste0(models, "gesModel2.rds"))
gesModel3 <- readRDS(paste0(models, "gesModel3.rds"))
gesModel4 <- readRDS(paste0(models, "gesModel4.rds"))
gesModel5 <- readRDS(paste0(models, "gesModel5.rds"))
```

#### Test models

Generate predictions and confusion matrices

```{r test models ges, echo=TRUE, message=FALSE, warning=FALSE}
# Generate predictions
gesPredictions1 <- predict(gesModel1, newdata = gesData5)
gesPredictions2 <- predict(gesModel2, newdata = gesData4)
gesPredictions3 <- predict(gesModel3, newdata = gesData3)
gesPredictions4 <- predict(gesModel4, newdata = gesData2)
gesPredictions5 <- predict(gesModel5, newdata = gesData1)

# Compute confusion matrices
gesCm1 <- confusionMatrix(gesPredictions1, gesData5$correction_info)
gesCm2 <- confusionMatrix(gesPredictions2, gesData4$correction_info)
gesCm3 <- confusionMatrix(gesPredictions3, gesData3$correction_info)
gesCm4 <- confusionMatrix(gesPredictions4, gesData2$correction_info)
gesCm5 <- confusionMatrix(gesPredictions5, gesData1$correction_info)

# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)
gesPValues <- c(gesCm1$overall['AccuracyPValue'], 
              gesCm2$overall['AccuracyPValue'], 
              gesCm3$overall['AccuracyPValue'], 
              gesCm4$overall['AccuracyPValue'], 
              gesCm5$overall['AccuracyPValue'])
```

Combine p-values using Fisher's method

```{r combine p-vals ges, echo=TRUE, message=FALSE, warning=FALSE}
# Fisher's method
gesFisher_combined <- -2 * sum(log(gesPValues))
df <- 2 * length(gesPValues)
gesPCcombined_fisher <- 1 - pchisq(gesFisher_combined, df)
print(gesPCcombined_fisher)

# Stouffer's method
gesZ_scores <- qnorm(1 - gesPValues/2)
gesCombined_z <- sum(gesZ_scores) / sqrt(length(gesPValues))
gesP_combined_stouffer <- 2 * (1 - pnorm(abs(gesCombined_z)))
print(gesP_combined_stouffer)
```

The p-values sum up to 0, since they are all so small.

#### Feature importance

##### Model 1

```{r ges feature importance 1, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel1 <- gesModel1$finalModel
importanceXGBgesModel1 <- xgb.importance(model = XGBgesModel1)
print(importanceXGBgesModel1)
xgb.plot.importance(importanceXGBgesModel1)
```

##### Model 2

```{r ges feature importance 2, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel2 <- gesModel2$finalModel
importanceXGBgesModel2 <- xgb.importance(model = XGBgesModel2)
print(importanceXGBgesModel2)
xgb.plot.importance(importanceXGBgesModel2)
```

##### Model 3

```{r ges feature importance 3, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel3 <- gesModel3$finalModel
importanceXGBgesModel3 <- xgb.importance(model = XGBgesModel3)
print(importanceXGBgesModel3)
xgb.plot.importance(importanceXGBgesModel3)
```

##### Model 4

```{r ges feature importance 4, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel4 <- gesModel4$finalModel
importanceXGBgesModel4 <- xgb.importance(model = XGBgesModel4)
print(importanceXGBgesModel4)
xgb.plot.importance(importanceXGBgesModel4)
```

##### Model 5

```{r ges feature importance 5, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel5 <- gesModel5$finalModel
importanceXGBgesModel5 <- xgb.importance(model = XGBgesModel5)
print(importanceXGBgesModel5)
xgb.plot.importance(importanceXGBgesModel5)
```

##### Cumulative feature importance

```{r ges cumulative importance, echo=TRUE, message=FALSE, warning=FALSE}
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
  importance <- xgb.importance(model = model)
  importance$Gain <- importance$Gain / sum(importance$Gain)
  return(importance)
}

# Extract normalized importance for each model
gesImportance1 <- get_normalized_importance(gesModel1$finalModel)
gesImportance2 <- get_normalized_importance(gesModel2$finalModel)
gesImportance3 <- get_normalized_importance(gesModel3$finalModel)
gesImportance4 <- get_normalized_importance(gesModel4$finalModel)
gesImportance5 <- get_normalized_importance(gesModel5$finalModel)

# Combine importances
gesAllImportances <- list(gesImportance1, gesImportance2, gesImportance3, gesImportance4, gesImportance5)

# Function to merge importances
merge_importances <- function(importances) {
  for (i in 2:length(importances)) {
    names(importances[[i]])[2:4] <- paste0(names(importances[[i]])[2:4], "_", i)
  }
  merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
  merged[is.na(merged)] <- 0  # Replace NAs with 0
  gain_cols <- grep("Gain", colnames(merged), value = TRUE)
  merged$Cumulative <- rowSums(merged[, ..gain_cols])
  return(merged[, .(Feature, Cumulative)])
}

# Merge and sort importances
gesCumulativeImportance <- merge_importances(gesAllImportances)
gesCumulativeImportance <- gesCumulativeImportance[order(-gesCumulativeImportance$Cumulative), ]

# Print cumulative feature importance
print(gesCumulativeImportance)
```


## Here goes PCA

SOMETHING LIKE THIS BUT NEEDS TO BE ADAPTED


```{r}

# Load necessary libraries
library(tidyverse)
library(factoextra)
library(ggplot2)

# Example: Replace 'df' with your actual dataset
# Assuming `correction` is the column with labels
# Assume numeric columns are used for PCA

# Data Preparation
# Only select numerical columns
Gdata <- data_ges %>% select(where(is.numeric))


# Replace NA with 0
Gdata[is.na(Gdata)] <- 0

# get rid of all gesture-superfluous cols
colstoremove <- c('numofArt') # it has 0 variance so PCA gives error
Gdata <- Gdata %>%
  select(-contains(colstoremove))


# Separate labels
labels <- data_ges$correction_info

# Standardize the features
data_scaled <- scale(Gdata)

# Perform PCA
pca <- prcomp(data_scaled, center = TRUE, scale. = TRUE)

biplot(pca, scale = 0, cex = 0.6)
screeplot(pca)

# Explained Variance
explained_variance <- summary(pca)$importance[2, ] # Proportion of Variance
cumulative_variance <- cumsum(explained_variance)

# Plot explained variance
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 1)) +
  ggtitle("Explained Variance by PCA")

# Number of components explaining 95% variance
threshold <- 0.80
num_components <- which(cumulative_variance >= threshold)[1]
cat("Number of components explaining 95% variance:", num_components, "\n")

# Feature Contributions to Components
feature_contributions <- as.data.frame(pca$rotation)
feature_contributions$Feature <- rownames(feature_contributions)
rownames(feature_contributions) <- NULL

# Print feature contributions to PCs
cat("\nFeature Contributions to Principal Components:\n")
print(feature_contributions)

# Top contributing features for each principal component
top_contributors <- feature_contributions %>%
  pivot_longer(-Feature, names_to = "PrincipalComponent", values_to = "Contribution") %>%
  group_by(PrincipalComponent) %>%
  slice_max(order_by = abs(Contribution), n = 10) %>%
  arrange(PrincipalComponent, desc(abs(Contribution)))

cat("\nTop Contributing Features per Component:\n")
print(top_contributors)

# Visualize contributions to first few principal components
top_pc <- colnames(feature_contributions)[1:min(5, ncol(feature_contributions) - 1)] # Top 5 PCs
for (pc in top_pc) {
  ggplot(filter(top_contributors, PrincipalComponent == pc), aes(x = reorder(Feature, Contribution), y = Contribution)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    ggtitle(paste("Feature Contributions to", pc)) +
    ylab("Contribution") +
    xlab("Features") +
    theme_minimal() +
    print()
}

# Visualize PCA result (e.g., PC1 vs PC2)
pca_df <- as.data.frame(pca$x)
pca_df$Correction <- labels

ggplot(pca_df, aes(x = PC1, y = PC2, color = Correction)) +
  geom_point() +
  ggtitle("PCA: Correction Clusters") +
  xlab("PC1") +
  ylab("PC2") +
  theme_minimal()


```


######################
# Vocalization #######
######################

First, for vocal modality, we need to remove columns with features that are not present in vocal modality overal. These are all gesture features. They should be anyway mostly filled with NaNs as in vocal modality, any gestures were 'forbidden'. However, we will keep the balance features (associated with center of pressure, COP) as they still reflect bodily posture during vocal performance. Additionally, we need to make sure that df does not contain any NAs. 

```{r}

# get rid of all gesture-superfluous cols
colstokeep <- c('envelope', 'loudness', 'roughness', 'flux', 'novelty', 
                     'harmEnergy', 'audio', 'envelope_change', 'f0', 
                     'f1', 'f2', 'f3', 'env_', 'duration_voc', 'COP', 'correction', 'TrialID', 'modality')

data_voc <- data_voc %>%
  select(matches(paste(colstokeep, collapse = "|")))

colstoremove <- c('sync', 'time')

data_voc <- data_voc %>%
  select(-contains(colstoremove))

# get rid of NA columns
data_voc[is.na(data_voc)] <- 0 # CAREFUL - here we loose lot of cols (MICE maybe needed)

data_voc <- data_voc %>%
  select(-TrialID, -modality)

# get rid of all char columns
data_voc <- data_voc[, sapply(data_voc, class) != "character"]

```

## Random forests

```{r}

# prepare predictors
predictors <- setdiff(names(data_voc), "correction_info")

formula_str <- paste("correction_info ~", paste(predictors, collapse = " + "))

# Convert the formula string to a formula object
vocTree_formula <- as.formula(formula_str)

# Now use the formula in rpart
vocTree <- rpart(formula = vocTree_formula, data = data_ges, 
                method='class', # Specify that it's a classification tree
                control = rpart.control(maxdepth = 5)  # Control parameters for the 'rpart' function
)

prp(
  vocTree,         # The decision tree object to be visualized
  extra = 1,      # Show extra information (like node statistics) in the plot
  varlen = 0,     # Length of variable names (0 means auto-determined)
  faclen = 0     # Length of factor levels displayed on the plot (increase as needed)
)
```

We will build a random forest first.

```{r set seed, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(995) # Set a seed for reproducibility
```

Split the data
```{r}


# Split the data into training and testing subsets
sample_indices <- sample(1:nrow(data_voc), 0.8*nrow(data_voc)) # 80% training, 20% testing
train_data <- data_voc[sample_indices, ]
test_data <- data_voc[-sample_indices, ]

```

Building the untuned model.

```{r untuned ger, echo=TRUE, message=FALSE, warning=FALSE}
# Untuned Model with importance (permutation) option set
vocUntuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:118], # FLAGGED: adapt
  num.trees = 500,
  importance = "permutation"
)

predictions <- predict(vocUntuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(vocUntuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)
```

Set the parameters for the random forest.

```{r settings for RF ges, echo=TRUE, message=FALSE, warning=FALSE}
# Define the number of CPU cores to use
num_cores <- detectCores()

# Create a cluster with specified number of cores
cl <- makeCluster(num_cores)
```

Tuning the random forest.

```{r tuning RF ger, message=FALSE, warning=FALSE}
tuneVoc <- makeClassifTask(data = data_voc[,0:119], # FLAGGED: adapt
                           target = "correction_info")

tuneVoc <- tuneRanger(tuneVoc,
                      measure = list(multiclass.brier),
                      num.trees = 500)

#Return hyperparameter values
tuneVoc
## Ola: ALWAYS REPORT THOSE BECAUSE THEY ARE DIFFERENT EVERY TIME BC OF RANDOMIZATION
# Recommended parameter settings: 
#   mtry min.node.size sample.fraction
# 1   20             4       0.2387222
# Results: 
#   multiclass.brier exec.time
# 1        0.8037542     0.17

vocTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:161],  #FLAGGED: adapt
  num.trees = 5000, 
  mtry = 61, # Set the recommended mtry value (number of features).
  min.node.size = 4, # Set the recommended min.node.size value (number of samples before a node terminates).
  sample.fraction = 0.3161722, # Set the recommended sample fraction value.(% of data for bagging).
  importance = "permutation" # Permutation is a computationally intensive test.
)

predictions <- predict(vocTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(vocTuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)


# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```


Create a tuned model only.

```{r tuned model imputed ger, echo=TRUE, message=FALSE, warning=FALSE}
# Create a classification task for tuning
tuneVoc <- makeClassifTask(data = train_data[, 0:162], target = "correction_info") # FLAGGED: adapt

# Tune the model
tuneVoc <- tuneRanger(tuneVoc, measure = list(multiclass.brier), num.trees = 500)

# Return hyperparameter values
tuneVoc
# Recommended parameter settings: 
#   mtry min.node.size sample.fraction
# 1    66             3       0.2254135
# Results: 
#   multiclass.brier exec.time
# 1        0.7845583     0.168

# Fit the tuned model on the training data
vocTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[, 0:165], # FLAGGED: adapt
  num.trees = 5000,
  mtry = 66,
  min.node.size = 3,
  sample.fraction = 0.2254135,
  importance = "permutation"
)

# Predict on the test data
predictions <- predict(vocTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(vocTuned, num.threads = 1, type = 1)

# Convert to data frame
feature_importance_df <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance_df$Feature <- rownames(feature_importance_df)
colnames(feature_importance_df) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance_df[order(-feature_importance_df$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)

# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```


Save  data frame.

```{r save imputed data, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}

write.csv(data_voc, file = paste0(datasets, "vocDataXGB.csv"), row.names = FALSE)
```

# XGBoost

Ensure parallel processing.

```{r parallel, echo=TRUE, message=FALSE, warning=FALSE}
# Detect the number of available cores
cores <- detectCores() #- 1  # Leave one core free

# Create a cluster with the detected number of cores
cl <- makeCluster(cores)

# Register the parallel backend
registerDoParallel(cl)
```

Define the grid and estimate runtime.

```{r grid, echo=TRUE, message=FALSE, warning=FALSE}
grid_tune <- expand.grid(
  nrounds = c(5000, 10000), 
  max_depth = c(3, 6), 
  eta = c(0.05, 0.1), 
  gamma = c(0.1), 
  colsample_bytree = c(0.6, 0.8), 
  min_child_weight = c(1), 
  subsample = c(0.75, 1.0)
)

# Calculate total combinations
total_combinations <- nrow(grid_tune)

# Estimate single model run time (assume 1 minute per run)
single_model_time <- 10 # minute

# Total runs for cross-validation
folds <- 5
total_runs <- total_combinations * folds

# Total time estimation without parallel processing
total_time <- total_runs * single_model_time # in minutes

# Convert to hours
total_time_hours <- total_time / 60

# Output estimated time without parallel processing
print(paste("Estimated time for grid search without parallel processing:", total_time_hours, "hours"))

# Parallel processing with 4 cores
cores <- 24
total_time_parallel <- total_time / cores # in minutes

# Convert to hours
total_time_parallel_hours <- total_time_parallel / 60

# Output estimated time with parallel processing
print(paste("Estimated time for grid search with", cores, "cores:", total_time_parallel_hours, "hours"))

rm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)
```

#### K-fold cross-validation

Create subsets to train and test data (80/20).

```{r k-fold subset, echo=TRUE, message=FALSE, warning=FALSE}
# Set seed for reproducibility
set.seed(998)

# Set up train control
train_control <- trainControl(
  method = "cv",        # Cross-validation
  number = 5,           # 5-fold cross-validation
  allowParallel = TRUE  # Enable parallel processing
)

# Define the number of subsets
numSubsets <- 5

# Create an empty list to store subsets
vocSubsets <- vector("list", length = numSubsets)

# load MICE imputed data
#gerDataXGB <- read_csv(paste0(datasets, "gerDataXGB.csv"))
vocDataXGB <- data_voc
# ensure percProm is factor
vocDataXGB$correction_info <- as.factor(vocDataXGB$correction_info)
levels(vocDataXGB$correction_info)
# only keep the columns of output and predictor variables
#gerDataXGB <- gerDataXGB[,13:52] 

# Calculate the number of samples in each subset
subsetSize <- nrow(vocDataXGB) %/% numSubsets

# Randomly assign samples to subsets
for (i in 1:numSubsets) {
  if (i < numSubsets) {
    vocSubsets[[i]] <- vocDataXGB[sample((1:nrow(vocDataXGB)), size = subsetSize), ]
  } else {
    vocSubsets[[i]] <- vocDataXGB[sample((1:nrow(vocDataXGB)), size = subsetSize + (nrow(vocDataXGB) %% numSubsets)), ]
  }
}

# Naming the subsets
names(vocSubsets) <- paste0("vocData", 1:numSubsets)

# Access the subsets (e.g., gerData1, gerData2, etc.)
vocData1 <- vocSubsets$vocData1
vocData2 <- vocSubsets$vocData2
vocData3 <- vocSubsets$vocData3
vocData4 <- vocSubsets$vocData4
vocData5 <- vocSubsets$vocData5

# Combine subsets into 80% groups.
vocData1234 <- rbind(vocData1, vocData2, vocData3, vocData4)
vocData1235 <- rbind(vocData1, vocData2, vocData3, vocData5)
vocData1245 <- rbind(vocData1, vocData2, vocData4, vocData5)
vocData1345 <- rbind(vocData1, vocData3, vocData4, vocData5)
vocData2345 <- rbind(vocData2, vocData3, vocData4, vocData5)

```

#### Models

Only run the models one time and then readRDS.

##### Model 1

```{r ger model1, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
vocModel1 <- caret::train(
  correction_info ~ .,              
  data = vocData1234,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(vocModel1, file = paste0(models, "vocModel1.rds"), compress = TRUE)
```

##### Model 2

```{r ger model2, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
vocModel2 <- caret::train(
  correction_info ~ .,              
  data = vocData1235,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(vocModel2, file = paste0(models, "vocModel2.rds"), compress = TRUE)
```

##### Model 3

```{r ger model3, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
vocModel3 <- caret::train(
  correction_info ~ .,              
  data = vocData1245,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(vocModel3, file = paste0(models, "vocModel3.rds"), compress = TRUE)
```

##### Model 4

```{r ger model4, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
vocModel4 <- caret::train(
  correction_info ~ .,              
  data = vocData1345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)
saveRDS(vocModel4, file = paste0(models, "vocModel4.rds"), compress = TRUE)
```

##### Model 5

```{r ger model5, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
vocModel5 <- caret::train(
  correction_info ~ .,              
  data = vocData2345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(vocModel5, file = paste0(models, "vocModel5.rds"), compress = TRUE)
```

##### Load models

Load all models after running, if necessary.

```{r loas models ger, echo=TRUE, message=FALSE, warning=FALSE}
vocModel1 <- readRDS(paste0(models, "vocModel1.rds"))
vocModel2 <- readRDS(paste0(models, "vocModel2.rds"))
vocModel3 <- readRDS(paste0(models, "vocModel3.rds"))
vocModel4 <- readRDS(paste0(models, "vocModel4.rds"))
vocModel5 <- readRDS(paste0(models, "vocModel5.rds"))
```

#### Test models

Generate predictions and confusion matrices

```{r test models ger, echo=TRUE, message=FALSE, warning=FALSE}
# Generate predictions
vocPredictions1 <- predict(vocModel1, newdata = vocData5)
vocPredictions2 <- predict(vocModel2, newdata = vocData4)
vocPredictions3 <- predict(vocModel3, newdata = vocData3)
vocPredictions4 <- predict(vocModel4, newdata = vocData2)
vocPredictions5 <- predict(vocModel5, newdata = vocData1)

# Compute confusion matrices
vocCm1 <- confusionMatrix(vocPredictions1, vocData5$correction_info)
vocCm2 <- confusionMatrix(vocPredictions2, vocData4$correction_info)
vocCm3 <- confusionMatrix(vocPredictions3, vocData3$correction_info)
vocCm4 <- confusionMatrix(vocPredictions4, vocData2$correction_info)
vocCm5 <- confusionMatrix(vocPredictions5, vocData1$correction_info)

# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)
vocPValues <- c(vocCm1$overall['AccuracyPValue'], 
              vocCm2$overall['AccuracyPValue'], 
              vocCm3$overall['AccuracyPValue'], 
              vocCm4$overall['AccuracyPValue'], 
              vocCm5$overall['AccuracyPValue'])
```

Combine p-values using Fisher's method

```{r combine p-vals ger, echo=TRUE, message=FALSE, warning=FALSE}
# Fisher's method
vocFisher_combined <- -2 * sum(log(vocPValues))
df <- 2 * length(vocPValues)
vocPCcombined_fisher <- 1 - pchisq(vocFisher_combined, df)
print(vocPCcombined_fisher)

# Stouffer's method
vocZ_scores <- qnorm(1 - vocPValues/2)
vocCombined_z <- sum(vocZ_scores) / sqrt(length(vocPValues))
vocP_combined_stouffer <- 2 * (1 - pnorm(abs(vocCombined_z)))
print(vocP_combined_stouffer)
```

The p-values sum up to 0, since they are all so small.

#### Feature importance

##### Model 1

```{r ger feature importance 1, echo=TRUE, message=FALSE, warning=FALSE}
XGBvocModel1 <- vocModel1$finalModel
importanceXGBvocModel1 <- xgb.importance(model = XGBvocModel1)
print(importanceXGBvocModel1)
xgb.plot.importance(importanceXGBvocModel1)
```

##### Model 2

```{r ger feature importance 2, echo=TRUE, message=FALSE, warning=FALSE}
XGBvocModel2 <- vocModel2$finalModel
importanceXGBvocModel2 <- xgb.importance(model = XGBvocModel2)
print(importanceXGBvocModel2)
xgb.plot.importance(importanceXGBvocModel2)
```

##### Model 3

```{r ger feature importance 3, echo=TRUE, message=FALSE, warning=FALSE}
XGBvocModel3 <- vocModel3$finalModel
importanceXGBvocModel3 <- xgb.importance(model = XGBvocModel3)
print(importanceXGBvocModel3)
xgb.plot.importance(importanceXGBvocModel3)
```

##### Model 4

```{r ger feature importance 4, echo=TRUE, message=FALSE, warning=FALSE}
XGBvocModel4 <- vocModel4$finalModel
importanceXGBvocModel4 <- xgb.importance(model = XGBvocModel4)
print(importanceXGBvocModel4)
xgb.plot.importance(importanceXGBvocModel4)
```

##### Model 5

```{r ger feature importance 5, echo=TRUE, message=FALSE, warning=FALSE}
XGBvocModel5 <- vocModel5$finalModel
importanceXGBvocModel5 <- xgb.importance(model = XGBvocModel5)
print(importanceXGBvocModel5)
xgb.plot.importance(importanceXGBvocModel5)
```

##### Cumulative feature importance

```{r ger cumulative importance, echo=TRUE, message=FALSE, warning=FALSE}
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
  importance <- xgb.importance(model = model)
  importance$Gain <- importance$Gain / sum(importance$Gain)
  return(importance)
}

# Extract normalized importance for each model
vocImportance1 <- get_normalized_importance(vocModel1$finalModel)
vocImportance2 <- get_normalized_importance(vocModel2$finalModel)
vocImportance3 <- get_normalized_importance(vocModel3$finalModel)
vocImportance4 <- get_normalized_importance(vocModel4$finalModel)
vocImportance5 <- get_normalized_importance(vocModel5$finalModel)

# Combine importances
vocAllImportances <- list(vocImportance1, vocImportance2, vocImportance3, vocImportance4, vocImportance5)

# Function to merge importances
merge_importances <- function(importances) {
  for (i in 2:length(importances)) {
    names(importances[[i]])[2:4] <- paste0(names(importances[[i]])[2:4], "_", i)
  }
  merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
  merged[is.na(merged)] <- 0  # Replace NAs with 0
  gain_cols <- grep("Gain", colnames(merged), value = TRUE)
  merged$Cumulative <- rowSums(merged[, ..gain_cols])
  return(merged[, .(Feature, Cumulative)])
}

# Merge and sort importances
vocCumulativeImportance <- merge_importances(vocAllImportances)
vocCumulativeImportance <- vocCumulativeImportance[order(-vocCumulativeImportance$Cumulative), ]

# Print cumulative feature importance
print(vocCumulativeImportance)
```


######################
# Multimodal #########
######################

In multimodal/combined condition, all features stay as both gesture and vocalization suppose to be used by performers. But we still need to make sure that df does not contain any NAs. 

```{r}

# get rid of NA columns
data_mult <- data_mult[, colSums(is.na(data_mult)) == 0] # FLAGGED: maybe use MICE instead

data_mult <- data_mult %>%
  select(-TrialID, -modality)

# get rid of all char columns
data_mult <- data_mult[, sapply(data_mult, class) != "character"]


```

## Random forests

```{r}

# prepare predictors
predictors <- setdiff(names(data_mult), "correction_info")

formula_str <- paste("correction_info ~", paste(predictors, collapse = " + "))

# Convert the formula string to a formula object
multTree_formula <- as.formula(formula_str)

# Now use the formula in rpart
multTree <- rpart(formula = multTree_formula, data = data_mult, 
                method='class', # Specify that it's a classification tree
                control = rpart.control(maxdepth = 5)  # Control parameters for the 'rpart' function
)

prp(
  multTree,         # The decision tree object to be visualized
  extra = 1,      # Show extra information (like node statistics) in the plot
  varlen = 0,     # Length of variable names (0 means auto-determined)
  faclen = 0     # Length of factor levels displayed on the plot (increase as needed)
)
```

We will build a random forest first.

```{r set seed, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(989) # Set a seed for reproducibility
```

Split the data
```{r}


# Split the data into training and testing subsets
sample_indices <- sample(1:nrow(data_mult), 0.8*nrow(data_mult)) # 80% training, 20% testing
train_data <- data_mult[sample_indices, ]
test_data <- data_mult[-sample_indices, ]

```

Building the untuned model.

```{r untuned ger, echo=TRUE, message=FALSE, warning=FALSE}
# Untuned Model with importance (permutation) option set
multUntuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:87], # FLAGGED: adapt
  num.trees = 500,
  importance = "permutation"
)

predictions <- predict(multUntuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(multUntuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)
```

Set the parameters for the random forest.

```{r settings for RF mult, echo=TRUE, message=FALSE, warning=FALSE}
# Define the number of CPU cores to use
num_cores <- detectCores()

# Create a cluster with specified number of cores
cl <- makeCluster(num_cores)
```

Tuning the random forest.

```{r tuning RF ger, message=FALSE, warning=FALSE}
tuneMult <- makeClassifTask(data = data_mult[,0:88], # FLAGGED: adapt
                           target = "correction_info")

tuneMult <- tuneRanger(tuneMult,
                      measure = list(multiclass.brier),
                      num.trees = 500)

#Return hyperparameter values
tuneMult
## Ola: ALWAYS REPORT THOSE BECAUSE THEY ARE DIFFERENT EVERY TIME BC OF RANDOMIZATION
# Recommended parameter settings: 
#   mtry min.node.size sample.fraction
# 1   71             3       0.8423578
# Results: 
#   multiclass.brier exec.time
# 1        0.7499833     0.17

multTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:87],  # FLAGGED: adapt
  num.trees = 5000, 
  mtry = 71, # Set the recommended mtry value (number of features).
  min.node.size = 3, # Set the recommended min.node.size value (number of samples before a node terminates).
  sample.fraction = 0.8423578, # Set the recommended sample fraction value.(% of data for bagging).
  importance = "permutation" # Permutation is a computationally intensive test.
)

predictions <- predict(multTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(multTuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)


# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```


Create a tuned model only.

```{r tuned model imputed ger, echo=TRUE, message=FALSE, warning=FALSE}
# Create a classification task for tuning
tuneMult <- makeClassifTask(data = train_data[, 0:88], target = "correction_info") # FLAGGED: adapt

# Tune the model
tuneMult <- tuneRanger(tuneMult, measure = list(multiclass.brier), num.trees = 500)

# Return hyperparameter values
tuneMult
# Recommended parameter settings: 
#   mtry min.node.size sample.fraction
# 1    66             3       0.2254135
# Results: 
#   multiclass.brier exec.time
# 1        0.7845583     0.168

# Fit the tuned model on the training data
multTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[, 0:87],  # FLAGGED: adapt
  num.trees = 5000,
  mtry = 66,
  min.node.size = 3,
  sample.fraction = 0.2254135,
  importance = "permutation"
)

# Predict on the test data
predictions <- predict(multTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(multTuned, num.threads = 1, type = 1)

# Convert to data frame
feature_importance_df <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance_df$Feature <- rownames(feature_importance_df)
colnames(feature_importance_df) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance_df[order(-feature_importance_df$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)

# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```


Save  data frame.

```{r save imputed data, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}

write.csv(data_mult, file = paste0(datasets, "multDataXGB.csv"), row.names = FALSE)
```

# XGBoost

Ensure parallel processing.

```{r parallel, echo=TRUE, message=FALSE, warning=FALSE}
# Detect the number of available cores
cores <- detectCores() #- 1  # Leave one core free

# Create a cluster with the detected number of cores
cl <- makeCluster(cores)

# Register the parallel backend
registerDoParallel(cl)
```

Define the grid and estimate runtime.

```{r grid, echo=TRUE, message=FALSE, warning=FALSE}
grid_tune <- expand.grid(
  nrounds = c(5000, 10000), 
  max_depth = c(3, 6), 
  eta = c(0.05, 0.1), 
  gamma = c(0.1), 
  colsample_bytree = c(0.6, 0.8), 
  min_child_weight = c(1), 
  subsample = c(0.75, 1.0)
)

# Calculate total combinations
total_combinations <- nrow(grid_tune)

# Estimate single model run time (assume 1 minute per run)
single_model_time <- 10 # minute

# Total runs for cross-validation
folds <- 5
total_runs <- total_combinations * folds

# Total time estimation without parallel processing
total_time <- total_runs * single_model_time # in minutes

# Convert to hours
total_time_hours <- total_time / 60

# Output estimated time without parallel processing
print(paste("Estimated time for grid search without parallel processing:", total_time_hours, "hours"))

# Parallel processing with 4 cores
cores <- 24
total_time_parallel <- total_time / cores # in minutes

# Convert to hours
total_time_parallel_hours <- total_time_parallel / 60

# Output estimated time with parallel processing
print(paste("Estimated time for grid search with", cores, "cores:", total_time_parallel_hours, "hours"))

rm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)
```

#### K-fold cross-validation

Create subsets to train and test data (80/20).

```{r k-fold subset, echo=TRUE, message=FALSE, warning=FALSE}
# Set seed for reproducibility
set.seed(998)

# Set up train control
train_control <- trainControl(
  method = "cv",        # Cross-validation
  number = 5,           # 5-fold cross-validation
  allowParallel = TRUE  # Enable parallel processing
)

# Define the number of subsets
numSubsets <- 5

# Create an empty list to store subsets
multSubsets <- vector("list", length = numSubsets)

# load MICE imputed data
#gerDataXGB <- read_csv(paste0(datasets, "gerDataXGB.csv"))
multDataXGB <- data_mult
# ensure percProm is factor
multDataXGB$correction_info <- as.factor(multDataXGB$correction_info)
levels(multDataXGB$correction_info)
# only keep the columns of output and predictor variables
#gerDataXGB <- gerDataXGB[,13:52] 

# Calculate the number of samples in each subset
subsetSize <- nrow(multDataXGB) %/% numSubsets

# Randomly assign samples to subsets
for (i in 1:numSubsets) {
  if (i < numSubsets) {
    multSubsets[[i]] <- multDataXGB[sample((1:nrow(multDataXGB)), size = subsetSize), ]
  } else {
    multSubsets[[i]] <- multDataXGB[sample((1:nrow(multDataXGB)), size = subsetSize + (nrow(multDataXGB) %% numSubsets)), ]
  }
}

# Naming the subsets
names(multSubsets) <- paste0("multData", 1:numSubsets)

# Access the subsets (e.g., gerData1, gerData2, etc.)
multData1 <- multSubsets$multData1
multData2 <- multSubsets$multData2
multData3 <- multSubsets$multData3
multData4 <- multSubsets$multData4
multData5 <- multSubsets$multData5

# Combine subsets into 80% groups.
multData1234 <- rbind(multData1, multData2, multData3, multData4)
multData1235 <- rbind(multData1, multData2, multData3, multData5)
multData1245 <- rbind(multData1, multData2, multData4, multData5)
multData1345 <- rbind(multData1, multData3, multData4, multData5)
multData2345 <- rbind(multData2, multData3, multData4, multData5)

```

#### Models

Only run the models one time and then readRDS.

##### Model 1

```{r ger model1, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
multModel1 <- caret::train(
  correction_info ~ .,              
  data = multData1234,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(multModel1, file = paste0(models, "multModel1.rds"), compress = TRUE)
```

##### Model 2

```{r ger model2, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
multModel2 <- caret::train(
  correction_info ~ .,              
  data = multData1235,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(multModel2, file = paste0(models, "multModel2.rds"), compress = TRUE)
```

##### Model 3

```{r ger model3, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
multModel3 <- caret::train(
  correction_info ~ .,              
  data = multData1245,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(multModel3, file = paste0(models, "multModel3.rds"), compress = TRUE)
```

##### Model 4

```{r ger model4, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
multModel4 <- caret::train(
  correction_info ~ .,              
  data = multData1345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)
saveRDS(multModel4, file = paste0(models, "multModel4.rds"), compress = TRUE)
```

##### Model 5

```{r ger model5, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
multModel5 <- caret::train(
  correction_info ~ .,              
  data = multData2345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(multModel5, file = paste0(models, "multModel5.rds"), compress = TRUE)
```

##### Load models

Load all models after running, if necessary.

```{r loas models ger, echo=TRUE, message=FALSE, warning=FALSE}
multModel1 <- readRDS(paste0(models, "multModel1.rds"))
multModel2 <- readRDS(paste0(models, "multModel2.rds"))
multModel3 <- readRDS(paste0(models, "multModel3.rds"))
multModel4 <- readRDS(paste0(models, "multModel4.rds"))
multModel5 <- readRDS(paste0(models, "multModel5.rds"))
```

#### Test models

Generate predictions and confusion matrices

```{r test models ger, echo=TRUE, message=FALSE, warning=FALSE}
# Generate predictions
multPredictions1 <- predict(multModel1, newdata = multData5)
multPredictions2 <- predict(multModel2, newdata = multData4)
multPredictions3 <- predict(multModel3, newdata = multData3)
multPredictions4 <- predict(multModel4, newdata = multData2)
multPredictions5 <- predict(multModel5, newdata = multData1)

# Compute confusion matrices
multCm1 <- confusionMatrix(multPredictions1, multData5$correction_info)
multCm2 <- confusionMatrix(multPredictions2, multData4$correction_info)
multCm3 <- confusionMatrix(multPredictions3, multData3$correction_info)
multCm4 <- confusionMatrix(multPredictions4, multData2$correction_info)
multCm5 <- confusionMatrix(multPredictions5, multData1$correction_info)

# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)
multPValues <- c(multCm1$overall['AccuracyPValue'], 
              multCm2$overall['AccuracyPValue'], 
              multCm3$overall['AccuracyPValue'], 
              multCm4$overall['AccuracyPValue'], 
              multCm5$overall['AccuracyPValue'])
```

Combine p-values using Fisher's method

```{r combine p-vals ger, echo=TRUE, message=FALSE, warning=FALSE}
# Fisher's method
multFisher_combined <- -2 * sum(log(multPValues))
df <- 2 * length(multPValues)
multPCcombined_fisher <- 1 - pchisq(multFisher_combined, df)
print(multPCcombined_fisher)

# Stouffer's method
multZ_scores <- qnorm(1 - multPValues/2)
multCombined_z <- sum(multZ_scores) / sqrt(length(multPValues))
multP_combined_stouffer <- 2 * (1 - pnorm(abs(multCombined_z)))
print(multP_combined_stouffer)
```

The p-values sum up to 0, since they are all so small.

#### Feature importance

##### Model 1

```{r ger feature importance 1, echo=TRUE, message=FALSE, warning=FALSE}
XGBmultModel1 <- multModel1$finalModel
importanceXGBmultModel1 <- xgb.importance(model = XGBmultModel1)
print(importanceXGBmultModel1)
xgb.plot.importance(importanceXGBmultModel1)
```

##### Model 2

```{r ger feature importance 2, echo=TRUE, message=FALSE, warning=FALSE}
XGBmultModel2 <- multModel2$finalModel
importanceXGBmultModel2 <- xgb.importance(model = XGBmultModel2)
print(importanceXGBmultModel2)
xgb.plot.importance(importanceXGBmultModel2)
```

##### Model 3

```{r ger feature importance 3, echo=TRUE, message=FALSE, warning=FALSE}
XGBmultModel3 <- multModel3$finalModel
importanceXGBmultModel3 <- xgb.importance(model = XGBmultModel3)
print(importanceXGBmultModel3)
xgb.plot.importance(importanceXGBmultModel3)
```

##### Model 4

```{r ger feature importance 4, echo=TRUE, message=FALSE, warning=FALSE}
XGBmultModel4 <- multModel4$finalModel
importanceXGBmultModel4 <- xgb.importance(model = XGBmultModel4)
print(importanceXGBmultModel4)
xgb.plot.importance(importanceXGBmultModel4)
```

##### Model 5

```{r ger feature importance 5, echo=TRUE, message=FALSE, warning=FALSE}
XGBmultModel5 <- multModel5$finalModel
importanceXGBmultModel5 <- xgb.importance(model = XGBmultModel5)
print(importanceXGBmultModel5)
xgb.plot.importance(importanceXGBmultModel5)
```

##### Cumulative feature importance

```{r ger cumulative importance, echo=TRUE, message=FALSE, warning=FALSE}
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
  importance <- xgb.importance(model = model)
  importance$Gain <- importance$Gain / sum(importance$Gain)
  return(importance)
}

# Extract normalized importance for each model
multImportance1 <- get_normalized_importance(multModel1$finalModel)
multImportance2 <- get_normalized_importance(multModel2$finalModel)
multImportance3 <- get_normalized_importance(multModel3$finalModel)
multImportance4 <- get_normalized_importance(multModel4$finalModel)
multImportance5 <- get_normalized_importance(multModel5$finalModel)

# Combine importances
multAllImportances <- list(multImportance1, multImportance2, multImportance3, multImportance4, multImportance5)

# Function to merge importances
merge_importances <- function(importances) {
  for (i in 2:length(importances)) {
    names(importances[[i]])[2:4] <- paste0(names(importances[[i]])[2:4], "_", i)
  }
  merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
  merged[is.na(merged)] <- 0  # Replace NAs with 0
  gain_cols <- grep("Gain", colnames(merged), value = TRUE)
  merged$Cumulative <- rowSums(merged[, ..gain_cols])
  return(merged[, .(Feature, Cumulative)])
}

# Merge and sort importances
multCumulativeImportance <- merge_importances(multAllImportances)
multCumulativeImportance <- multCumulativeImportance[order(-multCumulativeImportance$Cumulative), ]

# Print cumulative feature importance
print(multCumulativeImportance)
```



