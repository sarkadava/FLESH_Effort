---
title: "Exploratory Analysis II: Identifying effort-related features contributing to misunderstanding resolution"
format:
  html:
    code-overflow: wrap
    code-width: 1200  # Adjust the width in pixels
execute:
   warning: false 
---

## Overview {#sec-xgboost}

This script is an adaption of pipeline by @cwiek_etal_inprep2.

In this script, we are going to use eXtreme Gradient Boosting (XGBoost, @chen_guestrin16) to identify effort-related features beyond those investigated within our confirmatory analysis, i.e., torque change, amplitude envelope, and change in center of pressure.

XGBoost is a machine learning algorithm that builds an ensemble of decision trees to predict an outcome based on input features. For us, outcome refers to *communicative attempt* (baseline, first correction, second correction) and input features refer to all *features of effort* collected in @sec-features.

We are going to use separate models for each modality, as we expect differences in how (significantly) features contribute to resolving misunderstanding in first and second correction. Finally, when having list of features ordered by their importance in predicting communicative attempt, we will combine the XGBoost-computed importance with PCA analysis we have performed in script @sec-pca. This is to ensure we pick features from uncorrelated dimensions and thus cover more explanatory planes of effort.

Note that the current version of the script is used with data only from dyad 0. Since this is not sufficient amount of data for any meaningful conclusions, this script serves for building the workflow. We will use identical pipeline with the full dataset, and any deviations from this script will be reported.

```{r source setup, echo = TRUE, message=FALSE, warning = FALSE}
#| code-fold: true
#| code-summary: Code to prepare the environment

parentfolder <- dirname(getwd())

datasets      <- paste0(parentfolder, '/08_Analysis_XGBoost/datasets/')
models        <- paste0(parentfolder, '/08_Analysis_XGBoost/models/')
plots         <- paste0(parentfolder, '/08_Analysis_XGBoost/plots/')

# Packages 
library(tibble) # Data Manipulation
library(stringr)
library(tidyverse) # includes readr, tidyr, dplyr, ggplot2
library(data.table)
 
library(ggforce) # Plotting
library(ggpubr)
library(gridExtra)

library(rpart) # Random Forests and XGBoost
library(rpart.plot)
library(ranger)
library(tuneRanger)
library(caret)
library(xgboost)
library(parallel)
library(mice)
library(doParallel)

# Use all available cores for parallel computing
options(mc.cores = parallel::detectCores())

colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                       "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

```

# Gesture

In the previous script (@sec-pca), we have already separated trials into modalities and clean the resulting dataframes of superfluous columns so now we can directly load them and continue modelling.

```{r gesture data}

# Load in
data_ges <- read_csv(paste0(datasets, "ges_clean_df.csv"))

# Make predictor a factor variable
data_ges$correction_info <- as.factor(data_ges$correction_info)

# Display 
head(data_ges, n=10)
```

## Random forests

We will build a random forest first.

```{r ges random forest}

# prepare predictors
predictors <- setdiff(names(data_ges), "correction_info")

formula_str <- paste("correction_info ~", paste(predictors, collapse = " + "))

# Convert the formula string to a formula object
gesTree_formula <- as.formula(formula_str)

# Now use the formula in rpart
gesTree <- rpart(formula = gesTree_formula, data = data_ges, 
                method='class', # Specify that it's a classification tree
                control = rpart.control(maxdepth = 5)  # Control parameters for the 'rpart' function
)

prp(
  gesTree,         # The decision tree object to be visualized
  extra = 1,      # Show extra information (like node statistics) in the plot
  varlen = 0,     # Length of variable names (0 means auto-determined)
  faclen = 0     # Length of factor levels displayed on the plot (increase as needed)
)
```

```{r ges set seed, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}

set.seed(998) # Set a seed for reproducibility
```

Split the data

```{r ges split the data, echo=TRUE, message=FALSE, warning=FALSE}

# This method should ensure that all levels of our dependent variable are present in both sets
# Ensure each level is present in both sets
train_data <- data_ges %>%
  group_by(correction_info) %>%
  sample_frac(0.8, replace = FALSE) %>%
  ungroup()

# Assign the remaining samples to the test set
test_data <- anti_join(data_ges, train_data)

```

Building the untuned model.

```{r untuned ges, echo=TRUE, message=FALSE, warning=FALSE}

# Untuned Model with importance (permutation) option set
gesUntuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:324],
  num.trees = 500,
  importance = "permutation"
)

predictions <- predict(gesUntuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(gesUntuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
head(sorted_feature_importance, n=10)
```

Set the parameters for the random forest.

```{r settings for RF ges, echo=TRUE, message=FALSE, warning=FALSE}

# Define the number of CPU cores to use
num_cores <- detectCores()

# Create a cluster with specified number of cores
cl <- makeCluster(num_cores)
```

Tuning the random forest.

```{r tuning RF ges, message=FALSE, warning=FALSE}

tuneGes <- makeClassifTask(data = data_ges[,0:325],
                           target = "correction_info")

tuneGes <- tuneRanger(tuneGes,
                      measure = list(multiclass.brier),
                      num.trees = 500)

# Return hyperparameter values
#tuneGes

# Recommended parameter settings: 
#   mtry min.node.size sample.fraction
# 1   57             4       0.2279307
# Results: 
#   multiclass.brier exec.time
# 1         0.790973     0.164

gesTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:324], 
  num.trees = 5000, 
  mtry = 57, # Set the recommended mtry value (number of features).
  min.node.size = 4, # Set the recommended min.node.size value (number of samples before a node terminates).
  sample.fraction = 0.2279307, # Set the recommended sample fraction value.(% of data for bagging).
  importance = "permutation" # Permutation is a computationally intensive test.
)

predictions <- predict(gesTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(gesTuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
head(sorted_feature_importance, n=10)


# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```

Create a tuned model only.

```{r tuned model imputed ges, echo=TRUE, message=FALSE, warning=FALSE}

# Create a classification task for tuning
tuneGes <- makeClassifTask(data = train_data[, 0:325], target = "correction_info")

# Tune the model
tuneGes <- tuneRanger(tuneGes, measure = list(multiclass.brier), num.trees = 500)

# Return hyperparameter values
#tuneGes

# Recommended parameter settings: 
#   mtry min.node.size sample.fraction
# 1  221             3       0.2056429
# Results: 
#   multiclass.brier exec.time
# 1        0.8124712     0.168

# Fit the tuned model on the training data
gesTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[, 0:324],
  num.trees = 5000,
  mtry = 221,
  min.node.size = 3,
  sample.fraction = 0.2056429,
  importance = "permutation"
)

# Predict on the test data
predictions <- predict(gesTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(gesTuned, num.threads = 1, type = 1)

# Convert to data frame
feature_importance_df <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance_df$Feature <- rownames(feature_importance_df)
colnames(feature_importance_df) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance_df[order(-feature_importance_df$Importance), ]

# Print sorted feature importance
head(sorted_feature_importance, n=10)

# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```

Save data frame.

```{r save imputed data, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}

write.csv(data_ges, file = paste0(datasets, "gesDataXGB.csv"), row.names = FALSE)
```

## XGBoost

Ensure parallel processing.

```{r parallel, echo=TRUE, message=FALSE, warning=FALSE}

# Detect the number of available cores
cores <- detectCores() #- 1  # Leave one core free

# Create a cluster with the detected number of cores
cl <- makeCluster(cores)

# Register the parallel backend
registerDoParallel(cl)
```

Define the grid and estimate runtime.

```{r grid, echo=TRUE, message=FALSE, warning=FALSE}

grid_tune <- expand.grid(
  nrounds = c(5000, 10000), 
  max_depth = c(3, 6), 
  eta = c(0.05, 0.1), 
  gamma = c(0.1), 
  colsample_bytree = c(0.6, 0.8), 
  min_child_weight = c(1), 
  subsample = c(0.75, 1.0)
)

# Calculate total combinations
total_combinations <- nrow(grid_tune)

# Estimate single model run time (assume 1 minute per run)
single_model_time <- 10 # minute

# Total runs for cross-validation
folds <- 5
total_runs <- total_combinations * folds

# Total time estimation without parallel processing
total_time <- total_runs * single_model_time # in minutes

# Convert to hours
total_time_hours <- total_time / 60

# Output estimated time without parallel processing
print(paste("Estimated time for grid search without parallel processing:", total_time_hours, "hours"))

# Parallel processing with 4 cores
cores <- 24
total_time_parallel <- total_time / cores # in minutes

# Convert to hours
total_time_parallel_hours <- total_time_parallel / 60

# Output estimated time with parallel processing
print(paste("Estimated time for grid search with", cores, "cores:", total_time_parallel_hours, "hours"))

rm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)

```

#### K-fold cross-validation

Create subsets to train and test data (80/20).

```{r ges subsetting}

# Set seed for reproducibility
set.seed(998)

# Set up train control
train_control <- trainControl(
  method = "cv",        # Cross-validation
  number = 5,           # 5-fold cross-validation
  allowParallel = TRUE  # Enable parallel processing
)

# Define the number of subsets
numSubsets <- 5

# Load data if needed
gesDataXGB <- read_csv(paste0(datasets, "gesDataXGB.csv"))

# Ensure 'correction_info' is a factor
gesDataXGB$correction_info <- as.factor(gesDataXGB$correction_info)

# Remove rows with only NA values
gesDataXGB <- gesDataXGB[rowSums(is.na(gesDataXGB)) < ncol(gesDataXGB), ]

# Split data by levels of 'correction_info'
correction_levels <- levels(gesDataXGB$correction_info)
split_data <- split(gesDataXGB, gesDataXGB$correction_info)

# Initialize a list to store subsets
gesSubsets <- vector("list", length = numSubsets)

# Distribute rows for each level equally across subsets
for (level in correction_levels) {
  level_data <- split_data[[level]]
  subset_sizes <- rep(floor(nrow(level_data) / numSubsets), numSubsets)
  remainder <- nrow(level_data) %% numSubsets
  
  # Distribute remainder rows randomly
  if (remainder > 0) {
    subset_sizes[seq_len(remainder)] <- subset_sizes[seq_len(remainder)] + 1
  }
  
  # Shuffle rows of the level and assign to subsets
  shuffled_data <- level_data[sample(nrow(level_data)), ]
  indices <- cumsum(c(0, subset_sizes))
  
  for (i in 1:numSubsets) {
    if (is.null(gesSubsets[[i]])) {
      gesSubsets[[i]] <- shuffled_data[(indices[i] + 1):indices[i + 1], ]
    } else {
      gesSubsets[[i]] <- rbind(gesSubsets[[i]], shuffled_data[(indices[i] + 1):indices[i + 1], ])
    }
  }
}

# Naming the subsets
names(gesSubsets) <- paste0("gesData", 1:numSubsets)

# Verify balance in subsets
for (i in 1:numSubsets) {
  cat("Subset", i, "contains rows:", nrow(gesSubsets[[i]]), "and levels:\n")
  print(table(gesSubsets[[i]]$correction_info))
}

# Remove any rows with only NAs from subsets just to ensure cleanliness
gesSubsets <- lapply(gesSubsets, function(subset) {
  subset[rowSums(is.na(subset)) < ncol(subset), ]
})

# Access the subsets
gesData1 <- gesSubsets$gesData1
gesData2 <- gesSubsets$gesData2
gesData3 <- gesSubsets$gesData3
gesData4 <- gesSubsets$gesData4
gesData5 <- gesSubsets$gesData5

# Combine subsets into 80% groups
gesData1234 <- rbind(gesData1, gesData2, gesData3, gesData4)
gesData1235 <- rbind(gesData1, gesData2, gesData3, gesData5)
gesData1245 <- rbind(gesData1, gesData2, gesData4, gesData5)
gesData1345 <- rbind(gesData1, gesData3, gesData4, gesData5)
gesData2345 <- rbind(gesData2, gesData3, gesData4, gesData5)

# Final verification of all levels in the combined datasets
combined_sets <- list(gesData1234, gesData1235, gesData1245, gesData1345, gesData2345)
names(combined_sets) <- c("gesData1234", "gesData1235", "gesData1245", "gesData1345", "gesData2345")

for (set_name in names(combined_sets)) {
  cat("Dataset", set_name, "contains rows:", nrow(combined_sets[[set_name]]), "and levels:\n")
  print(table(combined_sets[[set_name]]$correction_info))
}

```

### Models

Only run the models one time and then readRDS.

#### Model 1

```{r ges model1, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel1 <- caret::train(
  correction_info ~ .,              
  data = gesData1234,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel1, file = paste0(models, "gesModel1.rds"), compress = TRUE)
```

#### Model 2

```{r ges model2, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel2 <- caret::train(
  correction_info ~ .,              
  data = gesData1235,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel2, file = paste0(models, "gesModel2.rds"), compress = TRUE)
```

#### Model 3

```{r ges model3, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel3 <- caret::train(
  correction_info ~ .,              
  data = gesData1245,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel3, file = paste0(models, "gesModel3.rds"), compress = TRUE)
```

#### Model 4

```{r ges model4, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel4 <- caret::train(
  correction_info ~ .,              
  data = gesData1345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)
saveRDS(gesModel4, file = paste0(models, "gesModel4.rds"), compress = TRUE)
```

#### Model 5

```{r ges model5, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel5 <- caret::train(
  correction_info ~ .,              
  data = gesData2345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel5, file = paste0(models, "gesModel5.rds"), compress = TRUE)
```

#### Load models

Load all models after running, if necessary.

```{r loas models ges, echo=TRUE, message=FALSE, warning=FALSE}
#| warning: false

gesModel1 <- readRDS(paste0(models, "gesModel1.rds"))
gesModel2 <- readRDS(paste0(models, "gesModel2.rds"))
gesModel3 <- readRDS(paste0(models, "gesModel3.rds"))
gesModel4 <- readRDS(paste0(models, "gesModel4.rds"))
gesModel5 <- readRDS(paste0(models, "gesModel5.rds"))

```

#### Test models

Generate predictions and confusion matrices

```{r test models ges, echo=TRUE, message=FALSE, warning=FALSE}

# Generate predictions
gesPredictions1 <- predict(gesModel1, newdata = gesData5)
gesPredictions2 <- predict(gesModel2, newdata = gesData4)
gesPredictions3 <- predict(gesModel3, newdata = gesData3)
gesPredictions4 <- predict(gesModel4, newdata = gesData2)
gesPredictions5 <- predict(gesModel5, newdata = gesData1)

# Compute confusion matrices
gesCm1 <- confusionMatrix(gesPredictions1, gesData5$correction_info)
gesCm2 <- confusionMatrix(gesPredictions2, gesData4$correction_info)
gesCm3 <- confusionMatrix(gesPredictions3, gesData3$correction_info)
gesCm4 <- confusionMatrix(gesPredictions4, gesData2$correction_info)
gesCm5 <- confusionMatrix(gesPredictions5, gesData1$correction_info)

# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)
gesPValues <- c(gesCm1$overall['AccuracyPValue'], 
              gesCm2$overall['AccuracyPValue'], 
              gesCm3$overall['AccuracyPValue'], 
              gesCm4$overall['AccuracyPValue'], 
              gesCm5$overall['AccuracyPValue'])
```

Combine p-values using Fisher's method

```{r combine p-vals ges, echo=TRUE, message=FALSE, warning=FALSE}
# Fisher's method
gesFisher_combined <- -2 * sum(log(gesPValues))
df <- 2 * length(gesPValues)
gesPCcombined_fisher <- 1 - pchisq(gesFisher_combined, df)
print(gesPCcombined_fisher)

# Stouffer's method
gesZ_scores <- qnorm(1 - gesPValues/2)
gesCombined_z <- sum(gesZ_scores) / sqrt(length(gesPValues))
gesP_combined_stouffer <- 2 * (1 - pnorm(abs(gesCombined_z)))
print(gesP_combined_stouffer)
```

The p-values should sum up to 0. Currently we do not have enough data.

#### Feature importance

##### Model 1

```{r ges feature importance 1, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel1 <- gesModel1$finalModel
importanceXGBgesModel1 <- xgb.importance(model = XGBgesModel1)
head(importanceXGBgesModel1, n=10)
xgb.plot.importance(importanceXGBgesModel1)
```

##### Model 2

```{r ges feature importance 2, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel2 <- gesModel2$finalModel
importanceXGBgesModel2 <- xgb.importance(model = XGBgesModel2)
head(importanceXGBgesModel2, n=10)
xgb.plot.importance(importanceXGBgesModel2)
```

##### Model 3

```{r ges feature importance 3, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel3 <- gesModel3$finalModel
importanceXGBgesModel3 <- xgb.importance(model = XGBgesModel3)
head(importanceXGBgesModel3, n=10)
xgb.plot.importance(importanceXGBgesModel3)
```

##### Model 4

```{r ges feature importance 4, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel4 <- gesModel4$finalModel
importanceXGBgesModel4 <- xgb.importance(model = XGBgesModel4)
head(importanceXGBgesModel4, n=10)
xgb.plot.importance(importanceXGBgesModel4)
```

##### Model 5

```{r ges feature importance 5, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel5 <- gesModel5$finalModel
importanceXGBgesModel5 <- xgb.importance(model = XGBgesModel5)
head(importanceXGBgesModel5, n=10)
xgb.plot.importance(importanceXGBgesModel5)
```

#### Cumulative feature importance

```{r ges cumulative importance, echo=TRUE, message=FALSE, warning=FALSE}
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
  importance <- xgb.importance(model = model)
  importance$Gain <- importance$Gain / sum(importance$Gain)
  return(importance)
}

# Extract normalized importance for each model
gesImportance1 <- get_normalized_importance(gesModel1$finalModel)
gesImportance2 <- get_normalized_importance(gesModel2$finalModel)
gesImportance3 <- get_normalized_importance(gesModel3$finalModel)
gesImportance4 <- get_normalized_importance(gesModel4$finalModel)
gesImportance5 <- get_normalized_importance(gesModel5$finalModel)

# Combine importances
gesAllImportances <- list(gesImportance1, gesImportance2, gesImportance3, gesImportance4, gesImportance5)

# Function to merge importances
merge_importances <- function(importances) {
  for (i in 2:length(importances)) {
    names(importances[[i]])[2:4] <- paste0(names(importances[[i]])[2:4], "_", i)
  }
  merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
  merged[is.na(merged)] <- 0  # Replace NAs with 0
  gain_cols <- grep("Gain", colnames(merged), value = TRUE)
  merged$Cumulative <- rowSums(merged[, ..gain_cols])
  return(merged[, .(Feature, Cumulative)])
}

# Merge and sort importances
gesCumulativeImportance <- merge_importances(gesAllImportances)
gesCumulativeImportance <- gesCumulativeImportance[order(-gesCumulativeImportance$Cumulative), ]

# Print cumulative feature importance
head(gesCumulativeImportance, n=10)
```

#### PCA

Now, to select features along different (uncorrelated) dimensions, we want to connect these results with the results of PCA we performed in script @sec-pca.

```{r ges pca function}
#| code-fold: true
#| code-summary: Custom function

# Function to select top 3 features per component
select_top_features <- function(pc_column, xgb_importance, top_n = 3) {

  # Find common features ranked by XGBoost importance
  common_features <- intersect(pc_column, xgb_importance$Feature)
  common_features <- xgb_importance %>%
    filter(Feature %in% common_features) %>%
    arrange(Rank) %>%
    pull(Feature)
  return(head(common_features, top_n))
}

```

First, we collect 10 features per component that have highest combined ranking from PCA and XGBoost. This means that for each feature we sum up the ranking it obtained in cumulative importance (XGBoost) and loading on a principal component (PCA).

```{r ges pca pick}

# Rank the features based on XGBoost importance (cumulative)
gesCumulativeImportance$XGB_Rank <- rank(-gesCumulativeImportance$Cumulative)

# Load in PCA for gesture
ges_pca <- read_csv(paste0(datasets, "PCA_top_contributors_ges.csv"))

# For each PC (PC1, PC2, PC3), rank the features based on their loadings
combined_ranks_per_pc <- list()

for (pc in c("PC1", "PC2", "PC3")) {
  # Extract the features and loadings for the current PC
  pca_pc_loadings <- ges_pca[, c(pc, paste0(pc, "_Loading"))]
  colnames(pca_pc_loadings) <- c("Feature", "Loading")
  
  # Rank the features based on the absolute loading values (higher loadings should get lower rank)
  pca_pc_loadings$PCA_Rank <- rank(-abs(pca_pc_loadings$Loading))
  
  # Merge PCA loadings with XGBoost importance ranks
  merged_data <- merge(pca_pc_loadings, gesCumulativeImportance[, c("Feature", "XGB_Rank")], by = "Feature")
  
  # Calculate combined rank by summing XGBoost rank and PCA rank
  merged_data$Combined_Rank <- merged_data$XGB_Rank + merged_data$PCA_Rank
  
  # Sort by the combined rank (lower rank is better)
  sorted_data <- merged_data[order(merged_data$Combined_Rank), ]
  
  # Select the top n features based on the combined rank for the current PC
  top_n_features <- 10  # Adjust the number of top features as needed
  combined_ranks_per_pc[[pc]] <- head(sorted_data, top_n_features)
}

# Output the top features per PC based on combined ranking
head(combined_ranks_per_pc, n=10)

```

For modelling, we want to pick only three features per component. Which would it be in this case?

```{r ges pca 3 feat}

# Number of top features to display
top_n_features <- 3

# Print the top 3 features per component
for (pc in c("PC1", "PC2", "PC3")) {
  cat("\nTop 3 Features for", pc, ":\n")
  
  # Get the top 3 features based on combined rank for the current PC
  top_features <- head(combined_ranks_per_pc[[pc]], top_n_features)
  
  # Print the results
  print(top_features[, c("Feature", "XGB_Rank", "PCA_Rank", "Combined_Rank")])
}

```

# Vocalization

Now, we just repeat the whole workflow using data for vocalization-only trials.

```{r voc data}

# Load in
data_voc <- read_csv(paste0(datasets, "voc_clean_df.csv"))

# Make predictor a factor variable
data_voc$correction_info <- as.factor(data_voc$correction_info)

# Display
head(data_voc, n=10)
```

## Random forests

We first run random forests.

```{r voc random forest}

# prepare predictors
predictors <- setdiff(names(data_voc), "correction_info")

formula_str <- paste("correction_info ~", paste(predictors, collapse = " + "))

# Convert the formula string to a formula object
vocTree_formula <- as.formula(formula_str)

# Now use the formula in rpart
vocTree <- rpart(formula = vocTree_formula, data = data_voc, 
                method='class', # Specify that it's a classification tree
                control = rpart.control(maxdepth = 5)  # Control parameters for the 'rpart' function
)

prp(
  vocTree,         # The decision tree object to be visualized
  extra = 1,      # Show extra information (like node statistics) in the plot
  varlen = 0,     # Length of variable names (0 means auto-determined)
  faclen = 0     # Length of factor levels displayed on the plot (increase as needed)
)
```

```{r voc set seed, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}

set.seed(995) # Set a seed for reproducibility

```

Split the data

```{r voc split, echo=FALSE, message=FALSE}

# This method should ensure that all levels of our dependent variable are present in both sets
# Ensure each level is present in both sets
train_data <- data_voc %>%
  group_by(correction_info) %>%
  sample_frac(0.8, replace = FALSE) %>%
  ungroup()

# Assign the remaining samples to the test set
test_data <- anti_join(data_voc, train_data)

```

Building the untuned model.

```{r untuned voc, echo=TRUE, message=FALSE, warning=FALSE}

# Untuned Model with importance (permutation) option set
vocUntuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:70], # without outcome var
  num.trees = 500,
  importance = "permutation"
)

predictions <- predict(vocUntuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(vocUntuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
head(sorted_feature_importance, n=10)
```

Set the parameters for the random forest.

```{r settings for RF voc, echo=TRUE, message=FALSE, warning=FALSE}
# Define the number of CPU cores to use
num_cores <- detectCores()

# Create a cluster with specified number of cores
cl <- makeCluster(num_cores)
```

Tuning the random forest.

```{r tuning RF voc, message=FALSE, warning=FALSE}
tuneVoc <- makeClassifTask(data = data_voc[,0:71], # with OV
                           target = "correction_info")

tuneVoc <- tuneRanger(tuneVoc,
                      measure = list(multiclass.brier),
                      num.trees = 500)

#Return hyperparameter values
#tuneVoc

# Recommended parameter settings: 
# mtry min.node.size sample.fraction
# 1    6             4        0.522745
# Results: 
#   multiclass.brier exec.time
# 1        0.7256575     0.166

vocTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:70],  #without OV
  num.trees = 5000, 
  mtry = 6, # Set the recommended mtry value (number of features).
  min.node.size = 4, # Set the recommended min.node.size value (number of samples before a node terminates).
  sample.fraction = 0.522745, # Set the recommended sample fraction value.(% of data for bagging).
  importance = "permutation" # Permutation is a computationally intensive test.
)

predictions <- predict(vocTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(vocTuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
head(sorted_feature_importance, n=10)


# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```

Create a tuned model only.

```{r tuned model imputed voc, echo=TRUE, message=FALSE, warning=FALSE}
# Create a classification task for tuning
tuneVoc <- makeClassifTask(data = train_data[, 0:71], target = "correction_info") #with OV

# Tune the model
tuneVoc <- tuneRanger(tuneVoc, measure = list(multiclass.brier), num.trees = 500)

# Return hyperparameter values
#tuneVoc
# Recommended parameter settings: 
#   mtry min.node.size sample.fraction
# 1    3             3       0.5835222
# Results: 
#   multiclass.brier exec.time
# 1        0.7457527      0.16

# Fit the tuned model on the training data
vocTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[, 0:70], # without OV
  num.trees = 5000,
  mtry = 3,
  min.node.size = 3,
  sample.fraction = 0.5835222,
  importance = "permutation"
)

# Predict on the test data
predictions <- predict(vocTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(vocTuned, num.threads = 1, type = 1)

# Convert to data frame
feature_importance_df <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance_df$Feature <- rownames(feature_importance_df)
colnames(feature_importance_df) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance_df[order(-feature_importance_df$Importance), ]

# Print sorted feature importance
head(sorted_feature_importance, n=10)

# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```

Save data frame.

```{r voc save imputed data, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}

write.csv(data_voc, file = paste0(datasets, "vocDataXGB.csv"), row.names = FALSE)
```

## XGBoost

Ensure parallel processing.

```{r voc parallel, echo=TRUE, message=FALSE, warning=FALSE}
# Detect the number of available cores
cores <- detectCores() #- 1  # Leave one core free

# Create a cluster with the detected number of cores
cl <- makeCluster(cores)

# Register the parallel backend
registerDoParallel(cl)
```

Define the grid and estimate runtime.

```{r voc grid, echo=TRUE, message=FALSE, warning=FALSE}
grid_tune <- expand.grid(
  nrounds = c(5000, 10000), 
  max_depth = c(3, 6), 
  eta = c(0.05, 0.1), 
  gamma = c(0.1), 
  colsample_bytree = c(0.6, 0.8), 
  min_child_weight = c(1), 
  subsample = c(0.75, 1.0)
)

# Calculate total combinations
total_combinations <- nrow(grid_tune)

# Estimate single model run time (assume 1 minute per run)
single_model_time <- 10 # minute

# Total runs for cross-validation
folds <- 5
total_runs <- total_combinations * folds

# Total time estimation without parallel processing
total_time <- total_runs * single_model_time # in minutes

# Convert to hours
total_time_hours <- total_time / 60

# Output estimated time without parallel processing
print(paste("Estimated time for grid search without parallel processing:", total_time_hours, "hours"))

# Parallel processing with 4 cores
cores <- 24
total_time_parallel <- total_time / cores # in minutes

# Convert to hours
total_time_parallel_hours <- total_time_parallel / 60

# Output estimated time with parallel processing
print(paste("Estimated time for grid search with", cores, "cores:", total_time_parallel_hours, "hours"))

rm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)
```

### K-fold cross-validation

Create subsets to train and test data (80/20).

```{r voc k-fold subset, echo=TRUE, message=FALSE, warning=FALSE}
# Set seed for reproducibility
set.seed(998)

# Set up train control
train_control <- trainControl(
  method = "cv",        # Cross-validation
  number = 5,           # 5-fold cross-validation
  allowParallel = TRUE  # Enable parallel processing
)


# Define the number of subsets
numSubsets <- 5

# Load MICE-imputed data (using placeholder 'data_ges' as the input dataset)
vocDataXGB <- read_csv(paste0(datasets, "vocDataXGB.csv"))

# Ensure 'correction_info' is a factor
vocDataXGB$correction_info <- as.factor(vocDataXGB$correction_info)

# Remove rows with only NA values
vocDataXGB <- vocDataXGB[rowSums(is.na(vocDataXGB)) < ncol(vocDataXGB), ]

# Split data by levels of 'correction_info'
correction_levels <- levels(vocDataXGB$correction_info)
split_data <- split(vocDataXGB, vocDataXGB$correction_info)

# Initialize a list to store subsets
vocSubsets <- vector("list", length = numSubsets)

# Distribute rows for each level equally across subsets
for (level in correction_levels) {
  level_data <- split_data[[level]]
  subset_sizes <- rep(floor(nrow(level_data) / numSubsets), numSubsets)
  remainder <- nrow(level_data) %% numSubsets
  
  # Distribute remainder rows randomly
  if (remainder > 0) {
    subset_sizes[seq_len(remainder)] <- subset_sizes[seq_len(remainder)] + 1
  }
  
  # Shuffle rows of the level and assign to subsets
  shuffled_data <- level_data[sample(nrow(level_data)), ]
  indices <- cumsum(c(0, subset_sizes))
  
  for (i in 1:numSubsets) {
    if (is.null(vocSubsets[[i]])) {
      vocSubsets[[i]] <- shuffled_data[(indices[i] + 1):indices[i + 1], ]
    } else {
      vocSubsets[[i]] <- rbind(vocSubsets[[i]], shuffled_data[(indices[i] + 1):indices[i + 1], ])
    }
  }
}

# Naming the subsets
names(vocSubsets) <- paste0("vocData", 1:numSubsets)

# Verify balance in subsets
for (i in 1:numSubsets) {
  cat("Subset", i, "contains rows:", nrow(vocSubsets[[i]]), "and levels:\n")
  print(table(vocSubsets[[i]]$correction_info))
}

# Remove any rows with only NAs from subsets just to ensure cleanliness
vocSubsets <- lapply(vocSubsets, function(subset) {
  subset[rowSums(is.na(subset)) < ncol(subset), ]
})

# Access the subsets
vocData1 <- vocSubsets$vocData1
vocData2 <- vocSubsets$vocData2
vocData3 <- vocSubsets$vocData3
vocData4 <- vocSubsets$vocData4
vocData5 <- vocSubsets$vocData5

# Combine subsets into 80% groups
vocData1234 <- rbind(vocData1, vocData2, vocData3, vocData4)
vocData1235 <- rbind(vocData1, vocData2, vocData3, vocData5)
vocData1245 <- rbind(vocData1, vocData2, vocData4, vocData5)
vocData1345 <- rbind(vocData1, vocData3, vocData4, vocData5)
vocData2345 <- rbind(vocData2, vocData3, vocData4, vocData5)

# Final verification of all levels in the combined datasets
combined_sets <- list(vocData1234, vocData1235, vocData1245, vocData1345, vocData2345)
names(combined_sets) <- c("vocData1234", "vocData1235", "vocData1245", "vocData1345", "vocData2345")

for (set_name in names(combined_sets)) {
  cat("Dataset", set_name, "contains rows:", nrow(combined_sets[[set_name]]), "and levels:\n")
  print(table(combined_sets[[set_name]]$correction_info))
}

```

### Models

#### Model 1

```{r voc model1, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
vocModel1 <- caret::train(
  correction_info ~ .,              
  data = vocData1234,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(vocModel1, file = paste0(models, "vocModel1.rds"), compress = TRUE)
```

#### Model 2

```{r voc model2, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
vocModel2 <- caret::train(
  correction_info ~ .,              
  data = vocData1235,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(vocModel2, file = paste0(models, "vocModel2.rds"), compress = TRUE)
```

#### Model 3

```{r voc model3, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
vocModel3 <- caret::train(
  correction_info ~ .,              
  data = vocData1245,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(vocModel3, file = paste0(models, "vocModel3.rds"), compress = TRUE)
```

#### Model 4

```{r voc model4, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
vocModel4 <- caret::train(
  correction_info ~ .,              
  data = vocData1345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)
saveRDS(vocModel4, file = paste0(models, "vocModel4.rds"), compress = TRUE)
```

#### Model 5

```{r voc model5, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
vocModel5 <- caret::train(
  correction_info ~ .,              
  data = vocData2345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(vocModel5, file = paste0(models, "vocModel5.rds"), compress = TRUE)
```

#### Load models

Load all models after running, if necessary.

```{r loas models voc, echo=TRUE, message=FALSE, warning=FALSE}
vocModel1 <- readRDS(paste0(models, "vocModel1.rds"))
vocModel2 <- readRDS(paste0(models, "vocModel2.rds"))
vocModel3 <- readRDS(paste0(models, "vocModel3.rds"))
vocModel4 <- readRDS(paste0(models, "vocModel4.rds"))
vocModel5 <- readRDS(paste0(models, "vocModel5.rds"))
```

#### Test models

Generate predictions and confusion matrices

```{r test models voc, echo=TRUE, message=FALSE, warning=FALSE}
#| warning: false

# Generate predictions
vocPredictions1 <- predict(vocModel1, newdata = vocData5)
vocPredictions2 <- predict(vocModel2, newdata = vocData4)
vocPredictions3 <- predict(vocModel3, newdata = vocData3)
vocPredictions4 <- predict(vocModel4, newdata = vocData2)
vocPredictions5 <- predict(vocModel5, newdata = vocData1)

# Compute confusion matrices
vocCm1 <- confusionMatrix(vocPredictions1, vocData5$correction_info)
vocCm2 <- confusionMatrix(vocPredictions2, vocData4$correction_info)
vocCm3 <- confusionMatrix(vocPredictions3, vocData3$correction_info)
vocCm4 <- confusionMatrix(vocPredictions4, vocData2$correction_info)
vocCm5 <- confusionMatrix(vocPredictions5, vocData1$correction_info)

# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)
vocPValues <- c(vocCm1$overall['AccuracyPValue'], 
              vocCm2$overall['AccuracyPValue'], 
              vocCm3$overall['AccuracyPValue'], 
              vocCm4$overall['AccuracyPValue'], 
              vocCm5$overall['AccuracyPValue'])
```

Combine p-values using Fisher's method

```{r combine p-vals voc, echo=TRUE, message=FALSE, warning=FALSE}
# Fisher's method
vocFisher_combined <- -2 * sum(log(vocPValues))
df <- 2 * length(vocPValues)
vocPCcombined_fisher <- 1 - pchisq(vocFisher_combined, df)
print(vocPCcombined_fisher)

# Stouffer's method
vocZ_scores <- qnorm(1 - vocPValues/2)
vocCombined_z <- sum(vocZ_scores) / sqrt(length(vocPValues))
vocP_combined_stouffer <- 2 * (1 - pnorm(abs(vocCombined_z)))
print(vocP_combined_stouffer)
```

The p-values should sum up to 0.

#### Feature importance

##### Model 1

```{r voc feature importance 1, echo=TRUE, message=FALSE, warning=FALSE}
XGBvocModel1 <- vocModel1$finalModel
importanceXGBvocModel1 <- xgb.importance(model = XGBvocModel1)
head(importanceXGBvocModel1, n=10)
xgb.plot.importance(importanceXGBvocModel1)
```

##### Model 2

```{r voc feature importance 2, echo=TRUE, message=FALSE, warning=FALSE}
XGBvocModel2 <- vocModel2$finalModel
importanceXGBvocModel2 <- xgb.importance(model = XGBvocModel2)
head(importanceXGBvocModel2, n=10)
xgb.plot.importance(importanceXGBvocModel2)
```

##### Model 3

```{r voc feature importance 3, echo=TRUE, message=FALSE, warning=FALSE}
XGBvocModel3 <- vocModel3$finalModel
importanceXGBvocModel3 <- xgb.importance(model = XGBvocModel3)
head(importanceXGBvocModel3, n=10)
xgb.plot.importance(importanceXGBvocModel3)
```

##### Model 4

```{r voc feature importance 4, echo=TRUE, message=FALSE, warning=FALSE}
XGBvocModel4 <- vocModel4$finalModel
importanceXGBvocModel4 <- xgb.importance(model = XGBvocModel4)
head(importanceXGBvocModel4, n=10)
xgb.plot.importance(importanceXGBvocModel4)
```

##### Model 5

```{r voc feature importance 5, echo=TRUE, message=FALSE, warning=FALSE}
XGBvocModel5 <- vocModel5$finalModel
importanceXGBvocModel5 <- xgb.importance(model = XGBvocModel5)
head(importanceXGBvocModel5, n=10)
xgb.plot.importance(importanceXGBvocModel5)
```

##### Cumulative feature importance

```{r voc cumulative importance, echo=TRUE, message=FALSE, warning=FALSE}
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
  importance <- xgb.importance(model = model)
  importance$Gain <- importance$Gain / sum(importance$Gain)
  return(importance)
}

# Extract normalized importance for each model
vocImportance1 <- get_normalized_importance(vocModel1$finalModel)
vocImportance2 <- get_normalized_importance(vocModel2$finalModel)
vocImportance3 <- get_normalized_importance(vocModel3$finalModel)
vocImportance4 <- get_normalized_importance(vocModel4$finalModel)
vocImportance5 <- get_normalized_importance(vocModel5$finalModel)

# Combine importances
vocAllImportances <- list(vocImportance1, vocImportance2, vocImportance3, vocImportance4, vocImportance5)

# Function to merge importances
merge_importances <- function(importances) {
  for (i in 2:length(importances)) {
    names(importances[[i]])[2:4] <- paste0(names(importances[[i]])[2:4], "_", i)
  }
  merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
  merged[is.na(merged)] <- 0  # Replace NAs with 0
  gain_cols <- grep("Gain", colnames(merged), value = TRUE)
  merged$Cumulative <- rowSums(merged[, ..gain_cols])
  return(merged[, .(Feature, Cumulative)])
}

# Merge and sort importances
vocCumulativeImportance <- merge_importances(vocAllImportances)
vocCumulativeImportance <- vocCumulativeImportance[order(-vocCumulativeImportance$Cumulative), ]

# Print cumulative feature importance
head(vocCumulativeImportance, n=10)
```

#### PCA

Now let's collect 10 features per component that have highest combined ranking from PCA and XGBoost. This means that for each feature we sum up the ranking it obtained in cumulative importance (XGBoost) and loading on a principal component (PCA).

```{r voc pca}

# Rank the features based on XGBoost importance (cumulative)
vocCumulativeImportance$XGB_Rank <- rank(-vocCumulativeImportance$Cumulative)

# Load in PCA for gesture
voc_pca <- read_csv(paste0(datasets, "PCA_top_contributors_voc.csv"))

# For each PC (PC1, PC2, PC3), rank the features based on their loadings
combined_ranks_per_pc <- list()

for (pc in c("PC1", "PC2", "PC3")) {
  # Extract the features and loadings for the current PC
  pca_pc_loadings <- voc_pca[, c(pc, paste0(pc, "_Loading"))]
  colnames(pca_pc_loadings) <- c("Feature", "Loading")
  
  # Rank the features based on the absolute loading values (higher loadings should get lower rank)
  pca_pc_loadings$PCA_Rank <- rank(-abs(pca_pc_loadings$Loading))
  
  # Merge PCA loadings with XGBoost importance ranks
  merged_data <- merge(pca_pc_loadings, vocCumulativeImportance[, c("Feature", "XGB_Rank")], by = "Feature")
  
  # Calculate combined rank by summing XGBoost rank and PCA rank
  merged_data$Combined_Rank <- merged_data$XGB_Rank + merged_data$PCA_Rank
  
  # Sort by the combined rank (lower rank is better)
  sorted_data <- merged_data[order(merged_data$Combined_Rank), ]
  
  # Select the top n features based on the combined rank for the current PC
  top_n_features <- 10  # Adjust the number of top features as needed
  combined_ranks_per_pc[[pc]] <- head(sorted_data, top_n_features)
}

# Output the top features per PC based on combined ranking
head(combined_ranks_per_pc, n=10)

```

 For modelling, we want to pick three features per component. Which would it be in this case?

```{r voc pca pick 3}

# Number of top features to display
top_n_features <- 3

# Print the top 3 features per component
for (pc in c("PC1", "PC2", "PC3")) {
  cat("\nTop 3 Features for", pc, ":\n")
  
  # Get the top 3 features based on combined rank for the current PC
  top_features <- head(combined_ranks_per_pc[[pc]], top_n_features)
  
  # Print the results
  print(top_features[, c("Feature", "XGB_Rank", "PCA_Rank", "Combined_Rank")])
}

```

# Multimodal

Now we take trials from combined modality and follow identical workflow.

```{r mult data}

# Load
data_mult <- read_csv(paste0(datasets, "multi_clean_df.csv"))

# Make predictor a factor variable
data_mult$correction_info <- as.factor(data_mult$correction_info)

# Display
head(data_mult, n=10)

```

## Random forests

We will build a random forest first.

```{r mult random forest}

# prepare predictors
predictors <- setdiff(names(data_mult), "correction_info")

formula_str <- paste("correction_info ~", paste(predictors, collapse = " + "))

# Convert the formula string to a formula object
multTree_formula <- as.formula(formula_str)

# Now use the formula in rpart
multTree <- rpart(formula = multTree_formula, data = data_mult, 
                method='class', # Specify that it's a classification tree
                control = rpart.control(maxdepth = 5)  # Control parameters for the 'rpart' function
)

prp(
  multTree,         # The decision tree object to be visualized
  extra = 1,      # Show extra information (like node statistics) in the plot
  varlen = 0,     # Length of variable names (0 means auto-determined)
  faclen = 0     # Length of factor levels displayed on the plot (increase as needed)
)
```

```{r mult set seed, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
set.seed(966) # Set a seed for reproducibility
```

Split the data

```{r mult split, message=FALSE, echo=FALSE}

# This method should ensure that all levels of our dependent variable are present in both sets
# Ensure each level is present in both sets
train_data <- data_mult %>%
  group_by(correction_info) %>%
  sample_frac(0.8, replace = FALSE) %>%
  ungroup()

# Assign the remaining samples to the test set
test_data <- anti_join(data_mult, train_data)

```

Building the untuned model.

```{r untuned mult, echo=TRUE, message=FALSE, warning=FALSE}
# Untuned Model with importance (permutation) option set
multUntuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:394], # without OV
  num.trees = 500,
  importance = "permutation"
)

predictions <- predict(multUntuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(multUntuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
head(sorted_feature_importance, n=10)
```

Set the parameters for the random forest.

```{r settings for RF mult, echo=TRUE, message=FALSE, warning=FALSE}
# Define the number of CPU cores to use
num_cores <- detectCores()

# Create a cluster with specified number of cores
cl <- makeCluster(num_cores)
```

Tuning the random forest.

```{r tuning RF mult, message=FALSE, warning=FALSE}

tuneMult <- makeClassifTask(data = data_mult[,0:395], # with OV
                           target = "correction_info")

tuneMult <- tuneRanger(tuneMult,
                      measure = list(multiclass.brier),
                      num.trees = 500)

#Return hyperparameter values
#tuneMult

# Recommended parameter settings: 
#   mtry min.node.size sample.fraction
# 1  232             3         0.70919
# Results: 
#   multiclass.brier exec.time
# 1        0.7385408       0.2

multTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:394],  # without OV
  num.trees = 5000, 
  mtry = 232, # Set the recommended mtry value (number of features).
  min.node.size = 3, # Set the recommended min.node.size value (number of samples before a node terminates).
  sample.fraction = 0.70919, # Set the recommended sample fraction value.(% of data for bagging).
  importance = "permutation" # Permutation is a computationally intensive test.
)

predictions <- predict(multTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(multTuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
head(sorted_feature_importance, n=10)


# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```

Create a tuned model only.

```{r tuned model imputed mult, echo=TRUE, message=FALSE, warning=FALSE}
# Create a classification task for tuning
tuneMult <- makeClassifTask(data = train_data[, 0:395], target = "correction_info") # with OV

# Tune the model
tuneMult <- tuneRanger(tuneMult, measure = list(multiclass.brier), num.trees = 500)

# Return hyperparameter values
#tuneMult
# Recommended parameter settings: 
# mtry min.node.size sample.fraction
# 1  268             3       0.3295434
# Results: 
#   multiclass.brier exec.time
# 1        0.8275102     0.172

# Fit the tuned model on the training data
multTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[, 0:394],  # without OV
  num.trees = 5000,
  mtry = 268,
  min.node.size = 3,
  sample.fraction = 0.3295434,
  importance = "permutation"
)

# Predict on the test data
predictions <- predict(multTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(multTuned, num.threads = 1, type = 1)

# Convert to data frame
feature_importance_df <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance_df$Feature <- rownames(feature_importance_df)
colnames(feature_importance_df) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance_df[order(-feature_importance_df$Importance), ]

# Print sorted feature importance
head(sorted_feature_importance, n=10)

# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```

Save data frame.

```{r save imputed data mult, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}

write.csv(data_mult, file = paste0(datasets, "multDataXGB.csv"), row.names = FALSE)
```

## XGBoost

Ensure parallel processing.

```{r mult parallel, echo=TRUE, message=FALSE, warning=FALSE}
# Detect the number of available cores
cores <- detectCores() #- 1  # Leave one core free

# Create a cluster with the detected number of cores
cl <- makeCluster(cores)

# Register the parallel backend
registerDoParallel(cl)
```

Define the grid and estimate runtime.

```{r mult grid, echo=TRUE, message=FALSE, warning=FALSE}
grid_tune <- expand.grid(
  nrounds = c(5000, 10000), 
  max_depth = c(3, 6), 
  eta = c(0.05, 0.1), 
  gamma = c(0.1), 
  colsample_bytree = c(0.6, 0.8), 
  min_child_weight = c(1), 
  subsample = c(0.75, 1.0)
)

# Calculate total combinations
total_combinations <- nrow(grid_tune)

# Estimate single model run time (assume 1 minute per run)
single_model_time <- 10 # minute

# Total runs for cross-validation
folds <- 5
total_runs <- total_combinations * folds

# Total time estimation without parallel processing
total_time <- total_runs * single_model_time # in minutes

# Convert to hours
total_time_hours <- total_time / 60

# Output estimated time without parallel processing
print(paste("Estimated time for grid search without parallel processing:", total_time_hours, "hours"))

# Parallel processing with 4 cores
cores <- 24
total_time_parallel <- total_time / cores # in minutes

# Convert to hours
total_time_parallel_hours <- total_time_parallel / 60

# Output estimated time with parallel processing
print(paste("Estimated time for grid search with", cores, "cores:", total_time_parallel_hours, "hours"))

rm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)
```

### K-fold cross-validation

Create subsets to train and test data (80/20).

```{r mult k-fold subset, echo=TRUE, message=FALSE, warning=FALSE}

# Set seed for reproducibility
set.seed(998)

# Set up train control
train_control <- trainControl(
  method = "cv",        # Cross-validation
  number = 5,           # 5-fold cross-validation
  allowParallel = TRUE  # Enable parallel processing
)

# Define the number of subsets
numSubsets <- 5

# Load MICE-imputed data (using placeholder 'data_ges' as the input dataset)
multDataXGB <- data_mult

# Ensure 'correction_info' is a factor
multDataXGB$correction_info <- as.factor(multDataXGB$correction_info)

# Remove rows with only NA values
multDataXGB <- multDataXGB[rowSums(is.na(multDataXGB)) < ncol(multDataXGB), ]

# Split data by levels of 'correction_info'
correction_levels <- levels(multDataXGB$correction_info)
split_data <- split(multDataXGB, multDataXGB$correction_info)

# Initialize a list to store subsets
multSubsets <- vector("list", length = numSubsets)

# Distribute rows for each level equally across subsets
for (level in correction_levels) {
  level_data <- split_data[[level]]
  subset_sizes <- rep(floor(nrow(level_data) / numSubsets), numSubsets)
  remainder <- nrow(level_data) %% numSubsets
  
  # Distribute remainder rows randomly
  if (remainder > 0) {
    subset_sizes[seq_len(remainder)] <- subset_sizes[seq_len(remainder)] + 1
  }
  
  # Shuffle rows of the level and assign to subsets
  shuffled_data <- level_data[sample(nrow(level_data)), ]
  indices <- cumsum(c(0, subset_sizes))
  
  for (i in 1:numSubsets) {
    if (is.null(multSubsets[[i]])) {
      multSubsets[[i]] <- shuffled_data[(indices[i] + 1):indices[i + 1], ]
    } else {
      multSubsets[[i]] <- rbind(multSubsets[[i]], shuffled_data[(indices[i] + 1):indices[i + 1], ])
    }
  }
}

# Naming the subsets
names(multSubsets) <- paste0("multData", 1:numSubsets)

# Verify balance in subsets
for (i in 1:numSubsets) {
  cat("Subset", i, "contains rows:", nrow(multSubsets[[i]]), "and levels:\n")
  print(table(multSubsets[[i]]$correction_info))
}

# Remove any rows with only NAs from subsets just to ensure cleanliness
multSubsets <- lapply(multSubsets, function(subset) {
  subset[rowSums(is.na(subset)) < ncol(subset), ]
})

# Access the subsets
multData1 <- multSubsets$multData1
multData2 <- multSubsets$multData2
multData3 <- multSubsets$multData3
multData4 <- multSubsets$multData4
multData5 <- multSubsets$multData5

# Combine subsets into 80% groups
multData1234 <- rbind(multData1, multData2, multData3, multData4)
multData1235 <- rbind(multData1, multData2, multData3, multData5)
multData1245 <- rbind(multData1, multData2, multData4, multData5)
multData1345 <- rbind(multData1, multData3, multData4, multData5)
multData2345 <- rbind(multData2, multData3, multData4, multData5)

# Final verification of all levels in the combined datasets
combined_sets <- list(multData1234, multData1235, multData1245, multData1345, multData2345)
names(combined_sets) <- c("multData1234", "multData1235", "multData1245", "multData1345", "multData2345")

for (set_name in names(combined_sets)) {
  cat("Dataset", set_name, "contains rows:", nrow(combined_sets[[set_name]]), "and levels:\n")
  print(table(combined_sets[[set_name]]$correction_info))
}


```

### Models

Only run the models one time and then readRDS.

#### Model 1

```{r mult model1, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
multModel1 <- caret::train(
  correction_info ~ .,              
  data = multData1234,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(multModel1, file = paste0(models, "multModel1.rds"), compress = TRUE)
```

#### Model 2

```{r mult model2, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
multModel2 <- caret::train(
  correction_info ~ .,              
  data = multData1235,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(multModel2, file = paste0(models, "multModel2.rds"), compress = TRUE)
```

#### Model 3

```{r mult model3, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
multModel3 <- caret::train(
  correction_info ~ .,              
  data = multData1245,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(multModel3, file = paste0(models, "multModel3.rds"), compress = TRUE)
```

#### Model 4

```{r mult model4, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
multModel4 <- caret::train(
  correction_info ~ .,              
  data = multData1345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)
saveRDS(multModel4, file = paste0(models, "multModel4.rds"), compress = TRUE)
```

#### Model 5

```{r mult model5, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
multModel5 <- caret::train(
  correction_info ~ .,              
  data = multData2345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(multModel5, file = paste0(models, "multModel5.rds"), compress = TRUE)
```

#### Load models

Load all models after running, if necessary.

```{r load models mult, echo=TRUE, message=FALSE, warning=FALSE}
multModel1 <- readRDS(paste0(models, "multModel1.rds"))
multModel2 <- readRDS(paste0(models, "multModel2.rds"))
multModel3 <- readRDS(paste0(models, "multModel3.rds"))
multModel4 <- readRDS(paste0(models, "multModel4.rds"))
multModel5 <- readRDS(paste0(models, "multModel5.rds"))
```

#### Test models

Generate predictions and confusion matrices

```{r test models mult, echo=TRUE, message=FALSE, warning=FALSE}
#| warning: false
#| 
# Generate predictions
multPredictions1 <- predict(multModel1, newdata = multData5)
multPredictions2 <- predict(multModel2, newdata = multData4)
multPredictions3 <- predict(multModel3, newdata = multData3)
multPredictions4 <- predict(multModel4, newdata = multData2)
multPredictions5 <- predict(multModel5, newdata = multData1)

# Compute confusion matrices
multCm1 <- confusionMatrix(multPredictions1, multData5$correction_info)
multCm2 <- confusionMatrix(multPredictions2, multData4$correction_info)
multCm3 <- confusionMatrix(multPredictions3, multData3$correction_info)
multCm4 <- confusionMatrix(multPredictions4, multData2$correction_info)
multCm5 <- confusionMatrix(multPredictions5, multData1$correction_info)

# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)
multPValues <- c(multCm1$overall['AccuracyPValue'], 
              multCm2$overall['AccuracyPValue'], 
              multCm3$overall['AccuracyPValue'], 
              multCm4$overall['AccuracyPValue'], 
              multCm5$overall['AccuracyPValue'])
```

Combine p-values using Fisher's method

```{r combine p-vals mult, echo=TRUE, message=FALSE, warning=FALSE}
# Fisher's method
multFisher_combined <- -2 * sum(log(multPValues))
df <- 2 * length(multPValues)
multPCcombined_fisher <- 1 - pchisq(multFisher_combined, df)
print(multPCcombined_fisher)

# Stouffer's method
multZ_scores <- qnorm(1 - multPValues/2)
multCombined_z <- sum(multZ_scores) / sqrt(length(multPValues))
multP_combined_stouffer <- 2 * (1 - pnorm(abs(multCombined_z)))
print(multP_combined_stouffer)
```

The p-values should sum up to 0. 

#### Feature importance

##### Model 1

```{r mult feature importance 1, echo=TRUE, message=FALSE, warning=FALSE}
XGBmultModel1 <- multModel1$finalModel
importanceXGBmultModel1 <- xgb.importance(model = XGBmultModel1)
head(importanceXGBmultModel1, n=10)
xgb.plot.importance(importanceXGBmultModel1)
```

##### Model 2

```{r mult feature importance 2, echo=TRUE, message=FALSE, warning=FALSE}
XGBmultModel2 <- multModel2$finalModel
importanceXGBmultModel2 <- xgb.importance(model = XGBmultModel2)
head(importanceXGBmultModel2, n=10)
xgb.plot.importance(importanceXGBmultModel2)
```

##### Model 3

```{r mult feature importance 3, echo=TRUE, message=FALSE, warning=FALSE}
XGBmultModel3 <- multModel3$finalModel
importanceXGBmultModel3 <- xgb.importance(model = XGBmultModel3)
head(importanceXGBmultModel3, n=10)
xgb.plot.importance(importanceXGBmultModel3)
```

##### Model 4

```{r mult feature importance 4, echo=TRUE, message=FALSE, warning=FALSE}
XGBmultModel4 <- multModel4$finalModel
importanceXGBmultModel4 <- xgb.importance(model = XGBmultModel4)
head(importanceXGBmultModel4, n=10)
xgb.plot.importance(importanceXGBmultModel4)
```

##### Model 5

```{r mult feature importance 5, echo=TRUE, message=FALSE, warning=FALSE}
XGBmultModel5 <- multModel5$finalModel
importanceXGBmultModel5 <- xgb.importance(model = XGBmultModel5)
head(importanceXGBmultModel5, n=10)
xgb.plot.importance(importanceXGBmultModel5)
```

##### Cumulative feature importance

```{r mult cumulative importance, echo=TRUE, message=FALSE, warning=FALSE}
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
  importance <- xgb.importance(model = model)
  importance$Gain <- importance$Gain / sum(importance$Gain)
  return(importance)
}

# Extract normalized importance for each model
multImportance1 <- get_normalized_importance(multModel1$finalModel)
multImportance2 <- get_normalized_importance(multModel2$finalModel)
multImportance3 <- get_normalized_importance(multModel3$finalModel)
multImportance4 <- get_normalized_importance(multModel4$finalModel)
multImportance5 <- get_normalized_importance(multModel5$finalModel)

# Combine importances
multAllImportances <- list(multImportance1, multImportance2, multImportance3, multImportance4, multImportance5)

# Function to merge importances
merge_importances <- function(importances) {
  for (i in 2:length(importances)) {
    names(importances[[i]])[2:4] <- paste0(names(importances[[i]])[2:4], "_", i)
  }
  merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
  merged[is.na(merged)] <- 0  # Replace NAs with 0
  gain_cols <- grep("Gain", colnames(merged), value = TRUE)
  merged$Cumulative <- rowSums(merged[, ..gain_cols])
  return(merged[, .(Feature, Cumulative)])
}

# Merge and sort importances
multCumulativeImportance <- merge_importances(multAllImportances)
multCumulativeImportance <- multCumulativeImportance[order(-multCumulativeImportance$Cumulative), ]

# Print cumulative feature importance
head(multCumulativeImportance, n=10)
```

#### PCA

Now let's collect 10 features per component that have highest combined ranking from PCA and XGBoost. This means that for each feature we sum up the ranking it obtained in cumulative importance (XGBoost) and loading on a principal component (PCA).

```{r mult pca}

# Rank the features based on XGBoost importance (cumulative)
multCumulativeImportance$XGB_Rank <- rank(-multCumulativeImportance$Cumulative)

# Load in PCA for gesture
mult_pca <- read_csv(paste0(datasets, "PCA_top_contributors_multi.csv"))

# For each PC (PC1, PC2, PC3), rank the features based on their loadings
combined_ranks_per_pc <- list()

for (pc in c("PC1", "PC2", "PC3")) {
  # Extract the features and loadings for the current PC
  pca_pc_loadings <- mult_pca[, c(pc, paste0(pc, "_Loading"))]
  colnames(pca_pc_loadings) <- c("Feature", "Loading")
  
  # Rank the features based on the absolute loading values (higher loadings should get lower rank)
  pca_pc_loadings$PCA_Rank <- rank(-abs(pca_pc_loadings$Loading))
  
  # Merge PCA loadings with XGBoost importance ranks
  merged_data <- merge(pca_pc_loadings, multCumulativeImportance[, c("Feature", "XGB_Rank")], by = "Feature")
  
  # Calculate combined rank by summing XGBoost rank and PCA rank
  merged_data$Combined_Rank <- merged_data$XGB_Rank + merged_data$PCA_Rank
  
  # Sort by the combined rank (lower rank is better)
  sorted_data <- merged_data[order(merged_data$Combined_Rank), ]
  
  # Select the top n features based on the combined rank for the current PC
  top_n_features <- 10  # Adjust the number of top features as needed
  combined_ranks_per_pc[[pc]] <- head(sorted_data, top_n_features)
}

# Output the top features per PC based on combined ranking
head(combined_ranks_per_pc, n=10)

```

For modelling, we want to pick three features per component. Which would it be in this case?

```{r mult pca pick 3}

# Number of top features to display
top_n_features <- 3

# Print the top 3 features per component
for (pc in c("PC1", "PC2", "PC3")) {
  cat("\nTop 3 Features for", pc, ":\n")
  
  # Get the top 3 features based on combined rank for the current PC
  top_features <- head(combined_ranks_per_pc[[pc]], top_n_features)
  
  # Print the results
  print(top_features[, c("Feature", "XGB_Rank", "PCA_Rank", "Combined_Rank")])
}

```
