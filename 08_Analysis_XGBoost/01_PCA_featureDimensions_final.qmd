---
title: 'Analysis: PCA to identify effort dimensions'
jupyter: tsprocess
format:
  html:
    code-overflow: wrap
    code-width: 1200  # Adjust the width in pixels
---


In this notebook, we will use Principal Component Analysis (PCA) to identify the most relevant planes (components) of effort among the features in the dataset(s) we created in the previous script. We do this paralel to extreme gradient boosting (XGBoost) because unlike PCA, XGBoost does not prevent from cummulating most relevant features that are correlated, i.e., they likely explain similar dimension of effort. To increase interpretative power of our analysis, we will combine these two methods to identify the most relevant features within the most relevant dimensions (i.e., components) of effort.


```{python}
#| code-fold: true
#| code-summary: Code to prepare the environment

import os
import glob
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler, LabelEncoder

curfolder = os.getcwd()

# This is where our features live
features = curfolder + '\\..\\07_TS_featureExtraction\\Datasets\\'
dfs = glob.glob(features + '*.csv') 

```

Because within the three distinct modalities - gesture, vocalization, combined - a set of different components could be decisive to characterize effort, we will perform PCA on each modality separately.

```{python}
# This is gesture data
ges = [x for x in dfs if 'gesture' in x]
data_ges = pd.read_csv(ges[0])

# This is vocalization data
voc = [x for x in dfs if 'vocal' in x]
data_voc = pd.read_csv(voc[0])

# This is multimodal data
multi = [x for x in dfs if 'combination' in x]
data_multi = pd.read_csv(multi[0])
```

## PCA: Gesture

Let's start by cleaning the dataframe. In gesture modality, some of the features are not relevant to the current analysis - those are mainly the ones that are related to acoustics and concept-related information. We will remove them from the dataframe before performing PCA.

```{python}
#| code-fold: true
#| code-summary: Custom functions

# Function to clean the data
def clean_df(df, colstodel):

    # Delete all desired columns
    df = df.loc[:,~df.columns.str.contains('|'.join(colstodel))]

    # Fill NaNs with 0
    df = df.fillna(0)   # FLAGGED: this we might change, maybe not the best method (alternative: MICE)

    # Save values from correction_info
    correction_info = df['correction_info']

    # Leave only numerical cols, except correction_info
    df = df.select_dtypes(include=['float64','int64'])

    # Add back correction_info
    df['correction_info'] = correction_info
    
    return df
```

```{python}
# These are answer related columns
conceptcols = ['answer', 'expressibility', 'response']

# These are vocalization related columns
voccols = ['envelope', 'audio', 'f0', 'f1', 'f2', 'f3', 'env_', 'duration_voc', 'CoG']

# Concatenate both lists
colstodel = conceptcols + voccols

# Clean the df
ges_clean = clean_df(data_ges, colstodel)

ges_clean.head(15)
```

Now, we first standardize the data and apply PCA to extract principal components. We use custom function `PCA_biplot` (adapted from [here](https://stackoverflow.com/questions/50796024/feature-variable-importance-after-a-pca-analysis)) to visualize the first two principal components. Data points are color-coded on the target variable (correction info) and red arrows represent the contributions of selected variables to the PC.

```{python}
#| code-fold: true
#| code-summary: Custom functions

# Function to plot PCA results
def PCA_biplot(score, coeff, labels=None, selected_vars=None):
    xs = score[:, 0]
    ys = score[:, 1]
    n = coeff.shape[0]
    scalex = 1.0 / (xs.max() - xs.min())
    scaley = 1.0 / (ys.max() - ys.min())

    # Ensure all arrays have the same length
    min_length = min(len(xs), len(ys), len(y))

    # Trim all arrays to the smallest length
    xs_trimmed = xs[:min_length]
    ys_trimmed = ys[:min_length]
    y_trimmed = y[:min_length]  # Adjust 'c' values to match

    
    plt.figure(figsize=(8, 6))
    # Now plot safely
    plt.scatter(xs_trimmed * scalex, ys_trimmed * scaley, c=y_trimmed, cmap="viridis", alpha=0.7)
    
    # If selected_vars is provided, only plot these variables
    if selected_vars is not None:
        for i in selected_vars:
            plt.arrow(0, 0, coeff[i, 0], coeff[i, 1], color='r', alpha=0.5)
            if labels is None:
                plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, "Var" + str(i + 1), 
                         color='g', ha='center', va='center', fontsize=9)
            else:
                plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, labels[i], 
                         color='g', ha='center', va='center', fontsize=9)
    else:
        for i in range(n):
            plt.arrow(0, 0, coeff[i, 0], coeff[i, 1], color='r', alpha=0.5)
            if labels is None:
                plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, "Var" + str(i + 1), 
                         color='g', ha='center', va='center', fontsize=9)
            else:
                plt.text(coeff[i, 0] * 1.15, coeff[i, 1] * 1.15, labels[i], 
                         color='g', ha='center', va='center', fontsize=9)

    # Zoom into the plot by narrowing the axis limits
    plt.xlim(-0.5, 0.5)  # Adjust the range as needed
    plt.ylim(-0.5, 0.5)  # Adjust the range as needed
    
    plt.xlabel("PC1", fontsize=14)
    plt.ylabel("PC2", fontsize=14)
    plt.grid()
    plt.title("PCA Biplot", fontsize=16)
    plt.show()
```

```{python}
# Prepare data
X = ges_clean.iloc[:, :-1].values  # All columns except the last as features
y = ges_clean.iloc[:, -1].values   # Last column as target variable

# Convert categorical target to numeric if necessary
if y.dtype == 'object' or y.dtype.name == 'category':
    le = LabelEncoder()
    y = le.fit_transform(y)  # Converts categorical labels into numeric labels

# Scale the data
scaler = StandardScaler()
X = scaler.fit_transform(X)    

# PCA transformation
pca = PCA()
x_new = pca.fit_transform(X)

# For intelligibility, let's select only plot some variables
selected_vars = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  

# Call the function. Use only the 2 PCs.
PCA_biplot(x_new[:, 0:2], np.transpose(pca.components_[0:2, :]), selected_vars=selected_vars)
```

What amount of variances each PC explains?

```{python}
#| echo: false

pca.explained_variance_ratio_
```

So we have really few data, therefore even the first principal component explains only 38% of the variance, second 13% and third 11%. But we will be focusing on the first three principal components, as they together explain at least 50% of the variance

Now we can check the most important features. The larger the absolute value of the Eigenvalue, the more important the feature is for the principal component.

```{python}
#| echo: false

print(abs( pca.components_ ))
```

```{python}
# Number of principal components
n_pcs = 3

# Feature names (excluding target column)
feature_names = ges_clean.columns[:-1]  

# Create storage for the ordered feature names and loadings
results_dict_ges = {}

for i in range(n_pcs):
    # Get all features sorted by absolute loading values
    sorted_indices = np.abs(pca.components_[i]).argsort()[::-1]
    sorted_features = feature_names[sorted_indices]  # Feature names
    sorted_loadings = pca.components_[i, sorted_indices]  # Loadings

    # Store in dictionary
    results_dict_ges[f'PC{i+1}'] = sorted_features.values
    results_dict_ges[f'PC{i+1}_Loading'] = sorted_loadings

# Convert dictionary to DataFrame
results_df_ges = pd.DataFrame(results_dict_ges)
results_df_ges.head(20)
```

Now we have dataframe for gesture modality where each column represents a principal component (PC1-PC3) and each row represents a feature. The values in the dataframe are the loadings of the features on the principal components. The loadings are the correlation coefficients between the features and the principal components. The higher the absolute value of the loading, the more important the feature is for the principal component. The dataframe is sorted by the absolute value of the loadings in descending order.

Save the top contributors as a file so that we can load it in for the XGBoost analysis. We will also save the clean data which we can use for XGBoost modeling too.

```{python}
# Save top contributors
results_df_ges.to_csv(curfolder + '\\datasets\\PCA_top_contributors_ges.csv', index=False)

# Save clean data
ges_clean.to_csv(curfolder + '\\datasets\\ges_clean_df.csv', index=False)
```

## PCA: Vocalizations

In the following repetitions, we will use custom function `pca_analysis` which does all the steps we performed previously for gesture modality in one go

```{python}
#| code-fold: true
#| code-summary: Custom PCA function

def pca_analysis(df_clean):
    # Prepare data
    X = df_clean.iloc[:, :-1].values  # All columns except the last as features
    y = df_clean.iloc[:, -1].values   # Last column as target variable

    # Convert categorical target to numeric if necessary
    if y.dtype == 'object' or y.dtype.name == 'category':
        le = LabelEncoder()
        y = le.fit_transform(y)  # Converts categorical labels into numeric labels

    # Scale the data
    scaler = StandardScaler()
    X = scaler.fit_transform(X)    

    # PCA transformation
    pca = PCA()
    x_new = pca.fit_transform(X)

    # Select few variables
    selected_vars = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  

    print('Biplot for the first 2 PCs:')
    PCA_biplot(x_new[:, 0:2], np.transpose(pca.components_[0:2, :]), selected_vars=selected_vars)

    PC1explained = pca.explained_variance_ratio_[0]*100
    PC2explained = pca.explained_variance_ratio_[1]*100
    PC3explained = pca.explained_variance_ratio_[2]*100

    print('PCs explained variance:')
    print(f'PC1: {PC1explained:.2f}%')
    print(f'PC2: {PC2explained:.2f}%')
    print(f'PC3: {PC3explained:.2f}%')

    # Getting most contributing features
    n_pcs = 3

    # Feature names (excluding target column)
    feature_names = df_clean.columns[:-1]  

    # Create storage for the ordered feature names and loadings
    results_dict = {}

    for i in range(n_pcs):
        # Get all features sorted by absolute loading values
        sorted_indices = np.abs(pca.components_[i]).argsort()[::-1]
        sorted_features = feature_names[sorted_indices]  # Feature names
        sorted_loadings = pca.components_[i, sorted_indices]  # Loadings

        # Store in dictionary
        results_dict[f'PC{i+1}'] = sorted_features.values
        results_dict[f'PC{i+1}_Loading'] = sorted_loadings

    # Convert dictionary to DataFrame
    results_df = pd.DataFrame(results_dict)

    return results_df
```

Before PCA, we need to clean the data such that only vocalization-relevant features are kept.

```{python}
# These are answer related columns
conceptcols = ['answer', 'expressibility', 'response']

# These are vocalization related columns
voccols = ['envelope', 'audio', 'f0', 'f1', 'f2', 'f3', 'env_', 'duration_voc', 'CoG', 'correction_info']

# Concatenate both lists
colstodel = conceptcols 

# Clean the df
voc_clean = clean_df(data_voc, colstodel)

# Keep only those cols that have some in name - at least partially - words from voccols
colstokeep = [col for col in voc_clean.columns if any(word in col for word in voccols)]

# Keep only those columns
voc_clean = voc_clean[colstokeep]

voc_clean.head(15)
```

Now we can use the function to perform the same PCA analysis but on vocal features

```{python}
# Perform PCA analysis
results_df_voc = pca_analysis(voc_clean)

# Display
results_df_voc.head(20)
```

Save contributors as a file

```{python}
# Save top contributors
results_df_voc.to_csv(curfolder + '\\datasets\\PCA_top_contributors_voc.csv', index=False)

# Save clean data
voc_clean.to_csv(curfolder + '\\datasets\\voc_clean_df.csv', index=False)
```

## PCA: Combined

Now we do the same for combined condition

```{python}
# These are answer related columns
conceptcols = ['answer', 'expressibility', 'response']

# Concatenate both lists
colstodel = conceptcols 

# Clean the df
multi_clean = clean_df(data_multi, colstodel)

multi_clean.head(15)
```

Now we can use the function to perform the same PCA analysis but all (i.e., both vocal and gestural) features

```{python}
# Perform PCA analysis
results_df_multi = pca_analysis(multi_clean)

# Display
results_df_multi.head(20)
```

```{python}
# Save top contributors
results_df_multi.to_csv(curfolder + '\\datasets\\PCA_top_contributors_multi.csv', index=False)

# Save clean data
multi_clean.to_csv(curfolder + '\\datasets\\multi_clean_df.csv', index=False)
```

