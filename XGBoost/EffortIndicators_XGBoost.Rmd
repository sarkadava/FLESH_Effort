---
title: 'Indicators of effort: XGBoost'
author: "Aleksandra Ä†wiek (original code), Sarka Kadava (adaptation)"
output: html_document
date: "2024-08-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data preparation

## Source setup

```{r source setup, echo = TRUE, message=FALSE, warning = FALSE}

########## folders ##########
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())

processfolder <- paste0(parentfolder, '/TS_processing/')
data          <- paste0(processfolder, '/Datasets/')

datasets      <- paste0(parentfolder, '/XGBoost/datasets/')
models        <- paste0(parentfolder, '/XGBoost/models/')
plots         <- paste0(parentfolder, '/XGBoost/plots/')
scripts       <- paste0(parentfolder, '/XGBoost/scripts/')

########## source file ##########

#source(paste0(scripts, "adjectives-preparation.R"))

#################### packages ####################
# Data Manipulation
library(tibble)
library(stringr)
library(tidyverse) # includes readr, tidyr, dplyr, ggplot2
library(data.table)

# Plotting
library(ggforce)
library(ggpubr)
library(gridExtra)

# Random Forests and XGBoost
library(rpart)
library(rpart.plot)
library(ranger)
library(tuneRanger)
library(caret)
library(xgboost)
library(parallel)
library(mice)
library(doParallel)
# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())

colorBlindBlack8  <- c("#000000", "#E69F00", "#56B4E9", "#009E73", 
                       "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

```

## Load in data frames

```{r read metadata, echo=TRUE, message=FALSE, warning=FALSE}

# Load cleaned effort data
data <- read_csv(paste0(data, "features_df_final.csv"))

```

# Data preparation

Inspect the data and convert to factors, if needed. Then split in modalities.

```{r inspect data, echo=TRUE, message=FALSE, warning=FALSE}
str(data)

# Turn percProm to factor
data$correction_info <- as.factor(data$correction_info)
data$TrialID <- as.factor(data$TrialID)
data$modality <- as.factor(data$modality)


# First, remove the specified columns
data <- data %>%
  select(-concept, -answer_fol, -answer_fol_dist, -answer_fol_rawsim, 
         -answer_prev, -answer_prev_dist, -answer_prev_rawsim, -concept_density_en, 
         -concept_density_nl, -concept_id, -expressibility, -response_time_sec,
         -correction)

# Create three dfs separately for each modality
data_ges <- data %>% 
  filter(modality == "gebaren")

data_voc <- data %>% 
  filter(modality == "geluiden")

data_mult <- data %>% 
  filter(modality == "combinatie")


```

# Gesture

First we need to make sure that df does not contain any NAs

```{r}

# get rid of NA columns
data_ges <- data_ges[, colSums(is.na(data_ges)) == 0]

data_ges <- data_ges %>%
  select(-TrialID, -modality)

# get rid of all char columns
data_ges <- data_ges[, sapply(data_ges, class) != "character"]


```

## Random forests

```{r}

# prepare predictors
predictors <- setdiff(names(data_ges), "correction_info")

formula_str <- paste("correction_info ~", paste(predictors, collapse = " + "))

# Convert the formula string to a formula object
gesTree_formula <- as.formula(formula_str)

# Now use the formula in rpart
gesTree <- rpart(formula = gesTree_formula, data = data_ges, 
                method='class', # Specify that it's a classification tree
                control = rpart.control(maxdepth = 5)  # Control parameters for the 'rpart' function
)

prp(
  gesTree,         # The decision tree object to be visualized
  extra = 1,      # Show extra information (like node statistics) in the plot
  varlen = 0,     # Length of variable names (0 means auto-determined)
  faclen = 0     # Length of factor levels displayed on the plot (increase as needed)
)
```

We will build a random forest first.

```{r set seed, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(998) # Set a seed for reproducibility
```

Split the data
```{r}


# Split the data into training and testing subsets
sample_indices <- sample(1:nrow(data_ges), 0.8*nrow(data_ges)) # 80% training, 30% testing
train_data <- data_ges[sample_indices, ]
test_data <- data_ges[-sample_indices, ]

```

Building the untuned model.

```{r untuned ger, echo=TRUE, message=FALSE, warning=FALSE}
# Untuned Model with importance (permutation) option set
gesUntuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:165],
  num.trees = 500,
  importance = "permutation"
)

predictions <- predict(gesUntuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(gesUntuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)
```

Set the parameters for the random forest.

```{r settings for RF ges, echo=TRUE, message=FALSE, warning=FALSE}
# Define the number of CPU cores to use
num_cores <- detectCores()

# Create a cluster with specified number of cores
cl <- makeCluster(num_cores)
```

Tuning the random forest.

```{r tuning RF ger, message=FALSE, warning=FALSE}
tuneGes <- makeClassifTask(data = data_ges[,0:166],
                           target = "correction_info")

tuneGes <- tuneRanger(tuneGes,
                      measure = list(multiclass.brier),
                      num.trees = 500)

#Return hyperparameter values
tuneGes
## Ola: ALWAYS REPORT THOSE BECAUSE THEY ARE DIFFERENT EVERY TIME BC OF RANDOMIZATION
# Recommended parameter settings: 
#   mtry min.node.size sample.fraction
# 1   20             4       0.2387222
# Results: 
#   multiclass.brier exec.time
# 1        0.8037542     0.17

gesTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[,0:165], 
  num.trees = 5000, 
  mtry = 20, # Set the recommended mtry value (number of features).
  min.node.size = 4, # Set the recommended min.node.size value (number of samples before a node terminates).
  sample.fraction = 0.2387222, # Set the recommended sample fraction value.(% of data for bagging).
  importance = "permutation" # Permutation is a computationally intensive test.
)

predictions <- predict(gesTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(gesTuned, num.threads = 1, type = 1) 

# Convert to data frame
feature_importance <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance$Feature <- rownames(feature_importance)
colnames(feature_importance) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance[order(-feature_importance$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)


# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```


Create a tuned model only.

```{r tuned model imputed ger, echo=TRUE, message=FALSE, warning=FALSE}
# Create a classification task for tuning
tuneGes <- makeClassifTask(data = train_data[, 0:166], target = "correction_info")

# Tune the model
tuneGes <- tuneRanger(tuneGes, measure = list(multiclass.brier), num.trees = 500)

# Return hyperparameter values
tuneGes
# Recommended parameter settings: 
#   mtry min.node.size sample.fraction
# 1    66             3       0.2254135
# Results: 
#   multiclass.brier exec.time
# 1        0.7845583     0.168

# Fit the tuned model on the training data
gesTuned <- ranger(
  y = train_data$correction_info,
  x = train_data[, 0:165],
  num.trees = 5000,
  mtry = 66,
  min.node.size = 3,
  sample.fraction = 0.2254135,
  importance = "permutation"
)

# Predict on the test data
predictions <- predict(gesTuned, data = test_data)$predictions

# Create a confusion matrix
confusion_matrix <- confusionMatrix(predictions, test_data$correction_info)

# Print the confusion matrix
print(confusion_matrix)

# Calculate feature importance
feature_importance <- importance(gesTuned, num.threads = 1, type = 1)

# Convert to data frame
feature_importance_df <- as.data.frame(feature_importance, stringsAsFactors = FALSE)
feature_importance_df$Feature <- rownames(feature_importance_df)
colnames(feature_importance_df) <- c("Importance", "Feature")

# Sort by importance
sorted_feature_importance <- feature_importance_df[order(-feature_importance_df$Importance), ]

# Print sorted feature importance
print(sorted_feature_importance)

# Close the cluster when you're done with your parallel tasks
#stopCluster(cl)
```


Save  data frame.

```{r save imputed data, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}


write.csv(data_ges, file = paste0(datasets, "gesDataXGB.csv"), row.names = FALSE)
```

# XGBoost

Ensure parallel processing.

```{r parallel, echo=TRUE, message=FALSE, warning=FALSE}
# Detect the number of available cores
cores <- detectCores() #- 1  # Leave one core free

# Create a cluster with the detected number of cores
cl <- makeCluster(cores)

# Register the parallel backend
registerDoParallel(cl)
```

Define the grid and estimate runtime.

```{r grid, echo=TRUE, message=FALSE, warning=FALSE}
grid_tune <- expand.grid(
  nrounds = c(5000, 10000), 
  max_depth = c(3, 6), 
  eta = c(0.05, 0.1), 
  gamma = c(0.1), 
  colsample_bytree = c(0.6, 0.8), 
  min_child_weight = c(1), 
  subsample = c(0.75, 1.0)
)

# Calculate total combinations
total_combinations <- nrow(grid_tune)

# Estimate single model run time (assume 1 minute per run)
single_model_time <- 10 # minute

# Total runs for cross-validation
folds <- 5
total_runs <- total_combinations * folds

# Total time estimation without parallel processing
total_time <- total_runs * single_model_time # in minutes

# Convert to hours
total_time_hours <- total_time / 60

# Output estimated time without parallel processing
print(paste("Estimated time for grid search without parallel processing:", total_time_hours, "hours"))

# Parallel processing with 4 cores
cores <- 24
total_time_parallel <- total_time / cores # in minutes

# Convert to hours
total_time_parallel_hours <- total_time_parallel / 60

# Output estimated time with parallel processing
print(paste("Estimated time for grid search with", cores, "cores:", total_time_parallel_hours, "hours"))

rm(total_combinations,single_model_time,folds,total_runs,total_time,total_time_hours,total_time_parallel,total_time_parallel_hours,cores)
```

#### K-fold cross-validation

Create subsets to train and test data (80/20).

```{r k-fold subset, echo=TRUE, message=FALSE, warning=FALSE}
# Set seed for reproducibility
set.seed(998)

# Set up train control
train_control <- trainControl(
  method = "cv",        # Cross-validation
  number = 5,           # 5-fold cross-validation
  allowParallel = TRUE  # Enable parallel processing
)

# Define the number of subsets
numSubsets <- 5

# Create an empty list to store subsets
gesSubsets <- vector("list", length = numSubsets)

# load MICE imputed data
#gerDataXGB <- read_csv(paste0(datasets, "gerDataXGB.csv"))
gesDataXGB <- data_ges
# ensure percProm is factor
gesDataXGB$correction_info <- as.factor(gesDataXGB$correction_info)
levels(gesDataXGB$correction_info)
# only keep the columns of output and predictor variables
#gerDataXGB <- gerDataXGB[,13:52] 

# Calculate the number of samples in each subset
subsetSize <- nrow(gesDataXGB) %/% numSubsets

# Randomly assign samples to subsets
for (i in 1:numSubsets) {
  if (i < numSubsets) {
    gesSubsets[[i]] <- gesDataXGB[sample((1:nrow(gesDataXGB)), size = subsetSize), ]
  } else {
    gesSubsets[[i]] <- gesDataXGB[sample((1:nrow(gesDataXGB)), size = subsetSize + (nrow(gesDataXGB) %% numSubsets)), ]
  }
}

# Naming the subsets
names(gesSubsets) <- paste0("gesData", 1:numSubsets)

# Access the subsets (e.g., gerData1, gerData2, etc.)
gesData1 <- gesSubsets$gesData1
gesData2 <- gesSubsets$gesData2
gesData3 <- gesSubsets$gesData3
gesData4 <- gesSubsets$gesData4
gesData5 <- gesSubsets$gesData5

# Combine subsets into 80% groups.
gesData1234 <- rbind(gesData1, gesData2, gesData3, gesData4)
gesData1235 <- rbind(gesData1, gesData2, gesData3, gesData5)
gesData1245 <- rbind(gesData1, gesData2, gesData4, gesData5)
gesData1345 <- rbind(gesData1, gesData3, gesData4, gesData5)
gesData2345 <- rbind(gesData2, gesData3, gesData4, gesData5)

```

#### Models

Only run the models one time and then readRDS.

##### Model 1

```{r ger model1, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel1 <- caret::train(
  correction_info ~ .,              
  data = gesData1234,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel1, file = paste0(models, "gesModel1.rds"), compress = TRUE)
```

##### Model 2

```{r ger model2, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel2 <- caret::train(
  correction_info ~ .,              
  data = gesData1235,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel2, file = paste0(models, "gesModel2.rds"), compress = TRUE)
```

##### Model 3

```{r ger model3, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel3 <- caret::train(
  correction_info ~ .,              
  data = gesData1245,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel3, file = paste0(models, "gesModel3.rds"), compress = TRUE)
```

##### Model 4

```{r ger model4, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel4 <- caret::train(
  correction_info ~ .,              
  data = gesData1345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)
saveRDS(gesModel4, file = paste0(models, "gesModel4.rds"), compress = TRUE)
```

##### Model 5

```{r ger model5, echo=TRUE, message=FALSE, warning=FALSE, eval = FALSE}
gesModel5 <- caret::train(
  correction_info ~ .,              
  data = gesData2345,
  method = "xgbTree",     
  trControl = train_control,
  tuneGrid = grid_tune    
)

saveRDS(gesModel5, file = paste0(models, "gesModel5.rds"), compress = TRUE)
```

##### Load models

Load all models after running, if necessary.

```{r loas models ger, echo=TRUE, message=FALSE, warning=FALSE}
gesModel1 <- readRDS(paste0(models, "gesModel1.rds"))
gesModel2 <- readRDS(paste0(models, "gesModel2.rds"))
gesModel3 <- readRDS(paste0(models, "gesModel3.rds"))
gesModel4 <- readRDS(paste0(models, "gesModel4.rds"))
gesModel5 <- readRDS(paste0(models, "gesModel5.rds"))
```

#### Test models

Generate predictions and confusion matrices

```{r test models ger, echo=TRUE, message=FALSE, warning=FALSE}
# Generate predictions
gesPredictions1 <- predict(gesModel1, newdata = gesData5)
gesPredictions2 <- predict(gesModel2, newdata = gesData4)
gesPredictions3 <- predict(gesModel3, newdata = gesData3)
gesPredictions4 <- predict(gesModel4, newdata = gesData2)
gesPredictions5 <- predict(gesModel5, newdata = gesData1)

# Compute confusion matrices
gesCm1 <- confusionMatrix(gesPredictions1, gesData5$correction_info)
gesCm2 <- confusionMatrix(gesPredictions2, gesData4$correction_info)
gesCm3 <- confusionMatrix(gesPredictions3, gesData3$correction_info)
gesCm4 <- confusionMatrix(gesPredictions4, gesData2$correction_info)
gesCm5 <- confusionMatrix(gesPredictions5, gesData1$correction_info)

# Extract p-values (you need to define how to extract these based on your metric, here assumed to be some metric from confusion matrix)
gesPValues <- c(gesCm1$overall['AccuracyPValue'], 
              gesCm2$overall['AccuracyPValue'], 
              gesCm3$overall['AccuracyPValue'], 
              gesCm4$overall['AccuracyPValue'], 
              gesCm5$overall['AccuracyPValue'])
```

Combine p-values using Fisher's method

```{r combine p-vals ger, echo=TRUE, message=FALSE, warning=FALSE}
# Fisher's method
gesFisher_combined <- -2 * sum(log(gesPValues))
df <- 2 * length(gesPValues)
gesPCcombined_fisher <- 1 - pchisq(gesFisher_combined, df)
print(gesPCcombined_fisher)

# Stouffer's method
gesZ_scores <- qnorm(1 - gesPValues/2)
gesCombined_z <- sum(gesZ_scores) / sqrt(length(gesPValues))
gesP_combined_stouffer <- 2 * (1 - pnorm(abs(gesCombined_z)))
print(gesP_combined_stouffer)
```

The p-values sum up to 0, since they are all so small.

#### Feature importance

##### Model 1

```{r ger feature importance 1, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel1 <- gesModel1$finalModel
importanceXGBgesModel1 <- xgb.importance(model = XGBgesModel1)
print(importanceXGBgesModel1)
xgb.plot.importance(importanceXGBgesModel1)
```

##### Model 2

```{r ger feature importance 2, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel2 <- gesModel2$finalModel
importanceXGBgesModel2 <- xgb.importance(model = XGBgesModel2)
print(importanceXGBgesModel2)
xgb.plot.importance(importanceXGBgesModel2)
```

##### Model 3

```{r ger feature importance 3, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel3 <- gesModel3$finalModel
importanceXGBgesModel3 <- xgb.importance(model = XGBgesModel3)
print(importanceXGBgesModel3)
xgb.plot.importance(importanceXGBgesModel3)
```

##### Model 4

```{r ger feature importance 4, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel4 <- gesModel4$finalModel
importanceXGBgesModel4 <- xgb.importance(model = XGBgesModel4)
print(importanceXGBgesModel4)
xgb.plot.importance(importanceXGBgesModel4)
```

##### Model 5

```{r ger feature importance 5, echo=TRUE, message=FALSE, warning=FALSE}
XGBgesModel5 <- gesModel5$finalModel
importanceXGBgesModel5 <- xgb.importance(model = XGBgesModel5)
print(importanceXGBgesModel5)
xgb.plot.importance(importanceXGBgesModel5)
```

##### Cumulative feature importance

```{r ger cumulative importance, echo=TRUE, message=FALSE, warning=FALSE}
# Function to extract and normalize importance
get_normalized_importance <- function(model) {
  importance <- xgb.importance(model = model)
  importance$Gain <- importance$Gain / sum(importance$Gain)
  return(importance)
}

# Extract normalized importance for each model
gesImportance1 <- get_normalized_importance(gesModel1$finalModel)
gesImportance2 <- get_normalized_importance(gesModel2$finalModel)
gesImportance3 <- get_normalized_importance(gesModel3$finalModel)
gesImportance4 <- get_normalized_importance(gesModel4$finalModel)
gesImportance5 <- get_normalized_importance(gesModel5$finalModel)

# Combine importances
gesAllImportances <- list(gesImportance1, gesImportance2, gesImportance3, gesImportance4, gesImportance5)

# Function to merge importances
merge_importances <- function(importances) {
  for (i in 2:length(importances)) {
    names(importances[[i]])[2:4] <- paste0(names(importances[[i]])[2:4], "_", i)
  }
  merged <- Reduce(function(x, y) merge(x, y, by = "Feature", all = TRUE), importances)
  merged[is.na(merged)] <- 0  # Replace NAs with 0
  gain_cols <- grep("Gain", colnames(merged), value = TRUE)
  merged$Cumulative <- rowSums(merged[, ..gain_cols])
  return(merged[, .(Feature, Cumulative)])
}

# Merge and sort importances
gesCumulativeImportance <- merge_importances(gesAllImportances)
gesCumulativeImportance <- gesCumulativeImportance[order(-gesCumulativeImportance$Cumulative), ]

# Print cumulative feature importance
print(gesCumulativeImportance)
```



