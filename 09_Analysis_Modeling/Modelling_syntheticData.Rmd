---
title: "Modeling-effort"
subtitle: "Consultation notes with Daniela"
author: "Sarka Kadava"
date: "2024-10-17"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

# Introduction

In this script, we will be modelling the causal relation between effort and correction to confirm/reject our hypothesis.

These are:

H1: In corrections, people enhance some effort-related kinematic and/or acoustic features of their behaviour relative to the baseline.

H2: The enhancement depends on similarity of the guesser's answer and the original meaning. More similar answer will require/result in smaller enhancement (but still enhancement) than less similar answer.

To assess the design and performance of the model, we will use synthetic data that we create to have certain interdependencies and where the effects have pre-defined values. We use this approach instead of using our dyad 0 data, because the pilot data do not include enough data to have a sensible testing sample. 

Since we cannot predict the outcomes of the XGBoost modelling (in the previous script) and therefore do not know the variables we will model after preregistration, our model only assumes a continuous variable but it is otherwise free of any other assumptions.

# Setting up the environment

```{r warning=FALSE}

library(here)
library(dagitty) # for dag
library(dplyr) # for data-wrangling
library(lme4)  # for linear mixed-effects models
library(tidyr)  # for reshaping data (if needed)
library(ggplot2)

# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())

datasets      <- paste0(parentfolder, '/09_Analysis_Modeling/datasets/')
models        <- paste0(parentfolder, '/09_Analysis_Modeling/models/')
plots         <- paste0(parentfolder, '/09_Analysis_Modeling/plots/')


```

# DAG - Question 1

To make sure we know what variables should be in the model and why, we draw a DAG that illustrates our assumptions.

Our relationship of interest is the causal effect of communicative attempt on effort. Our assumptions include:

* Personality traits (measured with Big5) will influence effort (e.g., people are more willing to put more effort if they are open-minded) and also communicative attempt (e.g., more extroverted people are better in this game, therefore they need less attempts)

* Familiarity with guessing partner influences effort (ditto) as well as communicative attempt (e.g., friends are better in this game than strangers)

* Similarly, participant will also directly influence effort and commAtt, because personality traits might not be capturing all variation

* Expressibility of concepts is going to influence effort (e.g., more expressible concepts allow more enhancement - but could be also other direction) as well as CommAtt (e.g., more expressible concepts are more readily guessed and don't need more attempts)

* Similarly, concept will also directly influence effort and commAtt, because expressibility might not be capturing all variation

* Modality (uni vs. multi) will influence the value of effort. We assume that in unimodal condition, the feature does not need to account for synergic relations with the other modality, and may carry the whole signal. In multimodal condition, however, there may be synergic relations that moderate the full expressiveness of this feature

* Lastly, trial number (characterising how far one is in the experiment) could be hinting on learning processes through out the experiment, or - the other direction - on increasing fatigue


```{r warning=FALSE}

dag <- dagitty('dag {
Big5 [adjusted,pos="-0.823,0.657"]
CommAtt [exposure,pos="-1.033,0.028"]
Conc [adjusted,pos="-1.136,-0.848"]
Eff [outcome,pos="-0.102,0.025"]
Expr [adjusted,pos="-0.758,-0.850"]
Fam [adjusted,pos="-0.379,0.663"]
Pcn [adjusted,pos="-0.589,1.214"]
TrNum [adjusted,pos="-1.686,-0.859"]
Big5 -> CommAtt     
Big5 -> Eff
CommAtt -> Eff
Conc -> Expr
Expr -> CommAtt
Expr -> Eff
Fam -> CommAtt
Fam -> Eff
Mod_bin -> Eff
Pcn -> Big5
Pcn -> CommAtt 
Pcn -> Eff
Pcn -> Fam
TrNum -> CommAtt
TrNum -> Eff
Conc -> CommAtt
Conc -> Eff
}')

plot(dag)

```

# Loading in our data

xxxxxxx


# Synthetic data

We will now create synthetic data that will copy the relations we assume in our DAG. Assigning concrete coefficients will also help us to test our model - we should find exactly those causalities that we had in mind when creating these data

```{r}

# Set seed for reproducibility
set.seed(0209)

# Define participants, total unique concepts, and modalities
n_participants <- 120
n_total_concepts <- 21  # Total unique concepts
n_concepts_per_participant <- 21  # Each participant works with 21 concepts
n_modalities <- 3  # gesture, vocal, combined

# Generate participant IDs
participants <- 1:n_participants

# Simulate Big5 personality traits (standardized between 0 and 1) and Familiarity (between 0 and 1) for participants
Big5 <- runif(n_participants, min = 0, max = 1)  # Continuous values between 0 and 1
Familiarity <- runif(n_participants, min = 0, max = 1)  # Continuous values between 0 and 1

# Create a matrix to hold expressibility values for each concept in each modality
expressibility_matrix <- matrix(runif(n_total_concepts * n_modalities, min = 0, max = 1), nrow = n_total_concepts, ncol = n_modalities)

# Randomly sample 21 unique concepts for each participant
final_data <- data.frame()

# Define a function to assign CommAtt and Eff for a single participant
simulate_participant <- function(participant_id) {
  # Randomly sample 21 unique concepts from the total pool of 84
  selected_concepts <- sample(1:n_total_concepts, n_concepts_per_participant)
  
  participant_data <- data.frame()
  trial_number <- 1  # Initialize trial number
  
  for (concept_id in selected_concepts) {
    # Randomly determine the modality for the concept
    modality <- sample(c("gesture", "vocal", "combined"), 1)
    
    # Calculate expressibility based on modality
    expressibility_score <- ifelse(modality == "vocal", expressibility_matrix[concept_id, 1] * 0.6, 
                                    ifelse(modality == "gesture", expressibility_matrix[concept_id, 2], 
                                           expressibility_matrix[concept_id, 3] * 1.5))
    
    # Determine Communicative Attempts based solely on expressibility, familiarity, and Big5
    base_prob <- c(0.33, 0.33, 0.33)  # Equal chance for 1, 2, or 3 attempts
    
    # Modify probabilities based on familiarity, Big5, and expressibility
    adjusted_prob <- base_prob * c(1 - Familiarity[participant_id], # 3 times for each
                                    1 - Familiarity[participant_id],
                                    1 - Familiarity[participant_id]) * 
                     c(1 - Big5[participant_id],
                       1 - Big5[participant_id],
                       1 - Big5[participant_id]) * 
                     c(1 - expressibility_score,
                       1 - expressibility_score,
                       1 - expressibility_score)
    
    # Normalize the adjusted probabilities
    adjusted_prob <- adjusted_prob / sum(adjusted_prob)
    
    # Sample the number of communicative attempts based on adjusted probabilities
    n_attempts <- sample(1:3, 1, prob = adjusted_prob)
    
    # Loop through the number of attempts and increment CommAtt correctly
    for (attempt in 1:n_attempts) {
      # Calculate Eff for the first attempt
      if (attempt == 1) {
        Eff <- 1.15 * Big5[participant_id] + 
               1.10 * Familiarity[participant_id] + 
               1.20 * expressibility_score + 
               rnorm(1, mean = 1, sd = 0.5)
        
        # Adjust Eff based on modality
        if (modality == "combined") {
          
          Eff <- Eff * 0.7  # Slight moderation for combined modality
        }
      }
      
      # Adjust Eff for subsequent attempts
      if (attempt == 2) {
        Eff <- 1.15 * Big5[participant_id] + 
               1.10 * Familiarity[participant_id] + 
               1.20 * expressibility_score + 
               rnorm(1, mean = 1, sd = 0.5)
        Eff <- Eff * 1.50  # Multiply effort by 1.50 for the second attempt
      } else if (attempt == 3) {
        Eff <- 1.15 * Big5[participant_id] + 
               1.10 * Familiarity[participant_id] + 
               1.20 * expressibility_score + 
               rnorm(1, mean = 1, sd = 0.5)
        Eff <- Eff * 0.70  # Multiply effort by 0.70 for the third attempt
      }
      
      # Create row for each attempt
      participant_data <- rbind(participant_data, data.frame(
        Participant = participant_id,
        Concept = concept_id,
        Modality = modality,
        Big5 = Big5[participant_id],
        Familiarity = Familiarity[participant_id],
        Expressibility = expressibility_score,
        CommAtt = attempt,  # Correctly set the attempt number
        Eff = Eff,
        TrialNumber = trial_number  # Set trial number for this attempt
      ))
      
      # Increment the trial number after each attempt
      trial_number <- trial_number + 1
    }
  }
  
  return(participant_data)
}

# Simulate data for all participants
for (i in participants) {
  final_data <- rbind(final_data, simulate_participant(i))
}

# Preview the first few rows of the final data
head(final_data)

```

So now we have synthetic data where we exactly defined (using coefficients) what is the relationship between certain variables

1) CommAtt -> Eff

The effort for second attempt is multiplied by 1.50 (Beta = 1.50)
The effort for third attempts by 0.7 (ie decreases, Beta = 0.7)

2) Fam -> Eff & CommAtt

Beta = 1.10 for Eff
Negative effect on CommAtt


3) Big5 -> Eff & CommAtt

Beta = 1.15 for Eff
Negative effect on CommAtt

4) Expr -> Eff & CommAtt

Beta = 1.20 for Eff
Negative effect on CommAtt

(For simplicity reasons, we do not create an affect of trial number, as it does not represent a crucial variable)

Note that the synthetic data do not copy the structure of the data and the experimental design perfectly, but it does provide a reliable ground to build the models and test their performance.

## Exploring structure (DP)

```{r}
nrow(final_data) # this is the number of datapoints
```


```{r}
hist(final_data$Eff)
```

```{r}
final_data |> 
  janitor::tabyl(Participant, Concept) # the number of repetition per concept by participant
```
## Check visual

```{r}

# Create a boxplot comparing Effort across different Communicative Attempts
ggplot(final_data, aes(x = as.factor(CommAtt), y = Eff)) +
  geom_boxplot(aes(fill = as.factor(CommAtt))) +  
  labs(title = "Comparison of Effort Across Communicative Attempts",
       x = "Communicative Attempts",
       y = "Effort",
       fill = "CommAtt") + 
  theme_minimal() +
  theme(legend.position = "none")  

```
The visual already hints on the effects we expect to exist in the data.


Save it
```{r}

write.csv(final_data, file = here("09_Analysis_Modeling", "datasets", "final_data_synth.csv"), row.names = FALSE)
```


# Modeling - Question 1

```{r}

library(ggplot2)
library(patchwork)
library(bayesplot)
library(brms)
library(beepr)
library(bayestestR)
library(tidyverse)

# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())

# Load in the data
final_data <- read_csv(paste0(datasets, "final_data_synth.csv"))


```

First, we do some preparation of the data, so that it's easier to interpret the results of the modeling.

## Contrast coding

Convert collumns to factors
```{r}

final_data$CommAtt <- as.factor(final_data$CommAtt)
final_data$Modality <- as.factor(final_data$Modality)
final_data$Participant <- as.factor(final_data$Participant)
final_data$Concept <- as.factor(final_data$Concept)

final_data$TrialNumber <- as.numeric(final_data$TrialNumber)  # Ensure TrialNumber is numeric
```

Check contrasts of factors
```{r}
contrasts(final_data$CommAtt) <- MASS::contr.sdif(3)
contrasts(final_data$Modality) <- contr.sum(3)/2
```


Center trial number
```{r}
final_data$TrialNumber_c <- final_data$TrialNumber - median(range(final_data$TrialNumber))
range(final_data$TrialNumber_c)
range(final_data$TrialNumber)
```
Standardize (z-score) all *continuous* predictors. 

The measures of familiarity, expressibility, and big5 are on a non-continuous rating scale, hence we can just subtract the median from these to centre them (instead of standardizing).

```{r}

final_data$Familiarity <- final_data$Familiarity - median(range(final_data$Familiarity))
final_data$Big5 <- final_data$Big5 - median(range(final_data$Big5))
```


Z-score expressibility (because it's continuous) within a modality
```{r}

final_data <-
  final_data |>
  group_by(Modality) |>
  mutate(Expressibility_z = (Expressibility - mean(Expressibility))/ sd(final_data$Expressibility, na.rm = T)) |>
  ungroup()
```


## Bayes models

We now want to simply reproduce our DAG. 


Our main predictor is communicative attempt (CommAtt). To account for confounders - variables affecting both the predictor and the dependent variable - we need to adjust for them in the model by including them as covariates to isolate the effect of the predictor. Based on our DAG, confounders include:

1. familiarity
2. big5
3. expressibility
4. trial number
5. modality
6. concept
7. participant

We include 1.-5. as fixed factors. For 6.-7., we include varying intercepts as we expect that each participant and concept may have they own baseline level of effort and thus allow for individual variation. Partial pooling is also beneficial in that extreme values (or categories will fewer datapoints) will be shrunk toward the overal average.

### Model 1 - DAG

The model looks as follows:

$$
Eff_{ijk} = \beta_0 + \beta_1 CommAtt_{ijk} + \beta_2 Familiarity_{ijk} + \beta_3 Big5_{ijk} + \beta_4 Expressibility\_z_{ijk} + \beta_5 TrialNumber\_c_{ijk} + \beta_6 Modality_{ijk} + u_i + v_j + \epsilon_{ijk}
$$
where: 

- \( Eff_{ijk} \) is the effort for observation \( k \) of participant \( i \) and concept \( j \).  
- \( \beta_0 \) is the intercept.  
- \( \beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6 \) are the fixed effect coefficients for each predictor.  
- \( u_i \sim \mathcal{N}(0, \sigma_{Participant}^2) \) is the random intercept for participant \( i \).  
- \( v_j \sim \mathcal{N}(0, \sigma_{Concept}^2) \) is the random intercept for concept \( j \).  
- \( \epsilon_{ijk} \sim \mathcal{N}(0, \sigma^2) \) is the residual error term.  


This is the model without setting any priors, leaving them to default values
```{r eval=FALSE}

fit_eff_dag <- brm(Eff ~ 1 + CommAtt + Familiarity + Big5 + Expressibility_z + TrialNumber_c + Modality + (1 | Participant) + (1 | Concept),
                data = final_data,
                iter = 4000,
                cores = 4)

saveRDS(fit_eff_dag, here("09_Analysis_Modeling", "models", "fit_eff_dag.rds"))

beep(5)

```


```{r}

fit_eff_dag <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff_dag.rds"))

# summary
summary(fit_eff_dag)
# the coefficients again look similar to what we set in the data (except perhaps expressibility?)

plot(fit_eff_dag)
plot(conditional_effects(fit_eff_dag), points = TRUE)
# the CrI seem in sensible width

pp_check(fit_eff_dag, type = "dens_overlay")
# Looks good but not amazing - mostly because the posteriors seem to not know effort cannot be negative

pp_check(fit_eff_dag, type = "error_scatter_avg")
# blobby, but still correlated


```
Overall, we see good directions of all predictors, mostly also in accordance with the expected coefficients. Of course, the synthetic data is quite complex so there might be other dependencies that moderate the causal relationships and that is why we do not see exactly the numbers we use to create the data.

Let's have another model for comparison.

The fixed predictors make sense like this, as it will allow us to assess the effect of each on the effort, despite it not being our main research question. But we can assume that participants and concept have not only different baselines of effort (varying intercept). The effect of CommAtt on effort might vary across them too, hence we can try to add varying slopes for them and see whether the diagnostics improves. We will also add TrialNumber as a varying intercept, because we expect variation between earlier and later performances (because of learning, or opposite, fatigue) and we do not really need a single coefficient for this predictor anyway.

### Model 2 - varying slopes and intercepts

```{r}

# same like model 1 but pcn and concept have varying slopes and trnumber has var intercept, priors are default

fit_eff_2 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), 
                data = final_data,
                iter = 4000,
                cores = 4)

saveRDS(fit_eff_2, here("09_Analysis_Modeling", "models", "fit_eff_2.rds"))

beep(5)

```

```{r}

fit_eff_2 <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff_2.rds"))

# summary
summary(fit_eff_2)
# the coefficients again look similar

plot(fit_eff_2)
plot(conditional_effects(fit_eff_2), points = TRUE)
# the CrI seem in sensible width

pp_check(fit_eff_2, type = "dens_overlay")
# no change really

pp_check(fit_eff_2, type = "error_scatter_avg")
# blobby, but still correlated


```

### Model 3 - no correlation coefficient

According to the ppcheck, we are getting some troubles from the correlation coefficients between slopes and intercept for participant. Let's get rid of it and add it later to our diagnostics. Priors are still default.

```{r eval=FALSE}

fit_eff3 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                iter = 4000,
                cores = 4)
                

saveRDS(fit_eff3, here("09_Analysis_Modeling", "models", "fit_eff3.rds"))

beep(5)

```

```{r}

fit_eff3 <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff3.rds"))

# summary
summary(fit_eff3)
# the coefficients again look similar for most, but for the fixed factors it looks better than in the previous model (meaning closer to the coefficients we had in mind when creating the synthetic data). Rhat also looks good

plot(fit_eff3)

plot(conditional_effects(fit_eff3), points = TRUE)
# the CrI seem in sensible width, here we finally see the modality effect we coded in the data

pp_check(fit_eff3, type = "dens_overlay")
# no change really

pp_check(fit_eff3, type = "error_scatter_avg")
# blobby, but still correlated
# positive correlation means that errors increase with predicted values. So the model does perform well for some range, but becomes less reliable with increase in the predicted values
# however blob suggests there is no clear pattern so the model does capture some variability
# also the blob is centered around 0 which is good

# it could be we are ignoring some interaction terms or non-linearity (which we know we kind of do). Transformation could also help (e.g., log). Of course, we are also still not specifying any priors so let's not yet make it a disaster

```

### Model 3.1 - adding priors

```{r}

priors_eff <- c(
  set_prior("normal(2.5, 0.5)", class = "Intercept", lb=0),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt2M1"),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt3M2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality1"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Big5"),
  set_prior("normal(0,0.25)", class = "b", coef = "Familiarity"),
  set_prior("normal(0,0.25)", class = "b", coef = "Expressibility_z"),
  
  set_prior("normal(0.5,0.1)", class = "sd", group = "TrialNumber_c"),
  set_prior("normal(0.5,0.1)", class = "sd", group = "Participant"),
  set_prior("normal(0.5,0.1)", class = "sd", group = "Concept"),
  set_prior("normal(1,0.1)", class = "sd"),
  
  set_prior("normal(0.5,0.25)", class = "sigma")
)

```

Now we add the priors to the last used model
```{r}

fit_eff3p <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                prior=priors_eff,
                family = gaussian,
                iter = 4000,
                cores = 4)

saveRDS(fit_eff3p, here("09_Analysis_Modeling", "models", "fit_eff3p.rds"))


```

```{r}

fit_eff3p <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff3p.rds"))

# summary
summary(fit_eff3p)
# the coefficients - esp for fixed factors (big5) improved even slightly

plot(fit_eff3p)

plot(conditional_effects(fit_eff3p), points = TRUE)

pp_check(fit_eff3p, type = "dens_overlay")
# no change really

pp_check(fit_eff3p, type = "error_scatter_avg")
```

### Model 4 - tightening priors

So there are still things to improve, one of them is the fact that it seems that effort intercept can be of negative values. No matter what feature will be modelled, this is never the case, so we should inform our priors in such a way that these cases are extremely unlikely

Thus, we now make the sd priors even more tigther
```{r}

priors_eff_t <- c(
  set_prior("normal(2.5, 0.5)", class = "Intercept", lb=0),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt2M1"),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt3M2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality1"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Big5"),
  set_prior("normal(0,0.25)", class = "b", coef = "Familiarity"),
  set_prior("normal(0,0.25)", class = "b", coef = "Expressibility_z"),
  
  set_prior("normal(0.5,0.05)", class = "sd", group = "TrialNumber_c"),
  set_prior("normal(0.5,0.05)", class = "sd", group = "Participant"),
  set_prior("normal(0.5,0.05)", class = "sd", group = "Concept"),
  set_prior("normal(1,0.05)", class = "sd"),
  
  set_prior("normal(0.5,0.1)", class = "sigma")
)

```

```{r}

fit_eff4 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                prior=priors_eff_t,
                family = gaussian,
                iter = 4000,
                cores = 4)

saveRDS(fit_eff4, here("09_Analysis_Modeling", "models", "fit_eff4.rds"))


```

```{r}

fit_eff4 <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff4.rds"))

# summary
summary(fit_eff4)
# the coefficients - esp for fixed factors (big5) improved even slightly

plot(fit_eff4)

plot(conditional_effects(fit_eff4), points = TRUE)

pp_check(fit_eff4, type = "dens_overlay")
# still negative values

pp_check(fit_eff4, type = "error_scatter_avg")

```

### Model 5 - student family

So we still see negative values in the posterior simulations, so let's try Student's t-distribution which is more robut to outliers and can potentially reduce the likelihood of negative values (if we reduce degrees of freedom)

(Note that log family could also help but for now we don't want to transform anything)

```{r}

priors_eff_5 <- c(
  set_prior("normal(2.5, 0.5)", class = "Intercept", lb=0),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt2M1"),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt3M2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality1"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Big5"),
  set_prior("normal(0,0.25)", class = "b", coef = "Familiarity"),
  set_prior("normal(0,0.25)", class = "b", coef = "Expressibility_z"),
  
  set_prior("normal(0.5,0.05)", class = "sd", group = "TrialNumber_c"),
  set_prior("normal(0.5,0.05)", class = "sd", group = "Participant"),
  set_prior("normal(0.5,0.05)", class = "sd", group = "Concept"),
  set_prior("normal(1,0.05)", class = "sd"),
  
  set_prior("normal(0.5,0.1)", class = "sigma"),
  set_prior("gamma(2, 0.1)", class = "nu")  # Prior for degrees of freedom
  
)

```

```{r}

fit_eff5 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                prior=priors_eff_5,
                family = student,
                iter = 4000,
                cores = 4)

saveRDS(fit_eff5, here("09_Analysis_Modeling", "models", "fit_eff5.rds"))


```

```{r}

fit_eff5 <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff5.rds"))

# summary
summary(fit_eff5)
# maybe the coeffs are already too tight here?
# careful, as with the last one, the Rhat for Intercept is slightly >1.01

plot(fit_eff5)

plot(conditional_effects(fit_eff5), points = TRUE)

pp_check(fit_eff5, type = "dens_overlay")
# still negative values

pp_check(fit_eff5, type = "error_scatter_avg")

```

### Model 6 - adapting priors

The negative values in the posterior are really persistent so let's try to play with the priors once more, for the gaussian family

```{r}

priors_eff6 <- c(
  set_prior("normal(3, 0.3)", class = "Intercept", lb=0),
  set_prior("normal(0,0.25)", class = "b", coef = "CommAtt2M1"),
  set_prior("normal(0,0.25)", class = "b", coef = "CommAtt3M2"),
  set_prior("normal(0,0.15)", class = "b", coef = "Modality1"),
  set_prior("normal(0,0.15)", class = "b", coef = "Modality2"),
  set_prior("normal(0,0.15)", class = "b", coef = "Big5"),
  set_prior("normal(0,0.15)", class = "b", coef = "Familiarity"),
  set_prior("normal(0,0.15)", class = "b", coef = "Expressibility_z"),
  
  set_prior("normal(0.5,0.05)", class = "sd", group = "TrialNumber_c"),
  set_prior("normal(0.5,0.05)", class = "sd", group = "Participant"),
  set_prior("normal(0.5,0.05)", class = "sd", group = "Concept"),
  set_prior("normal(1,0.05)", class = "sd"),
  
  set_prior("normal(0.5,0.1)", class = "sigma")
)

```

```{r}

fit_eff6 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                prior=priors_eff6,
                family = gaussian,
                iter = 4000,
                cores = 4)

saveRDS(fit_eff6, here("09_Analysis_Modeling", "models", "fit_eff6.rds"))


```

```{r}

fit_eff6 <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff6.rds"))

# summary
summary(fit_eff6)
# all CrI seems ok and in expected range
# Rhat is ok

plot(fit_eff6)

plot(conditional_effects(fit_eff6), points = TRUE)

pp_check(fit_eff6, type = "dens_overlay")
# still negative values

pp_check(fit_eff6, type = "error_scatter_avg")

```

### Model 7 - restricting priors with exponential distribution

Let's do one more test with the priors, tightening them with exponential distribution

```{r}

priors_eff7 <- c(
  set_prior("normal(3, 0.3)", class = "Intercept", lb = 0),
  set_prior("normal(0, 0.25)", class = "b", coef = "CommAtt2M1"),
  set_prior("normal(0, 0.25)", class = "b", coef = "CommAtt3M2"),
  set_prior("normal(0, 0.15)", class = "b", coef = "Modality1"),
  set_prior("normal(0, 0.15)", class = "b", coef = "Modality2"),
  set_prior("normal(0, 0.15)", class = "b", coef = "Big5"),
  set_prior("normal(0, 0.15)", class = "b", coef = "Familiarity"),
  set_prior("normal(0, 0.15)", class = "b", coef = "Expressibility_z"),

  # Exponential priors for standard deviations
  set_prior("exponential(3)", class = "sd", group = "TrialNumber_c"), # exp(3) has a mean of 1/3 and concentrates most density around small values
  set_prior("exponential(3)", class = "sd", group = "Participant"),
  set_prior("exponential(3)", class = "sd", group = "Concept"),
  set_prior("exponential(1)", class = "sd"),  # Generic sd prior

  # Residual standard deviation - keep it narrow
  set_prior("normal(0.5, 0.1)", class = "sigma")
)
```

```{r}

fit_eff7 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                prior=priors_eff7,
                family = gaussian,
                iter = 4000,
                cores = 4)

saveRDS(fit_eff7, here("09_Analysis_Modeling", "models", "fit_eff7.rds"))


```

```{r}

fit_eff7 <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff7.rds"))

# summary
summary(fit_eff7)
# all CrI seems ok and in expected range
# Rhat is ok

plot(fit_eff7)

plot(conditional_effects(fit_eff7), points = TRUE)

pp_check(fit_eff7, type = "dens_overlay", ndraws = 300)
# still negative values

pp_check(fit_eff7, type = "error_scatter_avg")

```

### Model 8 - log-normal distribution

ADD TEST FOR LOGNORMAL DISTRIBUTION

```{r}

# same like model 1 but pcn and concept have varying slopes and trnumber has var intercept, priors are default

fit_eff8 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                family = lognormal(),
                iter = 4000,
                cores = 4)

saveRDS(fit_eff8, here("09_Analysis_Modeling", "models", "fit_eff8.rds"))

beep(5)

##TODO: we tried both | (with correlation) and || and with correlation seems much better in terms of ESS, even though this model stays anyway at the top as the most predictive. But we need to show it

```


```{r}

fit_eff8 <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff8.rds"))

# summary
summary(fit_eff8)
# this is for now uninterpretable, since it's on log-scale

posterior_summary(fit_eff8)
# likewise

plot(fit_eff8)
plot(conditional_effects(fit_eff8), points = TRUE)
# but the effects seem to have a good direction

pp_check(fit_eff8, type = "dens_overlay")
# this looks so much better
pp_check(fit_eff8, type = "error_scatter_avg")
# blobby, and the points are bit more dispersed, but correlation is still there


```

#### Converting to original scale

This is how we can get to the values of an effort under certain value of a predictor
```{r}

library(emmeans)

# Compute estimated marginal means on log-scale
em_mdl_fit_eff8 <- emmeans(fit_eff8, ~ CommAtt) #~ CommAtt

#Backtransform the post.beta values
em_mdl_fit_eff8@post.beta <- exp(em_mdl_fit_eff8@post.beta)
print(em_mdl_fit_eff8)
```
So we indeed see that effort in CommAtt2 increase but then decreases again for CommAtt3. Taking the results back to our coefficients when creating synthetic data
```{r}

coeff1 <- 2.31*1.5 
coeff2 <- 2.31*0.7

print(coeff1)
print(coeff2) # which is very close to the mean values we see from the model
```
Now let's try a different method to get the coefficients (code adapted from https://bruno.nicenboim.me/bayescogsci/ch-reg.html#sec-trial)

```{r}

# Extract posterior samples
alpha_samples <- as_draws_df(fit_eff8)$b_Intercept
beta_2_vs_1 <- as_draws_df(fit_eff8)$b_CommAtt2M1
beta_3_vs_2 <- as_draws_df(fit_eff8)$b_CommAtt3M2

# Compute expected values on the log scale
mu_1 <- alpha_samples  # CommAtt 1
mu_2 <- alpha_samples + beta_2_vs_1  # CommAtt 2
mu_3 <- alpha_samples + beta_2_vs_1 + beta_3_vs_2  # CommAtt 3

# Transform to original scale
effect_1 <- exp(mu_1)
effect_2 <- exp(mu_2)
effect_3 <- exp(mu_3)

# Calculate contrasts on the original scale
effect_diff_2_vs_1 <- effect_2 - effect_1
effect_diff_3_vs_2 <- effect_3 - effect_2
effect_diff_3_vs_1 <- effect_3 - effect_1

# Summarize the effects
list(
  mean_diff_2_vs_1 = c(mean = mean(effect_diff_2_vs_1), quantile(effect_diff_2_vs_1, c(0.025, 0.975))),
  mean_diff_3_vs_2 = c(mean = mean(effect_diff_3_vs_2), quantile(effect_diff_3_vs_2, c(0.025, 0.975))),
  mean_diff_3_vs_1 = c(mean = mean(effect_diff_3_vs_1), quantile(effect_diff_3_vs_1, c(0.025, 0.975)))
)

```
This is for all predictors (except concept and participant)

```{r}

# Extract posterior samples
posterior_samples <- as_draws_df(fit_eff8)
alpha_samples <- posterior_samples$b_Intercept

# Create a list to store effects for each fixed factor
effect_list <- list()

# Helper function to calculate summary statistics
get_effect_summary <- function(effect_samples) {
  mean_effect <- mean(effect_samples)
  se_effect <- sd(effect_samples)
  ci_effect <- quantile(effect_samples, c(0.025, 0.975))
  post_prob <- mean(effect_samples > 0)
  c(mean = mean_effect, 
    se = se_effect, 
    lower_ci = ci_effect[1], 
    upper_ci = ci_effect[2], 
    post_prob = post_prob)
}

# COMMATT (successive differences coding)
if ("b_CommAtt2M1" %in% colnames(posterior_samples) & "b_CommAtt3M2" %in% colnames(posterior_samples)) {
  beta_2_vs_1 <- posterior_samples$b_CommAtt2M1
  beta_3_vs_2 <- posterior_samples$b_CommAtt3M2
  
  mu_1 <- alpha_samples
  mu_2 <- alpha_samples + beta_2_vs_1
  mu_3 <- alpha_samples + beta_2_vs_1 + beta_3_vs_2
  
  effect_list$CommAtt <- rbind(
    "2 vs 1" = get_effect_summary(exp(mu_2) - exp(mu_1)),
    "3 vs 2" = get_effect_summary(exp(mu_3) - exp(mu_2)),
    "3 vs 1" = get_effect_summary(exp(mu_3) - exp(mu_1))
  )
}

# MODALITY (sum contrasts scaled by 0.5)
if ("b_Modality1" %in% colnames(posterior_samples) & "b_Modality2" %in% colnames(posterior_samples)) {
  beta_mod_1 <- posterior_samples$b_Modality1
  beta_mod_2 <- posterior_samples$b_Modality2
  
  mu_mod_1 <- alpha_samples + beta_mod_1
  mu_mod_2 <- alpha_samples + beta_mod_2
  mu_mod_3 <- alpha_samples - beta_mod_1 - beta_mod_2
  
  effect_list$Modality <- rbind(
    "1 vs 2" = get_effect_summary(exp(mu_mod_1) - exp(mu_mod_2)),
    "1 vs 3" = get_effect_summary(exp(mu_mod_1) - exp(mu_mod_3)),
    "2 vs 3" = get_effect_summary(exp(mu_mod_2) - exp(mu_mod_3))
  )
}

# BIG5 (continuous)
if ("b_Big5" %in% colnames(posterior_samples)) {
  beta_big5 <- posterior_samples$b_Big5
  effect_list$Big5 <- get_effect_summary(exp(alpha_samples + beta_big5) - exp(alpha_samples))
}

# FAMILIARITY (continuous)
if ("b_Familiarity" %in% colnames(posterior_samples)) {
  beta_fam <- posterior_samples$b_Familiarity
  effect_list$Familiarity <- get_effect_summary(exp(alpha_samples + beta_fam) - exp(alpha_samples))
}

# EXPRESSIBILITY_Z (continuous)
if ("b_Expressibility_z" %in% colnames(posterior_samples)) {
  beta_expr <- posterior_samples$b_Expressibility_z
  effect_list$Expressibility_z <- get_effect_summary(exp(alpha_samples + beta_expr) - exp(alpha_samples))
}

# TRIAL NUMBER (centered continuous)
if ("b_TrialNumber_c" %in% colnames(posterior_samples)) {
  beta_trial <- posterior_samples$b_TrialNumber_c
  effect_list$TrialNumber_c <- get_effect_summary(exp(alpha_samples + beta_trial) - exp(alpha_samples))
}

# Convert to a nicely formatted data frame
effect_df <- do.call(rbind, effect_list)

# View effects
effect_df



```
### Model 8.1 - with correlation
```{r}


fit_eff8c <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), 
                data = final_data,
                family = lognormal(),
                iter = 4000,
                cores = 4)

saveRDS(fit_eff8c, here("09_Analysis_Modeling", "models", "fit_eff8c.rds"))

beep(5)

##TODO: we tried both | (with correlation) and || and with correlation seems much better in terms of ESS, even though this model stays anyway at the top as the most predictive. But we need to show it

```

```{r}

fit_eff8c <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff8c.rds"))

# summary
summary(fit_eff8c)
# this is for now uninterpretable, since it's on log-scale

posterior_summary(fit_eff8c)
# likewise

plot(fit_eff8c)
plot(conditional_effects(fit_eff8c), points = TRUE)
# but the effects seem to have a good direction

pp_check(fit_eff8c, type = "dens_overlay")
# this looks so much better
pp_check(fit_eff8c, type = "error_scatter_avg")
# blobby, and the points are bit more dispersed, but correlation is still there


```

Let's now check which model has the best diagnostics

### Diagnostics I

```{r}

model_list <- list(fit_eff_dag, fit_eff_2, fit_eff3, fit_eff3p, fit_eff4, fit_eff5, fit_eff6, fit_eff7, fit_eff8)

```

#### R^2

todo: add r squared




#### Rhat

```{r}

# Extract R-hat values for each model
rhat_list <- lapply(model_list, function(model) {
  rhat_values <- rhat(model)
  data.frame(model = deparse(substitute(model)), 
             max_rhat = max(rhat_values), 
             min_rhat = min(rhat_values))
})

# Combine and inspect
do.call(rbind, rhat_list)
```
So model fit_eff_2 and fit_eff6 look the worst. All the rest is comparably OK.

#### ESS

Effective sample size tells how many independent samples the model has effectively drawn from the PD. Low ESS suggests autocorrelation (i.e., sample explores one part of posterior), while high ESS means good mix

```{r}

# Extract n_eff values for each model
neff_ratio_list <- lapply(model_list, function(model) {
  neff_values <- neff_ratio(model)              # Here we calculate ratio (not the raw number of effective samples)
  data.frame(model = deparse(substitute(model)), 
             min_neff = min(neff_values), 
             max_neff = max(neff_values),
             mean_neff = mean(neff_values))
               
})

# Combine and inspect
do.call(rbind, neff_ratio_list)

```

So the highest ratio have model fit_eff8 but in fact they are all quite comparable. Let's loot at 5 highest

```{r}

effective_sample(fit_eff8) # this one indeed seems the best as it has all ESS around 10k
effective_sample(fit_eff3)
effective_sample(fit_eff7) 
effective_sample(fit_eff3p) 
effective_sample(fit_eff_dag)

```

#### LOO & WAIC

```{r}

# First we need to add the loo and waic to the model objects (recommended workflow)
fit_eff_dag <- add_criterion(fit_eff_dag, criterion = c("loo", "waic"))
fit_eff_2 <- add_criterion(fit_eff_2, criterion = c("loo", "waic")) #Warnung: 65 (1.3%) p_waic estimates greater than 0.4. We recommend trying loo instead.
fit_eff3 <- add_criterion(fit_eff3, criterion = c("loo", "waic")) #Warnung: 7 (0.1%) p_waic estimates greater than 0.4. We recommend trying loo instead.
fit_eff3p <- add_criterion(fit_eff3p, criterion = c("loo", "waic")) #Warnung: 20 (0.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.
fit_eff4 <- add_criterion(fit_eff4, criterion = c("loo", "waic")) #Warnung: 53 (1.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.
fit_eff5 <- add_criterion(fit_eff5, criterion = c("loo", "waic"))
fit_eff6 <- add_criterion(fit_eff6, criterion = c("loo", "waic")) #Warnung: 54 (1.1%) p_waic estimates greater than 0.4. We recommend trying loo instead.
fit_eff7 <- add_criterion(fit_eff7, criterion = c("loo", "waic")) #Warnung: 7 (0.1%) p_waic estimates greater than 0.4. We recommend trying loo instead
fit_eff8 <- add_criterion(fit_eff8, criterion = c("loo", "waic")) #Warnung: 11 (0.2%) p_waic estimates greater than 0.4. We recommend trying loo instead.
fit_eff8c <- add_criterion(fit_eff8c, criterion = c("loo", "waic")) #Warnung: 13 (0.3%) p_waic estimates greater than 0.4. We recommend trying loo instead.
```
```{r}

# This is how to access the criteria
fit_eff8$criteria$waic
fit_eff8$criteria$loo

```

Leave-one-out (loo) validation
```{r}

l <- loo_compare(fit_eff_dag, fit_eff_2, fit_eff3, fit_eff3p, fit_eff4, fit_eff5, fit_eff6, fit_eff7, fit_eff8, criterion = "loo")

print(l, simplify = F)

```
elpd_loo: This is the expected log pointwise predictive density for LOO. Higher values indicate a better fit to the data.

se_elpd_loo: The standard error of the elpd_loo, representing uncertainty in the model’s predictive fit according to LOO.

looic: The LOO Information Criterion, which is similar to waic but based on leave-one-out cross-validation. Lower values are better.

p_loo: The effective number of parameters according to LOO, indicating the model’s complexity.

se_p_loo: The standard error of p_loo, representing uncertainty around the effective number of parameters.

So lognormal seems the best (and gaussian without priors and student-t with tighter priors follow shortly). 


Information criterion (WAIC)
```{r}

w <- loo_compare(fit_eff_dag, fit_eff_2, fit_eff3, fit_eff3p, fit_eff4, fit_eff5, fit_eff6, fit_eff7, fit_eff8, criterion = "waic")

print(w, simplify = F)

# see Solomon Kurz
cbind(waic_diff = w[,1] * -2,
      se = w[,2] * 2)

```
elpd_waic (expected log pointwise predictive density for WAIC): This represents the model's predictive fit to the data. Higher values indicate a better fit.

se_elpd_waic (standard error of elpd_waic): Measures uncertainty around the elpd_waic estimate.

waic: The Widely Applicable Information Criterion, a measure of model fit where lower values indicate a better fit.

se_waic (standard error of WAIC): Uncertainty around the WAIC estimate.

elpd_diff: The difference in the elpd_waic between the model in question and the baseline model (fit_eff_2, which has elpd_diff of 0). A negative value indicates that the model fits worse than fit_eff_2.

se_diff: The standard error of the elpd_diff, indicating how much uncertainty there is in the difference in predictive performance.

p_waic: The number of effective parameters in the model (related to model complexity). Lower values indicate simpler models, and higher values suggest more complexity.

Plot the comparison
```{r}

library(tibble)
library(tidyverse)
library(rcartocolor)

w[, 7:8] %>% 
  data.frame() %>% 
  rownames_to_column("model_name") %>% 
  mutate(model_name = fct_reorder(model_name, waic, .desc = T)) %>% 
  
  ggplot(aes(x = waic, y = model_name, 
             xmin = waic - se_waic, 
             xmax = waic + se_waic)) +
  geom_pointrange(color = carto_pal(7, "BurgYl")[7], 
                  fill = carto_pal(7, "BurgYl")[5], shape = 21) +
  labs(title = "WAIC plot",
       x = NULL, y = NULL) +
  theme(axis.ticks.y = element_blank())

```
```{r}

model_weights(fit_eff_dag, fit_eff_2, fit_eff3, fit_eff3p, fit_eff4, fit_eff5, fit_eff6, fit_eff7, fit_eff8, weights = "waic") %>% 
  round(digits = 2)

```

So as ppcheck already suggested, lognormal model indeed seem to have the most predictive power. For this particular (synthetic) data, we will now proceed with model fit_eff8.

We will first add some mildly informative priors, and then we also try to add some interaction terms and do a comparison once again.

### Model 9 - lognormal with priors

Let's first check what priors have been selected as default for fit_eff8
```{r}

# Print priors
prior_summary(fit_eff8)

```
Ok, we can keep all defaulted ones, but we do not need to leave flat priors for the beta coefficients as we do have some assumptions/expectations

Since fit_eff3p and fit_eff5 are models with non-default priors that have the best diagnostics after fit_eff8 - and the priors have, in fact, the same values, we will just re-use them again

```{r}

priors_eff_9 <- c(
  set_prior("normal(2.5, 0.5)", class = "Intercept", lb=0),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt2M1"),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt3M2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality1"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Big5"),
  set_prior("normal(0,0.25)", class = "b", coef = "Familiarity"),
  set_prior("normal(0,0.25)", class = "b", coef = "Expressibility_z")
)

# The rest we will leave default (and check afterwards)
```

```{r}

fit_eff9 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), 
                data = final_data,
                family = lognormal(),
                prior = priors_eff_9,
                iter = 4000,
                cores = 4)

# we will immediatelly add the diagnostic criteria
fit_eff9 <- add_criterion(fit_eff9, criterion = c("loo", "waic"))  #Warnung: 12 (0.2%) p_waic estimates greater than 0.4. We recommend trying loo instead.

# and save it
saveRDS(fit_eff9, here("09_Analysis_Modeling", "models", "fit_eff9.rds"))

beep(5)

```
Check the priors

```
Check out: https://paulbuerkner.com/brms/reference/set_prior.html
```

```{r}

prior_summary(fit_eff9)

pp_check(fit_eff9,
         type = "stat",
         stat = "mean",
         bins = 50,
         prefix = "ppd") +
  coord_cartesian(xlim = c(-10, 10)) +
  ggtitle("Prior predictive distribution of means")
# this look okay

pp_check(fit_eff9,
         type = "stat",
         stat = "min",
         prefix = "ppd") +
  coord_cartesian(xlim = c(-15, 10)) +
  ggtitle("Prior predictive distribution of minimal values")
# this is better but we still see some 0s

pp_check(fit_eff9,
         type = "stat",
         stat = "max",
         prefix = "ppd") +
  coord_cartesian(xlim = c(-10, 15)) +
  ggtitle("Prior predictive distribution of maximal values")
# this looks reasonable



```


```{r}

fit_eff9 <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff9.rds"))

# summary
summary(fit_eff9)
# this is for now uninterpretable, since it's on log-scale
# buth Rhat looks ok

posterior_summary(fit_eff9)
# likewise

plot(fit_eff9)
plot(conditional_effects(fit_eff9), points = TRUE)
# but the effects seem to have a good direction

pp_check(fit_eff9, type = "dens_overlay")
# this looks so much better
pp_check(fit_eff9, type = "error_scatter_avg")
# blobby, and the points are bit more dispersed, but correlation is still there


```
### Model 10 - adding interactions

Possible interactions include:

- CommAtt x Modality - Effort could increase differently across modalities, depending on whether concept is guessed on the first, second or third attempt. E.g., gesture might require more effort on initial attempt, but vocal require more effort in repeated attempt

Note: Interesting, it would help disentagle the benefit of multimodality over other - maybe more effort overal but less attempts. However, we already model effect of modality on effort, so maybe this is not top priority

- CommAtt x Expressibility - Higher expressibility should moderate the effect of repeated attempts, such that the increase in effort with each additional attempt is smaller (or bigger?) for more expressible concepts.

Note: Not priority I would say

- Modality × Expressibility_z - The influence of expressibility on effort could be modality-specific — perhaps effort increases less with expressibility in the combined modality.

Note: Not priority (especially since expressibiliy has already modality encoded)

- Familiarity x CommAtt - More familiar partners may guess faster (fewer attempts) and require less effort, but this effect could diminish over multiple attempts.

Note: Does not seem to be priority

- Big5 × Modality or Big5 × CommAtt - More open/extraverted participants might maintain higher effort over attempts, or adjust more dynamically depending on the communicative channel.

Note: Not priority, but it is interesting if we want to tap more into the interindividual variability

Let's try CommAtt x Modality and Big5 x CommAtt

```{r}

fit_eff10 <- brm(Eff ~ 1 + CommAtt * Modality + Big5 * CommAtt + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), 
                data = final_data,
                family = lognormal(),
                prior = priors_eff_9,
                iter = 4000,
                cores = 4)

# we will immediatelly add the diagnostic criteria
fit_eff10 <- add_criterion(fit_eff10, criterion = c("loo", "waic"))  #Warnung: 19 (0.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.

# and save it
saveRDS(fit_eff10, here("09_Analysis_Modeling", "models", "fit_eff10.rds"))

beep(5)

#Warnung: Found 1 observations with a pareto_k > 0.7 in model 'fit_eff10'. 
```

```{r}

fit_eff10 <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff10.rds"))

# summary
summary(fit_eff10)
# Rhat ok

posterior_summary(fit_eff10)
# likewise

plot(fit_eff10) # all catterpillars seem ok
plot(conditional_effects(fit_eff10), points = TRUE)
# but the effects seem to have a good direction

pp_check(fit_eff10, type = "dens_overlay")
# this looks so much better
pp_check(fit_eff10, type = "error_scatter_avg")
# blobby, and the points are bit more dispersed, but correlation is still there


```
### Diagnostics II

```{r}

effective_sample(fit_eff8) 
effective_sample(fit_eff8c)
effective_sample(fit_eff9) 
effective_sample(fit_eff10) 


# They all look comparably good, fit_eff8 have lots of ESS, but our last model with interactions is also good
```

```{r}

l <- loo_compare(fit_eff8, fit_eff8c, fit_eff9, fit_eff10, criterion = "loo")

print(l, simplify = F)

```
```{r}

w <- loo_compare(fit_eff8, fit_eff8c, fit_eff9, fit_eff10, criterion = "waic")

print(w, simplify = F)

# see Solomon Kurz
cbind(waic_diff = w[,1] * -2,
      se = w[,2] * 2)

```
So the interaction terms really help for the predictive power and we will stick to it for now

# DAG - Question 2

Now we can need to account also for our second hypothesis, namely

*H2: The enhancement depends on similarity of the guesser's answer and the original meaning. More similar answer will require/result in smaller enhancement (but still enhancement) than less similar answer.*

```{r}

dag2 <- dagitty('dag {
Big5 [adjusted,pos="-0.823,0.657"]
CommAtt [exposure,pos="-1.033,0.028"]
Conc [adjusted,pos="-1.136,-0.848"]
Eff [outcome,pos="-0.102,0.025"]
Expr [adjusted,pos="-0.758,-0.850"]
Fam [adjusted,pos="-0.379,0.663"]
Pcn [adjusted,pos="-0.589,1.214"]
TrNum [adjusted,pos="-1.686,-0.859"]
PrevAn [adjusted,pos="-0.500,-0.200"]
Big5 -> CommAtt     
Big5 -> Eff
CommAtt -> Eff
Conc -> Expr
Expr -> CommAtt
Expr -> Eff
Fam -> CommAtt
Fam -> Eff
Mod_bin -> Eff
Pcn -> Big5
Pcn -> CommAtt 
Pcn -> Eff
Pcn -> Fam
TrNum -> CommAtt
TrNum -> Eff
Conc -> CommAtt
Conc -> Eff
PrevAn -> Eff
PrevAn -> CommAtt
Expr -> PrevAn
TrNum -> PrevAn
}')

plot(dag2)

```
```{r}

impliedConditionalIndependencies(dag2)

```

```{r}

dagitty::adjustmentSets(dag2, exposure = "CommAtt", outcome = "Eff")

```

# Adding similarity to synthetic data

```{r}

# Set seed for reproducibility
set.seed(0209)

# Define participants, total unique concepts, and modalities
n_participants <- 120
n_total_concepts <- 21  # Total unique concepts
n_concepts_per_participant <- 21  # Each participant works with 21 concepts
n_modalities <- 3  # gesture, vocal, combined

# Generate participant IDs
participants <- 1:n_participants

# Simulate Big5 personality traits (standardized between 0 and 1) and Familiarity (between 0 and 1) for participants
Big5 <- runif(n_participants, min = 0, max = 1)  # Continuous values between 0 and 1
Familiarity <- runif(n_participants, min = 0, max = 1)  # Continuous values between 0 and 1

# Create a matrix to hold expressibility values for each concept in each modality
expressibility_matrix <- matrix(runif(n_total_concepts * n_modalities, min = 0, max = 1), nrow = n_total_concepts, ncol = n_modalities)

# Randomly sample 21 unique concepts for each participant
final_data_2 <- data.frame()

# Define a function to assign CommAtt and Eff for a single participant
simulate_participant <- function(participant_id) {
  # Randomly sample 21 unique concepts from the total pool of 84
  selected_concepts <- sample(1:n_total_concepts, n_concepts_per_participant)
  
  participant_data <- data.frame()
  trial_number <- 1  # Initialize trial number
  prev_answer_similarity <- NA  # Initialize PrevAn for the first trial
  
  for (concept_id in selected_concepts) {
    # Randomly determine the modality for the concept
    modality <- sample(c("gesture", "vocal", "combined"), 1)
    
    # Calculate expressibility based on modality
    expressibility_score <- ifelse(modality == "vocal", expressibility_matrix[concept_id, 1] * 0.6, 
                                    ifelse(modality == "gesture", expressibility_matrix[concept_id, 2], 
                                           expressibility_matrix[concept_id, 3] * 1.5))
    
    # Determine Communicative Attempts based solely on expressibility, familiarity, and Big5
    base_prob <- c(0.33, 0.33, 0.33)  # Equal chance for 1, 2, or 3 attempts
    
    # Modify probabilities based on familiarity, Big5, and expressibility
    adjusted_prob <- base_prob * c(1 - Familiarity[participant_id], 
                                    1 - Familiarity[participant_id],
                                    1 - Familiarity[participant_id]) * 
                     c(1 - Big5[participant_id],
                       1 - Big5[participant_id],
                       1 - Big5[participant_id]) * 
                     c(1 - expressibility_score,
                       1 - expressibility_score,
                       1 - expressibility_score)
    
    # Normalize the adjusted probabilities
    adjusted_prob <- adjusted_prob / sum(adjusted_prob)
    
    # Sample the number of communicative attempts based on adjusted probabilities
    n_attempts <- sample(1:3, 1, prob = adjusted_prob)
    
    # Loop through the number of attempts and increment CommAtt correctly
    for (attempt in 1:n_attempts) {
      # Calculate Eff for the first attempt
      if (attempt == 1) {
        Eff <- 1.15 * Big5[participant_id] + 
               1.10 * Familiarity[participant_id] + 
               1.20 * expressibility_score + 
               rnorm(1, mean = 1, sd = 0.5)
        
        # Adjust Eff based on modality
        if (modality == "combined") {
          Eff <- Eff * 0.7  # Slight moderation for combined modality
        }
        
        # Set PrevAn to NA for the first attempt
        prev_answer_similarity <- NA
      }
      
      # Adjust Eff for subsequent attempts
      if (attempt == 2) {
        Eff <- 1.15 * Big5[participant_id] + 
               1.10 * Familiarity[participant_id] + 
               1.20 * expressibility_score + 
               rnorm(1, mean = 1, sd = 0.5)
        Eff <- Eff * 1.50  # Multiply effort by 1.50 for the second attempt
      } else if (attempt == 3) {
        Eff <- 1.15 * Big5[participant_id] + 
               1.10 * Familiarity[participant_id] + 
               1.20 * expressibility_score + 
               rnorm(1, mean = 1, sd = 0.5)
        Eff <- Eff * 0.70  # Multiply effort by 0.70 for the third attempt
      }
      
      # Adjust Eff based on PrevAn (previous answer similarity)
      if (!is.na(prev_answer_similarity)) {
        Eff <- Eff * (1 + (1 - prev_answer_similarity) * 0.5)  # 0.5 is the coefficient for the modulation effect
      }
      
      # Create row for each attempt
      participant_data <- rbind(participant_data, data.frame(
        Participant = participant_id,
        Concept = concept_id,
        Modality = modality,
        Big5 = Big5[participant_id],
        Familiarity = Familiarity[participant_id],
        Expressibility = expressibility_score,
        CommAtt = attempt,  # Correctly set the attempt number
        Eff = Eff,
        TrialNumber = trial_number,  # Set trial number for this attempt
        PrevAn = prev_answer_similarity  # Add PrevAn value for the current attempt
      ))
      
      # Increment the trial number after each attempt
      trial_number <- trial_number + 1
      
      # Update PrevAn similarity for the next attempt
      prev_answer_similarity <- runif(1, min = 0, max = 1)  # Simulate similarity for the next attempt
    }
  }
  
  return(participant_data)
}

# Simulate data for all participants
for (i in participants) {
  final_data_2 <- rbind(final_data_2, simulate_participant(i))
}

# Preview the first few rows of the final data
head(final_data_2)


```

```{r}

hist(final_data_2$Eff)
```


Because we are dealing xxx

```{r}

final_data_2 <- final_data_2 %>%
  group_by(Participant, Concept) %>%
  mutate(
    Effort_1 = Eff[CommAtt == 1][1],  # Effort for attempt 1
    Effort_2 = Eff[CommAtt == 2][1],  # Effort for attempt 2
    Effort_3 = Eff[CommAtt == 3][1],  # Effort for attempt 3
    Effort_Change_Ratio_1_to_2 = case_when(
      CommAtt == 2 & !is.na(Effort_1) ~ Eff / Effort_1,
      TRUE ~ NA_real_
    ),
    Effort_Change_Ratio_2_to_3 = case_when(
      CommAtt == 3 & !is.na(Effort_2) ~ Eff / Effort_2,
      TRUE ~ NA_real_
    )
  ) %>%
  select(-Effort_1, -Effort_2, -Effort_3) %>%  # Clean up intermediate columns
  ungroup()

# View the result
head(final_data_2)


```

```{r}

final_data_2 <- final_data_2 %>%
  mutate(
    Effort_Change_Ratio = coalesce(Effort_Change_Ratio_1_to_2, Effort_Change_Ratio_2_to_3)
  ) %>%
  select(-Effort_Change_Ratio_1_to_2, -Effort_Change_Ratio_2_to_3)

# View the result
head(final_data_2)

```




Let's check in plots
```{r}

# Filter out CommAtt == 1
filtered_data <- final_data_2[final_data_2$CommAtt != 1, ]

# Scatter plot with regression line
library(ggplot2)

ggplot(filtered_data, aes(x = PrevAn, y = Eff)) +
  geom_point(alpha = 0.6, color = "blue") +  # Scatter points
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Regression line
  labs(x = "Previous Answer Similarity (PrevAn)", 
       y = "Effort (Eff)", 
       title = "Relationship between Effort and Previous Answer Similarity") +
  theme_minimal()


```
```{r}


# Filter out CommAtt == 1
filtered_data <- final_data_2[final_data_2$CommAtt != 1, ]

# Scatter plot with regression line
library(ggplot2)

ggplot(filtered_data, aes(x = PrevAn, y = Effort_Change_Ratio)) +
  geom_point(alpha = 0.6, color = "blue") +  # Scatter points
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Regression line
  labs(x = "Previous Answer Similarity (PrevAn)", 
       y = "Effort (Eff)", 
       title = "Relationship between Effort and Previous Answer Similarity") +
  theme_minimal()



```


```{r}

hist(filtered_data$Effort_Change_Ratio)

```

# Modeling - Question 2

## Contrast coding

Convert collumns to factors
```{r}

filtered_data$CommAtt <- as.factor(filtered_data$CommAtt)
filtered_data$Modality <- as.factor(filtered_data$Modality)
filtered_data$Participant <- as.factor(filtered_data$Participant)
filtered_data$Concept <- as.factor(filtered_data$Concept)

filtered_data$TrialNumber <- as.numeric(filtered_data$TrialNumber)  # Ensure TrialNumber is numeric
```

Check contrasts of factors
```{r}
contrasts(filtered_data$CommAtt) <- MASS::contr.sdif(2)
contrasts(filtered_data$Modality) <- contr.sum(3)/2
```


Center trial number
```{r}
filtered_data$TrialNumber_c <- filtered_data$TrialNumber - median(range(filtered_data$TrialNumber))
range(filtered_data$TrialNumber_c)
range(filtered_data$TrialNumber)
```
Standardize (z-score) all *continuous* predictors. 

The measures of familiarity, expressibility, and big5 are on a non-continuous rating scale, hence we can just subtract the median from these to centre them (instead of standardizing).

```{r}

filtered_data$Familiarity <- filtered_data$Familiarity - median(range(filtered_data$Familiarity))
filtered_data$Big5 <- filtered_data$Big5 - median(range(filtered_data$Big5))
```


Z-score expressibility (because it's continuous) within a modality
```{r}

filtered_data <-
  filtered_data |>
  group_by(Modality) |>
  mutate(Expressibility_z = (Expressibility - mean(Expressibility))/ sd(filtered_data$Expressibility, na.rm = T)) |>
  ungroup()

filtered_data <-
  filtered_data |>
  #group_by(Modality) |>
  mutate(PrevAn_z = (PrevAn - mean(PrevAn))/ sd(filtered_data$PrevAn, na.rm = T)) |>
  ungroup()
```

## Model - DAG

```{r}


fit_eff_prevan_dag <- brm(Effort_Change_Ratio ~ 1 + PrevAn_z + CommAtt + Familiarity + Big5 + Expressibility_z + TrialNumber_c + Modality + (1 | Participant) + (1 | Concept),
                data = filtered_data,
                iter = 4000,
                cores = 4)

saveRDS(fit_eff_prevan_dag, here("09_Analysis_Modeling", "models", "fit_eff_prevan_dag.rds"))

beep(5)


```

```{r}

fit_eff_prevan_dag <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff_prevan_dag.rds"))

# summary
summary(fit_eff_prevan_dag)

plot(fit_eff_prevan_dag)

plot(conditional_effects(fit_eff_prevan_dag), points = TRUE)

pp_check(fit_eff_prevan_dag, type = "dens_overlay", ndraws = 300)


pp_check(fit_eff_prevan_dag, type = "error_scatter_avg")
```
## Model X - log-normal

```{r}

priors_eff_prevan_2 <- c(
  set_prior("normal(2.5, 0.5)", class = "Intercept", lb=0),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality1"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Big5"),
  set_prior("normal(0,0.25)", class = "b", coef = "Familiarity"),
  set_prior("normal(0,0.25)", class = "b", coef = "Expressibility_z")

)

## ad prior for PrevAn_z
## re-do contrast coding (commAtt is probably not the best now)

# The rest we will leave default (and check afterwards)
```

```{r}

fit_eff_prevan_2 <- brm(Effort_Change_Ratio ~ 1 + PrevAn_z + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), 
                data = filtered_data,
                family = lognormal(),
                prior = priors_eff_prevan_2,
                iter = 4000,
                cores = 4)

# we will immediatelly add the diagnostic criteria
fit_eff_prevan_2 <- add_criterion(fit_eff_prevan_2, criterion = c("loo", "waic"))  

# and save it
saveRDS(fit_eff_prevan_2, here("09_Analysis_Modeling", "models", "fit_eff_prevan_2.rds"))

beep(5)

```

```{r}

fit_eff_prevan_2 <- readRDS(here("09_Analysis_Modeling", "models", "fit_eff_prevan_2.rds"))

# summary
summary(fit_eff_prevan_2)

plot(fit_eff_prevan_2)

plot(conditional_effects(fit_eff_prevan_2), points = TRUE)

pp_check(fit_eff_prevan_2, type = "dens_overlay", ndraws = 300)


pp_check(fit_eff_prevan_2, type = "error_scatter_avg")

prior_summary(fit_eff_prevan_2)

```

## Model X - lognormal with interactions



## Model X - fitting b-splines

We will use Richard McElreath's pipeline, or Solomon Kurz's adapted worksflow for brms

See more: https://bookdown.org/content/4857/geocentric-models.html#

```{r}

test <- filtered_data

```

```{r}
test
```

```{r}

test %>% 
  select(where(is.numeric)) %>%  # Select only numeric columns
  pivot_longer(cols = everything(), names_to = "key", values_to = "value") %>%
  group_by(key) %>%
  summarise(
    mean = mean(value, na.rm = TRUE),
    sd   = sd(value, na.rm = TRUE),
    ll   = quantile(value, probs = 0.055, na.rm = TRUE),
    ul   = quantile(value, probs = 0.945, na.rm = TRUE)
  ) %>%
  mutate(across(where(is.double), round, digits = 2))

print(test)

```
```{r}

test %>% 
  ggplot(aes(x = PrevAn, y = Effort_Change_Ratio)) +
  # color from here: https://www.colorhexa.com/ffb7c5
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        # color from here: https://www.colordic.org/w/, inspired by https://chichacha.netlify.com/2018/11/29/plotting-traditional-colours-of-japan/
        panel.background = element_rect(fill = "#4f455c"))
```
```{r}

num_knots <- 15
knot_list <- quantile(test$PrevAn, probs = seq(from = 0, to = 1, length.out = num_knots))

```


```{r}
knot_list
```

```{r}
str(filtered_data)

```
```{r}

test %>% 
  ggplot(aes(x = PrevAn, y = Effort_Change_Ratio)) +
  geom_vline(xintercept = knot_list, color = "white", alpha = 1/2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())

```


```{r}

library(splines)

B <- bs(test$PrevAn,
        knots = knot_list[-c(1, num_knots)], 
        degree = 3, 
        intercept = TRUE)

```



```{r}
B %>% str()
```


```{r}

knot_list[c(1, num_knots)]
```




```{r}

# wrangle a bit
b <-
  B %>% 
  data.frame() %>% 
  set_names(str_c(0, 1:9), 10:17) %>%  
  bind_cols(select(test, PrevAn)) %>% 
  pivot_longer(-PrevAn,
               names_to = "bias_function",
               values_to = "bias")

# plot
b %>% 
  ggplot(aes(x = PrevAn, y = bias, group = bias_function)) +
  geom_vline(xintercept = knot_list, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  ylab("bias value") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())

```


```{r}

test2 <-
  test %>% 
  mutate(B = B) 

# take a look at the structure of `d3
test2 %>% glimpse()

```

```{r}

spline1 <- 
  brm(data = test2,
      family = gaussian,
      Effort_Change_Ratio ~ 1 + B,
      prior = c(prior(normal(100, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 4)

```
Model summary
```{r}

print(spline1)

```
```{r}

post <- as_draws_df(spline1)

glimpse(post)

```
```{r}

post %>% 
  select(b_B1:b_B17) %>% 
  set_names(c(str_c(0, 1:9), 10:17)) %>% 
  pivot_longer(everything(), names_to = "bias_function") %>% 
  group_by(bias_function) %>% 
  summarise(weight = mean(value)) %>% 
  full_join(b, by = "bias_function") %>% 
  
  # plot
  ggplot(aes(x = PrevAn, y = bias * weight, group = bias_function)) +
  geom_vline(xintercept = knot_list, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank()) 

```

```{r}

f <- fitted(spline1)

f %>% 
  data.frame() %>% 
  bind_cols(test2) %>% 
  
  ggplot(aes(x = PrevAn, y = Effort_Change_Ratio, ymin = Q2.5, ymax = Q97.5)) + 
  geom_vline(xintercept = knot_list, color = "white", alpha = 1/2) +
  geom_hline(yintercept = fixef(spline1)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(x = "Previous Answer",
       y = "Effort Change Ratio") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())

```
## Model XX - Bayesian GAMs

We will use Richard McElreath's pipeline, or Solomon Kurz's adapted worksflow for brms

See more: https://bookdown.org/content/4857/geocentric-models.html#

```{r}

gam <-
  brm(data = test2,
      family = gaussian,
      Effort_Change_Ratio ~ 1 + s(PrevAn_z),
      prior = c(prior(normal(100, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(student_t(3, 0, 5.9), class = sds),
                prior(exponential(1), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 4,
      control = list(adapt_delta = .99)
  )

```

```{r}

print(gam)

```


```{r}

fitted(gam) %>% 
  data.frame() %>% 
  bind_cols(select(test2, PrevAn, Effort_Change_Ratio)) %>% 
  
  ggplot(aes(x = PrevAn, y = Effort_Change_Ratio, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = fixef(gam)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(subtitle = "gam using s(PrevAn)",
       y = "Effort Change Ratio") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"), 
        panel.grid = element_blank())

```
 Now GAM with specifying we want b-splines
```{r}


gam_s <-
  brm(data = test2,
      family = gaussian,
      Effort_Change_Ratio ~ 1 + s(PrevAn, bs = "bs", k = 19),
      prior = c(prior(normal(100, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(student_t(3, 0, 5.9), class = sds),
                prior(exponential(1), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 4,
      control = list(adapt_delta = .99)
      )

```
```{r}

print(gam_s)

```

```{r}


plot(conditional_effects(gam_s), points = TRUE)
# but the effects seem to have a good direction

pp_check(gam_s, type = "dens_overlay")
# this looks so much better
pp_check(gam_s, type = "error_scatter_avg")

```

```{r}

fitted(gam_s) %>% 
  data.frame() %>% 
  bind_cols(select(test2, PrevAn, Effort_Change_Ratio)) %>% 
  
  ggplot(aes(x = PrevAn, y = Effort_Change_Ratio, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = fixef(gam_s)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(subtitle = 'gam_s using s(PrevAn, bs = "bs")',
       y = "Effort Chnage Ratio") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"), 
        panel.grid = element_blank())
```


## Model - GAMs with f+r effects
```{r}

gam_s2 <- 
  brm(
    data = test2,
    family = gaussian,
    Effort_Change_Ratio ~ 1 +
      s(PrevAn_z, bs = "bs", k = 19) +  # Smooth for Previous Answer similarity
      CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  # Fixed effects
      (1 | Participant) + (1 | Concept),  # Random effects
    prior = c(
      prior(normal(100, 10), class = Intercept),
      prior(normal(0, 10), class = b),
      prior(student_t(3, 0, 5.9), class = sds),
      prior(exponential(1), class = sigma)
    ),
    iter = 4000, warmup = 2000, chains = 4, cores = 4,
    seed = 4,
    control = list(adapt_delta = .99)
  )


```

```{r}

print(gam_s2)

```

```{r}


plot(conditional_effects(gam_s2), points = TRUE)
# but the effects seem to have a good direction

pp_check(gam_s2, type = "dens_overlay")
# this looks so much better
pp_check(gam_s2, type = "error_scatter_avg")

```

```{r}

fitted(gam_s2) %>% 
  data.frame() %>% 
  bind_cols(select(test2, PrevAn_z, Effort_Change_Ratio)) %>% 
  
  ggplot(aes(x = PrevAn_z, y = Effort_Change_Ratio, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = fixef(gam_s2)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(subtitle = 'gam_s using s(PrevAn, bs = "bs")',
       y = "Effort Chnage Ratio") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"), 
        panel.grid = element_blank())
```

## Model X - GAMs with lognormal distribution

```{r}

priors_gam_s3 <- c(
  set_prior("normal(2.5, 0.5)", class = "Intercept", lb = 0),
  set_prior("normal(0, 0.25)", class = "b", coef = "Modality1"),
  set_prior("normal(0, 0.25)", class = "b", coef = "Modality2"),
  set_prior("normal(0, 0.25)", class = "b", coef = "Big5"),
  set_prior("normal(0, 0.25)", class = "b", coef = "Familiarity"),
  set_prior("normal(0, 0.25)", class = "b", coef = "Expressibility_z")
)

gam_s3 <- 
  brm(
    Effort_Change_Ratio ~ 1 + 
      s(PrevAn_z, bs = "bs", k = 19) +  # Nonlinear effect for Previous Answer similarity
      CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  
      (1 + CommAtt | Participant) +  # Random slopes and intercepts for Participant
      (1 + CommAtt | Concept) +      # Random slopes and intercepts for Concept
      (1 | TrialNumber_c),           # Random intercept for Trial Number
    data = test2,
    family = lognormal(),
    prior = priors_gam_s3,
    iter = 4000,
    warmup = 2000,
    chains = 4,
    cores = 4,
    seed = 4,
    control = list(adapt_delta = .99)
  )

# Add model diagnostic criteria
gam_s3 <- add_criterion(gam_s3, criterion = c("loo", "waic"))

# Save the model
saveRDS(gam_s3, here("09_Analysis_Modeling", "models", "gam_s3.rds"))

beep(5)


```

```{r}

print(gam_s3)

```

```{r}


plot(conditional_effects(gam_s3), points = TRUE)

pp_check(gam_s3, type = "dens_overlay")

pp_check(gam_s3, type = "error_scatter_avg")

```

```{r}

fitted(gam_s3) %>% 
  data.frame() %>% 
  bind_cols(select(test2, PrevAn, Effort_Change_Ratio)) %>% 
  
  ggplot(aes(x = PrevAn, y = Effort_Change_Ratio, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = fixef(gam_s3)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(subtitle = 'gam_s using s(PrevAn, bs = "bs")',
       y = "Effort Chnage Ratio") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"), 
        panel.grid = element_blank())
```
## Model X - GAMs with interaction

todo


## Diagnostics

# QUESTIONS/DISCUSSION

**How/whether to include participant in addition to dyad in the random effects**

Our thoughts: participant could maybe be nested within dyad, or do we not need participant? Must be something suggested in the literature.

Participant probably must be included because the Big 5 personality trait measure corresponds to each participant within the dyad.

Check: https://github.com/FredericBlum/exploring-individual-variation-in-turkish-heritage-speakers-complex-linguistic-productions

Onur and Frede on nesting:

First, we do not use mono- versus bilingualism or even the group variable as a main independent variable in our model. Instead, it is incorporated in a nested random effect. This indicates that we do not view the speaker group status as a main driver of DM production. Second, the group variable that we utilize does not take one of the three speaker groups as a baseline. We sum-coded the variable which incorporates the idea that monolinguals cannot be an adequate baseline for bilinguals. 





