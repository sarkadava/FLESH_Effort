---
title: "Modeling-effort"
subtitle: "Consultation notes with Daniela"
author: "Sarka Kadava"
date: "2024-10-17"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---

# Introduction

In this script, we will be modelling the causal relation between effort and correction to confirm/reject our hypothesis.

These are:

H1: In corrections, people enhance some effort-related kinematic and/or acoustic features of their behaviour relative to the baseline.

H2: The enhancement depends on similarity of the guesser's answer and the original meaning. More similar answer will require/result in smaller enhancement (but still enhancement) than less similar answer.

To assess the design and performance of the model, we will use synthetic data that we create to have certain interdependencies and where the effects have pre-defined values. We use this approach instead of using our dyad 0 data, because the pilot data do not include enough data to have a sensible testing sample. 

Since we cannot predict the outcomes of the XGBoost modelling (in the previous script) and therefore do not know the variables we will model after preregistration, our model only assumes a continuous variable but it is otherwise free of any other assumptions.

# Setting up the environment

```{r warning=FALSE}

library(here)
library(dagitty) # for dag
library(dplyr) # for data-wrangling
library(lme4)  # for linear mixed-effects models
library(tidyr)  # for reshaping data (if needed)
library(ggplot2)

# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())

datasets      <- paste0(parentfolder, '/09_Analysis_Modeling/datasets/')
models        <- paste0(parentfolder, '/09_Analysis_Modeling/models/')
plots         <- paste0(parentfolder, '/09_Analysis_Modeling/plots/')


```

# Loading in our data

```{r}

```

Because current pilot data are quite likely unsifficient in terms of power, and would hinder testing the design of the statistical models, we will create synthetic data with assumed coefficients for all predictors we plan to add to the model.

# Creating synthetic data
```{r}

# Set seed for reproducibility
set.seed(0209)

# Set coefficients
b_exp_vocal <- 0.6  # Vocal has lower expressibility
b_exp_multi <- 1.5  # Multimodal has higher expressibility
b_bif <- 1.15  # More extroverted → more effort
b_fam <- 1.10  # More familiarity → more effort
b_exp <- 1.20  # More expressibility → more effort
b_multi <- 0.70  # Multimodal → slightly reduced effort
b_comatt2 <- 1.50  # Effort increases for second attempt
b_comatt3 <- 0.50  # Effort decreases for third attempt
b_prevan <- 0.50  # Higher previous answer similarity → less effort

# Define participants and concepts
n_participants <- 120
n_total_concepts <- 21  # Each participant works with all 21 concepts
n_modalities <- 3  # Gesture, vocal, combined

# Generate participant-level data
participants <- 1:n_participants
Big5 <- runif(n_participants, min = 0, max = 2)
Familiarity <- runif(n_participants, min = 0, max = 2)

# Generate expressibility scores for each concept across modalities
expressibility_matrix <- matrix(runif(n_total_concepts * n_modalities, min = 0, max = 1),
                                nrow = n_total_concepts, ncol = n_modalities)

# Initialize data storage
final_data_synt <- data.frame()

# Define function to simulate participant data
simulate_participant <- function(participant_id) {
  participant_data <- data.frame()
  trial_number <- 1
  
  for (concept_id in 1:n_total_concepts) {
    # Assign a random modality
    modality <- sample(c("gesture", "vocal", "combined"), 1)
    
    # Get expressibility score based on modality
    expressibility_score <- ifelse(modality == "vocal", expressibility_matrix[concept_id, 1] * b_exp_vocal, 
                                   ifelse(modality == "gesture", expressibility_matrix[concept_id, 2], 
                                          expressibility_matrix[concept_id, 3] * b_exp_multi))
    
    # Base effort before adjustments
    base_effort <- b_bif * Big5[participant_id] + 
                   b_fam * Familiarity[participant_id] + 
                   b_exp * expressibility_score + 
                   rnorm(1, mean = 1, sd = 0.5)
    
    # Adjust effort for multimodal condition
    if (modality == "combined") {
      base_effort <- base_effort * b_multi
    }
    
    # Sample the number of communicative attempts (CommAtt)
    adjusted_prob <- c(1 - Familiarity[participant_id],
                       1 - Familiarity[participant_id],
                       1 - Familiarity[participant_id]) * 
                     c(1 - Big5[participant_id],
                       1 - Big5[participant_id],
                       1 - Big5[participant_id]) * 
                     c(1 - expressibility_score,
                       1 - expressibility_score,
                       1 - expressibility_score)
    
    adjusted_prob <- adjusted_prob / sum(adjusted_prob)
    n_attempts <- sample(1:3, 1, prob = adjusted_prob)
    
    prev_answer_similarity <- NA  # First attempt has no previous similarity
    
    # Generate rows for each communicative attempt
    for (attempt in 1:n_attempts) {
      Eff <- base_effort  # Start with base effort
      
      # Modify effort for second and third attempts
      if (attempt == 2) {
        Eff <- Eff * b_comatt2
      } else if (attempt == 3) {
        Eff <- Eff * b_comatt3
      }
      
      # Adjust effort based on previous answer similarity
      if (!is.na(prev_answer_similarity)) {
        Eff <- Eff * (1 + (1 - prev_answer_similarity) * b_prevan)
      }
      
      # Store row
      participant_data <- rbind(participant_data, data.frame(
        Participant = participant_id,
        Concept = concept_id,
        Modality = modality,
        Big5 = Big5[participant_id],
        Familiarity = Familiarity[participant_id],
        Expressibility = expressibility_score,
        CommAtt = attempt,
        Eff = Eff,
        TrialNumber = trial_number,
        PrevAn = prev_answer_similarity
      ))
      
      # Update for next attempt
      trial_number <- trial_number + 1
      prev_answer_similarity <- runif(1, min = 0, max = 1)  # Simulate next similarity
    }
  }
  
  return(participant_data)
}

# Simulate data for all participants
final_data_synt <- do.call(rbind, lapply(participants, simulate_participant))

# Preview results
head(final_data_synt)


```

So now we have synthetic data where we exactly defined what is the relationship between certain variables

1) CommAtt -> Eff

The effort for second attempt is multiplied by 1.50 (Beta = 1.50)
The effort for third attempts by 0.7 (ie decreases, Beta = 0.5)

2) Fam -> Eff & CommAtt

Beta = 1.10 for Eff
Negative effect on CommAtt


3) Big5 -> Eff & CommAtt

Beta = 1.15 for Eff
Negative effect on CommAtt

4) Expr -> Eff & CommAtt

Beta = 1.20 for Eff
Negative effect on CommAtt

(For simplicity reasons, we do not create an affect of trial number, as it does not represent a crucial variable)

Note that the synthetic data do not copy the structure of the data and the experimental design perfectly, but it does provide a reliable ground to build the models and test their performance.


# Exploring structure 

```{r}
nrow(final_data_synt) # this is the number of datapoints
```

```{r}
hist(final_data_synt$Eff)
```

```{r}
final_data_synt |> 
  janitor::tabyl(Participant, Concept) # the number of repetition per concept by participant
```
## Plot: H1

```{r}

# Create a boxplot comparing Effort across different Communicative Attempts
ggplot(final_data_synt, aes(x = as.factor(CommAtt), y = Eff)) +
  geom_boxplot(aes(fill = as.factor(CommAtt))) +  
  labs(title = "Comparison of Effort Across Communicative Attempts",
       x = "Communicative Attempts",
       y = "Effort",
       fill = "CommAtt") + 
  theme_minimal() +
  theme(legend.position = "none")  

```
The visual already hints on the effects we expect to exist in the data.

## Plot: H2

```{r}

# Filter out CommAtt == 1
filtered_data <- final_data_synt[final_data_synt$CommAtt != 1, ]

# Scatter plot with regression line
library(ggplot2)

ggplot(filtered_data, aes(x = PrevAn, y = Eff)) +
  geom_point(alpha = 0.6, color = "blue") +  # Scatter points
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Regression line
  labs(x = "Previous Answer Similarity (PrevAn)", 
       y = "Effort (Eff)", 
       title = "Relationship between Effort and Previous Answer Similarity") +
  theme_minimal()


```

Save it
```{r}

write.csv(final_data_synt, file = here("09_Analysis_Modeling", "datasets", "synthetic_data.csv"), row.names = FALSE)

```


# H1: Stating causal model

Before doing that, we create a causal diagram, so-called directed acyclic graphs (DAG). This will help us clarify the causal model and introduce the predictors we aim to model.

Our two hypothesis go as follows:

*H1: Correction recruits more physical effort than the baseline performance.*

*H2: A higher degree of misunderstanding will require a performer to engage in more effortful correction.*


The relationship of interest is the causal effect of communicative attempt on effort. Our assumptions include:

* Personality traits (measured with Big5) will influence effort (e.g., people are more willing to put more effort if they are open-minded) and also communicative attempt (e.g., more extroverted people are better in this game, therefore they need less attempts)

* Familiarity with guessing partner influences effort (ditto) as well as communicative attempt (e.g., friends are better in this game than strangers)

* Similarly, participant will also directly influence effort and commAtt, because personality traits might not be capturing all variation

* Expressibility of concepts is going to influence effort (e.g., more expressible concepts allow more enhancement - but could be also other direction) as well as CommAtt (e.g., more expressible concepts are more readily guessed and don't need more attempts)

* Similarly, concept will also directly influence effort and commAtt, because expressibility might not be capturing all variation

* Modality (uni vs. multi) will influence the value of effort. We assume that in unimodal condition, the feature does not need to account for synergic relations with the other modality, and may carry the whole signal. In multimodal condition, however, there may be synergic relations that moderate the full expressiveness of this feature

* Trial number (characterising how far one is in the experiment) could be hinting on learning processes through out the experiment, or - the other direction - on increasing fatigue

* Previous answer (PrevAn) will affect the effort (more similar answer will require less effortful correction) as well as communicative attempt (correct answers do not require further corrections)

* Expressibility of concepts will influence the answer (more expressible are easier to guess)

* Trial number will also affect previous answer if there is learning involved, or conversely, increasing fatigue


```{r}

daggy_h1 <- dagitty('dag {
Big5 [adjusted,pos="-0.823,0.657"]
CommAtt [exposure,pos="-1.033,0.028"]
Conc [adjusted,pos="-1.136,-0.848"]
Eff [outcome,pos="-0.102,0.025"]
Expr [adjusted,pos="-0.758,-0.850"]
Fam [adjusted,pos="-0.379,0.663"]
Pcn [adjusted,pos="-0.589,1.214"]
TrNum [adjusted,pos="-1.686,-0.859"]
Big5 -> CommAtt     
Big5 -> Eff
CommAtt -> Eff
Conc -> Expr
Expr -> CommAtt
Expr -> Eff
Fam -> CommAtt
Fam -> Eff
Mod -> Eff
Mod -> CommAtt
Pcn -> Big5
Pcn -> CommAtt 
Pcn -> Eff
Pcn -> Fam
TrNum -> CommAtt
TrNum -> Eff
Conc -> CommAtt
Conc -> Eff
}')

plot(daggy_h1)

```

```{r}

impliedConditionalIndependencies(daggy_h1)

```
This is the adjustment set that needs to be included in the model to make sure we are not confounding
```{r}

dagitty::adjustmentSets(daggy_h1, exposure = "CommAtt", outcome = "Eff")

```


# H1: Modelling

*H1: Correction recruits more physical effort than the baseline performance.*

```{r}

library(ggplot2)
library(patchwork)
library(bayesplot)
library(brms)
library(beepr)
library(bayestestR)
library(tidyverse)

# use all available cores for parallel computing
options(mc.cores = parallel::detectCores())
```


## Contrast coding

Load in the data
```{r}

final_data <- final_data_synt
```


Convert columns to factors
```{r}

final_data$CommAtt <- as.factor(final_data$CommAtt)
final_data$Modality <- as.factor(final_data$Modality)
final_data$Participant <- as.factor(final_data$Participant)
final_data$Concept <- as.factor(final_data$Concept)

final_data$TrialNumber <- as.numeric(final_data$TrialNumber)  # Ensure TrialNumber is numeric
```

Check contrasts of factors
```{r}
contrasts(final_data$CommAtt) <- MASS::contr.sdif(3)
contrasts(final_data$Modality) <- contr.sum(3)/2
```

This is how CommAtt is contrast-coded now

         2-1        3-2
1 -0.6666667 -0.3333333
2  0.3333333 -0.3333333
3  0.3333333  0.6666667

This is how modality is cc-ed

         [,1] [,2]
combined  0.5  0.0
gesture   0.0  0.5
vocal    -0.5 -0.5

Center trial number
```{r}
final_data$TrialNumber_c <- final_data$TrialNumber - median(range(final_data$TrialNumber))
range(final_data$TrialNumber_c)
range(final_data$TrialNumber)
```
For now, we will just center Familiarity and Big5 because we created them synthetically. But the real data have these two variables already z-scored
```{r}

final_data$Familiarity <- final_data$Familiarity - median(range(final_data$Familiarity))
final_data$Big5 <- final_data$Big5 - median(range(final_data$Big5))

```


Z-score expressibility (because it's continuous) within a modality
```{r}

final_data <-
  final_data |>
  group_by(Modality) |>
  mutate(Expressibility_z = (Expressibility - mean(Expressibility))/ sd(final_data$Expressibility, na.rm = T)) |>
  ungroup()
```

## Model 1 - Simple reproduction of DAG

First we want to do a simple model that, however, reproduce our DAG. 

Our main predictor is communicative attempt (CommAtt). To account for confounders - variables affecting both the predictor and the dependent variable - we need to adjust for them in the model by including them as covariates to isolate the effect of the predictor. Based on our DAG, confounders include:

1. familiarity
2. big5
3. expressibility
4. trial number
5. modality
6. concept
7. participant

We include 1.-5. as fixed factors. For 6.-7., we include varying intercepts as we expect that each participant and concept may have they own baseline level of effort and thus allow for individual variation. Partial pooling is also beneficial in that extreme values (or categories will fewer datapoints) will be shrunk toward the overal average.

We will now not include PrevAn (previous answer) variable because we will need to do some further data-wrangling when building model for H2. That is mainly because PrevAn has some NA values, concretely for first attempts. Models would automatically exclude NA data, and we would therefore loose all effort data for attempt 1. For H1, however, we want to keep it there.

The model looks as follows:

$$
Eff_{ijk} = \beta_0 + \beta_1 CommAtt_{ijk} + \beta_2 Familiarity_{ijk} + \beta_3 Big5_{ijk} + \beta_4 Expressibility\_z_{ijk} + \beta_5 TrialNumber\_c_{ijk} + \beta_6 Modality_{ijk} + u_i + v_j + \epsilon_{ijk}
$$
where: 

- \( Eff_{ijk} \) is the effort for observation \( k \) of participant \( i \) and concept \( j \).  
- \( \beta_0 \) is the intercept.  
- \( \beta_1, \beta_2, \beta_3, \beta_4, \beta_5, \beta_6 \) are the fixed effect coefficients for each predictor.  
- \( u_i \sim \mathcal{N}(0, \sigma_{Participant}^2) \) is the random intercept for participant \( i \).  
- \( v_j \sim \mathcal{N}(0, \sigma_{Concept}^2) \) is the random intercept for concept \( j \).  
- \( \epsilon_{ijk} \sim \mathcal{N}(0, \sigma^2) \) is the residual error term.  


This is the model without setting any priors, leaving them to default values
```{r eval=FALSE}

h1.m1 <- brm(Eff ~ 1 + CommAtt + Familiarity + Big5 + Expressibility_z + TrialNumber_c + Modality + (1 | Participant) + (1 | Concept),
                data = final_data,
                iter = 4000,
                cores = 4)

# Add criterions for later diagnostics
h1.m1 <- add_criterion(h1.m1, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h1.m1_R2 <- bayes_R2(h1.m1)

# Save both as objects
saveRDS(h1.m1, here("09_Analysis_Modeling", "models", "h1.m1.rds"))
saveRDS(h1.m1_R2, here("09_Analysis_Modeling", "models", "h1.m1_R2.rds"))

beep(5)

```


```{r}
h1.m1 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m1.rds"))
h1.m1_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m1_R2.rds"))


# Summary
summary(h1.m1)

# Regression Coefficients:
#                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
# Intercept            4.02      0.02     3.98     4.06 1.00     8044     5943
# CommAtt2M1           3.08      0.03     3.03     3.14 1.00    12657     6394
# CommAtt3M2          -4.39      0.04    -4.47    -4.32 1.00    12764     6298
# Familiarity          1.26      0.02     1.21     1.30 1.00    11423     5561
# Big5                 1.29      0.02     1.25     1.34 1.00    12279     5608
# Expressibility_z     0.46      0.02     0.43     0.49 1.00    13924     5962
# TrialNumber_c       -0.00      0.00    -0.00     0.00 1.00     9813     6937
# Modality1           -1.30      0.04    -1.38    -1.23 1.00    11060     6036
# Modality2            0.64      0.04     0.57     0.71 1.00    12543     6349

# Note on modality: Modality1 represents the baseline difference between combined and the other modalities, and Modality2 represents the difference between gesture and vocal compared to combined.

```

### Transforming coefficients

To be able to link these estimates back to the simulation coefficients, let's create a function

```{r}

transform_attempt <- function(intercept, CommAtt2M1, CommAtt3M2) {
  # Effort for the first attempt (base effort)
  Eff_attempt_1 <- intercept
  
  # Effort for the second attempt
  Eff_attempt_2 <- Eff_attempt_1 + CommAtt2M1
  
  # Effort for the third attempt
  Eff_attempt_3 <- Eff_attempt_1 + CommAtt2M1 + CommAtt3M2
  
  # Calculate ratios
  b_attempt2 <- Eff_attempt_2 / Eff_attempt_1
  b_attempt3 <- Eff_attempt_3 / Eff_attempt_2
  
  return(data.frame(b_attempt2, b_attempt3))
}


h1m1_coeff <- transform_attempt(4.02, 3.08, -4.39)
print(h1m1_coeff)

# For centered familiarity
fam = (4.02 + 1.26) / 4.02
print(fam)

# For centered BIF
bif = (4.02 + 1.29) / 4.02
print(bif)

# Expressibility is z-scored so we will not get to the coefficient in the same way but we can check the conditinal effects for checking whether it looks good

```
Let's also check the visuals

```{r}

plot(h1.m1)
# all caterpillars look nice

plot(conditional_effects(h1.m1), points = TRUE)
# the effects all go in good direction

pp_check(h1.m1, type = "dens_overlay")
# Looks good but not amazing - mostly because the posteriors seem to not know effort cannot be negative

pp_check(h1.m1, type = "error_scatter_avg")
# half blobby, half correlated, so still some room for improvement
# positive correlation means that errors increase with predicted values. So the model does perform well for some range, but becomes less reliable with increase in the predicted values
# also the blob is centered around 0 which is good

# it could be we are ignoring some interaction terms or non-linearity (which we know we kind of do). Transformation could also help (e.g., log). Of course, we are also still not specifying any priors so let's not yet make it a disaster

h1.m1_R2
# explained variance around 83%

```
Overall, we see good directions of all predictors, mostly also in accordance with the expected coefficients. Of course, the synthetic data is quite complex so there might be other dependencies that moderate the causal relationships and that is why we do not see exactly the numbers we use to create the data.

Let's have another model for comparison.

The fixed predictors make sense like this, as it will allow us to assess the effect of each on the effort, despite it not being our main research question. But we can assume that participants and concept have not only different baselines of effort (varying intercept). The effect of CommAtt on effort might vary across them too, hence we can try to add varying slopes for them and see whether the diagnostics improves. We will also add TrialNumber as a varying intercept, because we expect variation between earlier and later performances (because of learning, or opposite, fatigue) and we do not really need a single coefficient for this predictor anyway.

## Model 2 - Varying slopes and intercepts

```{r}

h1.m2 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), 
                data = final_data,
                iter = 4000,
                cores = 4)

# Add criterions for later diagnostics
h1.m2 <- add_criterion(h1.m2, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h1.m2_R2 <- bayes_R2(h1.m2)

# Save both as objects
saveRDS(h1.m2, here("09_Analysis_Modeling", "models", "h1.m2.rds"))
saveRDS(h1.m2_R2, here("09_Analysis_Modeling", "models", "h1.m2_R2.rds"))

beep(5)


```

```{r}

h1.m2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m2.rds"))
h1.m2_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m2_R2.rds"))


# Summary
summary(h1.m2)
# Coefficients remain mostly unchanged but there were some divergent transitions

```

```{r}

plot(h1.m2)
# some some of the caterpillars are not that pretty anymore

plot(conditional_effects(h1.m2), points = TRUE)
# the effects all go in good direction

pp_check(h1.m2, type = "dens_overlay")
# Looks good but not amazing - mostly because the posteriors seem to not know effort cannot be negative

pp_check(h1.m2, type = "error_scatter_avg")
# This looks a bit more blobby than before but still lots of errors for higher values

h1.m2_R2
# explained variance around 87%

```
One of the reasons why we might be getting the divergent transition is because of the correlation between slopes and intercepts in the previous model. Let's now get rid of it


## Model 3 - No correlation coefficient

```{r eval=FALSE}

h1.m3 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                iter = 4000,
                cores = 4)
                
# Add criterions for later diagnostics
h1.m3 <- add_criterion(h1.m3, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h1.m3_R2 <- bayes_R2(h1.m3)

# Save both as objects
saveRDS(h1.m3, here("09_Analysis_Modeling", "models", "h1.m3.rds"))
saveRDS(h1.m3_R2, here("09_Analysis_Modeling", "models", "h1.m3_R2.rds"))

beep(5)

```

```{r}


h1.m3 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m3.rds"))
h1.m3_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m3_R2.rds"))


# Summary
summary(h1.m3)
# Coefficients remain mostly unchanged 


```

```{r}

plot(h1.m3)
# no correlation fixed the performance

plot(conditional_effects(h1.m3), points = TRUE)
# the effects all go in good direction

pp_check(h1.m3, type = "dens_overlay")
# the problem with predicting negative values remains

pp_check(h1.m3, type = "error_scatter_avg")
# unchanged

h1.m3_R2
# explained variance remains around 87%

```
## Model 3.1 - adding priors

```{r}

priors_h1m3p <- c(
  set_prior("normal(2.5, 0.5)", class = "Intercept", lb=0),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt2M1"),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt3M2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality1"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Big5"),
  set_prior("normal(0,0.25)", class = "b", coef = "Familiarity"),
  set_prior("normal(0,0.25)", class = "b", coef = "Expressibility_z"),
  
  set_prior("normal(0.5,0.1)", class = "sd", group = "TrialNumber_c"),
  set_prior("normal(0.5,0.1)", class = "sd", group = "Participant"),
  set_prior("normal(0.5,0.1)", class = "sd", group = "Concept"),
  set_prior("normal(1,0.1)", class = "sd"),
  
  set_prior("normal(0.5,0.25)", class = "sigma")
  
)

h1.m3p <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                prior=priors_h1m3p,
                family = gaussian,
                iter = 4000,
                cores = 4)


# Add criterions for later diagnostics
h1.m3p <- add_criterion(h1.m3p, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h1.m3p_R2 <- bayes_R2(h1.m3p)

# Save both as objects
saveRDS(h1.m3p, here("09_Analysis_Modeling", "models", "h1.m3p.rds"))
saveRDS(h1.m3p_R2, here("09_Analysis_Modeling", "models", "h1.m3p_R2.rds"))

beep(5)

```
```{r}


h1.m3p <- readRDS(here("09_Analysis_Modeling", "models", "h1.m3p.rds"))
h1.m3p_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m3p_R2.rds"))


# Summary
summary(h1.m3p)
# Coefficients remain mostly unchanged 


```

```{r}

plot(h1.m3p)
# all good

plot(conditional_effects(h1.m3p), points = TRUE)
# the effects all go in good direction

pp_check(h1.m3p, type = "dens_overlay")
# the problem with predicting negative values remains

pp_check(h1.m3p, type = "error_scatter_avg")
# unchanged

h1.m3p_R2
# explained variance remains around 87%

```

## Model 4 - Restricting priors with exponential distribution

Let's do one more test with the priors, restricting sd them with exponential distribution, i.e., negative values are not possible

```{r}

priors_h1m4 <- c(
  set_prior("normal(3, 0.3)", class = "Intercept", lb = 0),
  set_prior("normal(0, 0.25)", class = "b", coef = "CommAtt2M1"),
  set_prior("normal(0, 0.25)", class = "b", coef = "CommAtt3M2"),
  set_prior("normal(0, 0.15)", class = "b", coef = "Modality1"),
  set_prior("normal(0, 0.15)", class = "b", coef = "Modality2"),
  set_prior("normal(0, 0.15)", class = "b", coef = "Big5"),
  set_prior("normal(0, 0.15)", class = "b", coef = "Familiarity"),
  set_prior("normal(0, 0.15)", class = "b", coef = "Expressibility_z"),

  # Exponential priors for standard deviations
  set_prior("exponential(3)", class = "sd", group = "TrialNumber_c"), # exp(3) has a mean of 1/3 and concentrates most density around small values
  set_prior("exponential(3)", class = "sd", group = "Participant"),
  set_prior("exponential(3)", class = "sd", group = "Concept"),
  set_prior("exponential(1)", class = "sd"),  # Generic sd prior

  # Residual standard deviation - keep it narrow
  set_prior("normal(0.5, 0.1)", class = "sigma")
)

h1.m4 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                prior=priors_h1m4,
                family = gaussian,
                iter = 4000,
                cores = 4)


# Add criterions for later diagnostics
h1.m4 <- add_criterion(h1.m4, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h1.m4_R2 <- bayes_R2(h1.m4)

# Save both as objects
saveRDS(h1.m4, here("09_Analysis_Modeling", "models", "h1.m4.rds"))
saveRDS(h1.m4_R2, here("09_Analysis_Modeling", "models", "h1.m4_R2.rds"))

beep(5)

```

```{r}


h1.m4 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m4.rds"))
h1.m4_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m4_R2.rds"))


# Summary
summary(h1.m4)
# the coefficients for commatt now seem much lower


```
```{r}


h1m1_coeff <- transform_attempt(4.02, 0.88, -0.36)
print(h1m1_coeff)

# For centered familiarity
fam = (4.02 + 1.17) / 4.02
print(fam)

# For centered BIF
bif = (4.02 + 1.21) / 4.02
print(bif)

```
So it seems that we are misleading with the priors and this will not be the way


```{r}

plot(h1.m4)
# all good

plot(conditional_effects(h1.m4), points = TRUE)
# the effect of main predictor is now very moderated 

pp_check(h1.m4, type = "dens_overlay")
# the problem with predicting negative values remains

pp_check(h1.m4, type = "error_scatter_avg")
# unchanged

h1.m4_R2
# explained variance remains around 87%

```

## Model 5 - student family

So we still see negative values in the posterior simulations, so let's try Student's t-distribution which is more robut to outliers and can potentially reduce the likelihood of negative values (if we reduce degrees of freedom)


```{r}

priors_h1m5 <- c(
  set_prior("normal(2.5, 0.5)", class = "Intercept", lb=0),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt2M1"),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt3M2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality1"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Big5"),
  set_prior("normal(0,0.25)", class = "b", coef = "Familiarity"),
  set_prior("normal(0,0.25)", class = "b", coef = "Expressibility_z"),
  
  set_prior("normal(0.5,0.05)", class = "sd", group = "TrialNumber_c"),
  set_prior("normal(0.5,0.05)", class = "sd", group = "Participant"),
  set_prior("normal(0.5,0.05)", class = "sd", group = "Concept"),
  set_prior("normal(1,0.05)", class = "sd"),
  
  set_prior("normal(0.5,0.1)", class = "sigma"),
  set_prior("gamma(2, 0.1)", class = "nu")  # Prior for degrees of freedom
  
)

h1.m5 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                prior=priors_h1m5,
                family = student,
                iter = 4000,
                cores = 4)


# Add criterions for later diagnostics
h1.m5 <- add_criterion(h1.m5, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h1.m5_R2 <- bayes_R2(h1.m5)

# Save both as objects
saveRDS(h1.m5, here("09_Analysis_Modeling", "models", "h1.m5.rds"))
saveRDS(h1.m5_R2, here("09_Analysis_Modeling", "models", "h1.m5_R2.rds"))

beep(5)


```

```{r}

h1.m5 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m5.rds"))
h1.m5_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m5_R2.rds"))


# Summary
summary(h1.m5)
# the coefficients look ok again. Familiarity and Big5 even got closer to our simulated betas


```


```{r}

plot(h1.m5)
# all good

plot(conditional_effects(h1.m5), points = TRUE)
# the effects all go in good direction

pp_check(h1.m5, type = "dens_overlay")
# the fit seems somewhat better than with gaussian, but still negative values are there

pp_check(h1.m5, type = "error_scatter_avg")
# unchanged

h1.m5_R2
# explained variance remains around 85%

```

## Model 6 - log-normal distribution

We have already seen - when plotting - that effort probably tends towards lognormal distribution. We can also use XXX test to confirm whether effort is normally distributed.

```{r}

## Daniela will hopefully remind me

```

We will keep the model constant, just exchange the distribution. Priors are kept default
```{r}

h1.m6 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt || Participant) + (1 + CommAtt || Concept) + (1 || TrialNumber_c), 
                data = final_data,
                family = lognormal(),
                iter = 4000,
                cores = 4)


# Add criterions for later diagnostics
h1.m6 <- add_criterion(h1.m6, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h1.m6_R2 <- bayes_R2(h1.m6)

# Save both as objects
saveRDS(h1.m6, here("09_Analysis_Modeling", "models", "h1.m6.rds"))
saveRDS(h1.m6_R2, here("09_Analysis_Modeling", "models", "h1.m6_R2.rds"))

beep(5)


```

```{r}

h1.m6 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m6.rds"))
h1.m6_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m6_R2.rds"))


# Summary
summary(h1.m6)
# Coefficients remain mostly unchanged 


```

### Converting to original scale

Of course now we have all on lognormal scale so it is not so straightforwadly interpretable. 
This is how we can get to the values of an effort under certain value of a predictor
```{r}

library(emmeans)

# Compute estimated marginal means on log-scale
em_h1.m6 <- emmeans(h1.m6, ~ CommAtt) #~ CommAtt

#Backtransform the post.beta values
em_h1.m6@post.beta <- exp(em_h1.m6@post.beta)
print(em_h1.m6)

```
So we indeed see that effort in CommAtt2 increase but then decreases again for CommAtt3. We can also try to get our simulated betas
```{r}

coeff1 <- 3.33*1.5 
coeff2 <- 3.33*0.5

print(coeff1)
print(coeff2) # which is very close to the mean values we see from the model
```
We see that the effect estimated by the model is now stronger than in our simulated data - also maybe because the values are averaged over predictor modality


Now let's try a different method to get the coefficients (code adapted from https://bruno.nicenboim.me/bayescogsci/ch-reg.html#sec-trial)

```{r}

# Extract posterior samples
alpha_samples <- as_draws_df(h1.m6)$b_Intercept
beta_2_vs_1 <- as_draws_df(h1.m6)$b_CommAtt2M1
beta_3_vs_2 <- as_draws_df(h1.m6)$b_CommAtt3M2

# Compute expected values on the log scale
mu_1 <- alpha_samples  # CommAtt 1
mu_2 <- alpha_samples + beta_2_vs_1  # CommAtt 2
mu_3 <- alpha_samples + beta_2_vs_1 + beta_3_vs_2  # CommAtt 3

# Transform to original scale
effect_1 <- exp(mu_1)
effect_2 <- exp(mu_2)
effect_3 <- exp(mu_3)

# Calculate contrasts on the original scale
effect_diff_2_vs_1 <- effect_2 - effect_1
effect_diff_3_vs_2 <- effect_3 - effect_2
effect_diff_3_vs_1 <- effect_3 - effect_1

# Summarize the effects
list(
  mean_intercept = mean(effect_1),
  mean_diff_2_vs_1 = c(mean = mean(effect_diff_2_vs_1), quantile(effect_diff_2_vs_1, c(0.025, 0.975))),
  mean_diff_3_vs_2 = c(mean = mean(effect_diff_3_vs_2), quantile(effect_diff_3_vs_2, c(0.025, 0.975))),
  mean_diff_3_vs_1 = c(mean = mean(effect_diff_3_vs_1), quantile(effect_diff_3_vs_1, c(0.025, 0.975)))
)

```
Now we can use these coefficients to transform to our simulated betas
```{r}

h1m6_coeff <- transform_attempt(3.419549, 2.950688, -4.240333)
print(h1m6_coeff)

```
Now this looks closer again to our betas.

This is for all predictors (except concept and participant)
```{r}

# Extract posterior samples
posterior_samples <- as_draws_df(h1.m6)
alpha_samples <- posterior_samples$b_Intercept

# Create a list to store effects for each fixed factor
effect_list <- list()

# Helper function to calculate summary statistics
get_effect_summary <- function(effect_samples) {
  mean_effect <- mean(effect_samples)
  se_effect <- sd(effect_samples)
  ci_effect <- quantile(effect_samples, c(0.025, 0.975))
  post_prob <- mean(effect_samples > 0)
  c(mean = mean_effect, 
    se = se_effect, 
    lower_ci = ci_effect[1], 
    upper_ci = ci_effect[2], 
    post_prob = post_prob)
}

# COMMATT (successive differences coding)
if ("b_CommAtt2M1" %in% colnames(posterior_samples) & "b_CommAtt3M2" %in% colnames(posterior_samples)) {
  beta_2_vs_1 <- posterior_samples$b_CommAtt2M1
  beta_3_vs_2 <- posterior_samples$b_CommAtt3M2
  
  mu_1 <- alpha_samples
  mu_2 <- alpha_samples + beta_2_vs_1
  mu_3 <- alpha_samples + beta_2_vs_1 + beta_3_vs_2
  
  effect_list$CommAtt <- rbind(
    "commat 2 vs 1" = get_effect_summary(exp(mu_2) - exp(mu_1)),
    "commat 3 vs 2" = get_effect_summary(exp(mu_3) - exp(mu_2)),
    "commat 3 vs 1" = get_effect_summary(exp(mu_3) - exp(mu_1))
  )
}

# MODALITY (sum contrasts scaled by 0.5)
if ("b_Modality1" %in% colnames(posterior_samples) & "b_Modality2" %in% colnames(posterior_samples)) {
  beta_mod_1 <- posterior_samples$b_Modality1
  beta_mod_2 <- posterior_samples$b_Modality2
  
  mu_mod_1 <- alpha_samples + beta_mod_1
  mu_mod_2 <- alpha_samples + beta_mod_2
  mu_mod_3 <- alpha_samples - beta_mod_1 - beta_mod_2
  
  effect_list$Modality <- rbind(
    "mod 1 vs 2" = get_effect_summary(exp(mu_mod_1) - exp(mu_mod_2)),
    "mod 1 vs 3" = get_effect_summary(exp(mu_mod_1) - exp(mu_mod_3)),
    "mod 2 vs 3" = get_effect_summary(exp(mu_mod_2) - exp(mu_mod_3))
  )
}

# BIG5 (continuous)
if ("b_Big5" %in% colnames(posterior_samples)) {
  beta_big5 <- posterior_samples$b_Big5
  effect_list$Big5 <- get_effect_summary(exp(alpha_samples + beta_big5) - exp(alpha_samples))
}

# FAMILIARITY (continuous)
if ("b_Familiarity" %in% colnames(posterior_samples)) {
  beta_fam <- posterior_samples$b_Familiarity
  effect_list$Familiarity <- get_effect_summary(exp(alpha_samples + beta_fam) - exp(alpha_samples))
}

# EXPRESSIBILITY_Z (continuous)
if ("b_Expressibility_z" %in% colnames(posterior_samples)) {
  beta_expr <- posterior_samples$b_Expressibility_z
  effect_list$Expressibility_z <- get_effect_summary(exp(alpha_samples + beta_expr) - exp(alpha_samples))
}

# TRIAL NUMBER (centered continuous)
if ("b_TrialNumber_c" %in% colnames(posterior_samples)) {
  beta_trial <- posterior_samples$b_TrialNumber_c
  effect_list$TrialNumber_c <- get_effect_summary(exp(alpha_samples + beta_trial) - exp(alpha_samples))
}

# Convert to a nicely formatted data frame
effect_df <- do.call(rbind, effect_list)

# View effects
effect_df

```
So here we also see the negative effect of combined modality (mod1)


```{r}

plot(h1.m6)
# all good

plot(conditional_effects(h1.m6), points = TRUE)
# the effects all go in good direction

pp_check(h1.m6, type = "dens_overlay")
# we got rid of negative predictions, and it looks very good

pp_check(h1.m6, type = "error_scatter_avg")
# very nice blob

h1.m6_R2
# explained variance increases to 88%

```

## Model 6.1 - with correlation

Since we now significantly improved the model performance, let's try once again the correlation between slope and intercept
```{r}


h1.m6c <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), 
                data = final_data,
                family = lognormal(),
                iter = 4000,
                cores = 4)


# Add criterions for later diagnostics
h1.m6c <- add_criterion(h1.m6c, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h1.m6c_R2 <- bayes_R2(h1.m6c)

# Save both as objects
saveRDS(h1.m6c, here("09_Analysis_Modeling", "models", "h1.m6c.rds"))
saveRDS(h1.m6c_R2, here("09_Analysis_Modeling", "models", "h1.m6c_R2.rds"))

beep(5)


```

```{r}

h1.m6c <- readRDS(here("09_Analysis_Modeling", "models", "h1.m6c.rds"))
h1.m6c_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m6c_R2.rds"))


# Summary
summary(h1.m6c)
# Coefficients remain mostly unchanged 


```


```{r}

plot(h1.m6c)
# now correlation does not seem to generate problems

plot(conditional_effects(h1.m6c), points = TRUE)
# the effects all go in good direction

pp_check(h1.m6c, type = "dens_overlay")
# ppcheck good

pp_check(h1.m6c, type = "error_scatter_avg")
# nice blob

h1.m6c_R2
# explained variance remains around 88%

```

## Diagnostics I

Let's now check which model has the best diagnostics

```{r}

model_list <- list(h1.m1, h1.m2, h1.m3, h1.m3p, h1.m4, h1.m5, h1.m6, h1.m6c)

r2_list <- list(h1.m1_R2, h1.m2_R2, h1.m3_R2, h1.m3p_R2, h1.m4_R2, h1.m5_R2, h1.m6_R2, h1.m6c_R2)

```

### Rhat

```{r}

# Extract R-hat values for each model
rhat_list <- lapply(model_list, function(model) {
  rhat_values <- rhat(model)
  data.frame(model = deparse(substitute(model)), 
             max_rhat = max(rhat_values), 
             min_rhat = min(rhat_values))
})

# Combine and inspect
do.call(rbind, rhat_list)

```
All models seems actually ok in terms of Rhat values except model 2 (h1.m2)

### ESS

Effective sample size tells how many independent samples the model has effectively drawn from the PD. Low ESS suggests autocorrelation (i.e., sample explores one part of posterior), while high ESS means good mix

```{r}

# Extract n_eff values for each model
neff_ratio_list <- lapply(model_list, function(model) {
  neff_values <- neff_ratio(model)              # Here we calculate ratio (not the raw number of effective samples)
  data.frame(model = deparse(substitute(model)), 
             min_neff = min(neff_values), 
             max_neff = max(neff_values),
             mean_neff = mean(neff_values))
               
})

# Combine and inspect
do.call(rbind, neff_ratio_list)

```

So the highest ratio have model h1.m6c (lognormal with correlation) but in fact they are all quite comparable. Let's loot at 3 highest

```{r}

effective_sample(h1.m6c) # this one indeed seems the best as it has all ESS around 10k
effective_sample(h1.m6)
effective_sample(h1.m3p) 

```

So there are some considerable differences for different coefficients

### LOO & WAIC

Add criteria to each model (this takes a while) - CAN BE DELETED AFTER WE RESAVE IT WITH THE MODELS WITHIN THEIR CHUNKS

```{r}

h1.m1 <- add_criterion(h1.m1, criterion = c("loo", "waic"))
h1.m2 <- add_criterion(h1.m2, criterion = c("loo", "waic"))
h1.m3 <- add_criterion(h1.m3, criterion = c("loo", "waic"))
h1.m3p <- add_criterion(h1.m3p, criterion = c("loo", "waic"))
h1.m4 <- add_criterion(h1.m4, criterion = c("loo", "waic"))
h1.m5 <- add_criterion(h1.m5, criterion = c("loo", "waic"))
h1.m6 <- add_criterion(h1.m6, criterion = c("loo", "waic"))
h1.m6c <- add_criterion(h1.m6c, criterion = c("loo", "waic"))

```


Leave-one-out (loo) validation
```{r}

l <- loo_compare(h1.m1, h1.m2, h1.m3, h1.m3p, h1.m4, h1.m5, h1.m6, h1.m6c, criterion = "loo")

print(l, simplify = F)

```
elpd_loo: This is the expected log pointwise predictive density for LOO. Higher values indicate a better fit to the data.

se_elpd_loo: The standard error of the elpd_loo, representing uncertainty in the model’s predictive fit according to LOO.

looic: The LOO Information Criterion, which is similar to waic but based on leave-one-out cross-validation. Lower values are better.

p_loo: The effective number of parameters according to LOO, indicating the model’s complexity.

se_p_loo: The standard error of p_loo, representing uncertainty around the effective number of parameters.

So lognormal seems the best. 


Information criterion (WAIC)
```{r}

w <- loo_compare(h1.m1, h1.m2, h1.m3, h1.m3p, h1.m4, h1.m5, h1.m6, h1.m6c, criterion = "waic")

print(w, simplify = F)

# see Solomon Kurz
cbind(waic_diff = w[,1] * -2,
      se = w[,2] * 2)

```
elpd_waic (expected log pointwise predictive density for WAIC): This represents the model's predictive fit to the data. Higher values indicate a better fit.

se_elpd_waic (standard error of elpd_waic): Measures uncertainty around the elpd_waic estimate.

waic: The Widely Applicable Information Criterion, a measure of model fit where lower values indicate a better fit.

se_waic (standard error of WAIC): Uncertainty around the WAIC estimate.

elpd_diff: The difference in the elpd_waic between the model in question and the baseline model (fit_eff_2, which has elpd_diff of 0). A negative value indicates that the model fits worse than fit_eff_2.

se_diff: The standard error of the elpd_diff, indicating how much uncertainty there is in the difference in predictive performance.

p_waic: The number of effective parameters in the model (related to model complexity). Lower values indicate simpler models, and higher values suggest more complexity.

Plot the comparison
```{r}

library(tibble)
library(tidyverse)
library(rcartocolor)

w[, 7:8] %>% 
  data.frame() %>% 
  rownames_to_column("model_name") %>% 
  mutate(model_name = fct_reorder(model_name, waic, .desc = T)) %>% 
  
  ggplot(aes(x = waic, y = model_name, 
             xmin = waic - se_waic, 
             xmax = waic + se_waic)) +
  geom_pointrange(color = carto_pal(7, "BurgYl")[7], 
                  fill = carto_pal(7, "BurgYl")[5], shape = 21) +
  labs(title = "WAIC plot",
       x = NULL, y = NULL) +
  theme(axis.ticks.y = element_blank())

```

```{r}

model_weights(h1.m1, h1.m2, h1.m3, h1.m3p, h1.m4, h1.m5, h1.m6, h1.m6c, weights = "waic") %>% 
  round(digits = 2)

```

So as ppcheck already suggested, lognormal model indeed seem to have the most predictive power. For this particular (synthetic) data, we will now proceed with model h1.m6c

We will first add some mildly informative priors, and then we also try to add some interaction terms and do a comparison once again.

## Model 7 - lognormal with priors

Let's first check what priors have been selected as default for h1.m6c
```{r}

# Print priors
prior_summary(h1.m6c)

```
Ok, we can keep all defaulted ones, but we do not need to leave flat priors for the beta coefficients as we do have some assumptions/expectations

Let's reuse priors we have already used for h1.m3p

```{r}

priors_h1m7 <- c(
  set_prior("normal(2.5, 0.5)", class = "Intercept", lb=0),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt2M1"),
  set_prior("normal(0,0.50)", class = "b", coef = "CommAtt3M2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality1"),
  set_prior("normal(0,0.25)", class = "b", coef = "Modality2"),
  set_prior("normal(0,0.25)", class = "b", coef = "Big5"),
  set_prior("normal(0,0.25)", class = "b", coef = "Familiarity"),
  set_prior("normal(0,0.25)", class = "b", coef = "Expressibility_z")
)

# The rest we will leave default (and check afterwards)

h1.m7 <- brm(Eff ~ 1 + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), 
                data = final_data,
                family = lognormal(),
                prior = priors_h1m7,
                iter = 4000,
                cores = 4)


# Add criterions for later diagnostics
h1.m7 <- add_criterion(h1.m7, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h1.m7_R2 <- bayes_R2(h1.m7)

# Save both as objects
saveRDS(h1.m7, here("09_Analysis_Modeling", "models", "h1.m7.rds"))
saveRDS(h1.m7_R2, here("09_Analysis_Modeling", "models", "h1.m7_R2.rds"))

beep(5)


```

```{r}

h1.m7 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m7.rds"))
h1.m7_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m7_R2.rds"))


# Summary
summary(h1.m7)
# Coefficients remain mostly unchanged 


```


```{r}

plot(h1.m7)
# all good

plot(conditional_effects(h1.m7), points = TRUE)
# the effects all go in good direction

pp_check(h1.m7, type = "dens_overlay")
# nice

pp_check(h1.m7, type = "error_scatter_avg")
# unchanged

h1.m7_R2
# explained variance remains around 88%

```

Let's now also check whether the priors make sense 
```{r}

prior_summary(h1.m7)

pp_check(h1.m7,
         type = "stat",
         stat = "mean",
         bins = 50,
         prefix = "ppd") +
  coord_cartesian(xlim = c(-10, 10)) +
  ggtitle("Prior predictive distribution of means")
# this look okay

pp_check(h1.m7,
         type = "stat",
         stat = "min",
         prefix = "ppd") +
  coord_cartesian(xlim = c(-15, 10)) +
  ggtitle("Prior predictive distribution of minimal values")
# this looks good

pp_check(h1.m7,
         type = "stat",
         stat = "max",
         prefix = "ppd") +
  coord_cartesian(xlim = c(-10, 15)) +
  ggtitle("Prior predictive distribution of maximal values")
# this too looks reasonable



```


## Model 8 - adding interactions

Possible interactions include:

- CommAtt x Modality - Effort could increase differently across modalities, depending on whether concept is guessed on the first, second or third attempt. E.g., gesture might require more effort on initial attempt, but vocal require more effort in repeated attempt

Note: Interesting, it would help disentagle the benefit of multimodality over other - maybe more effort overal but less attempts. However, we already model effect of modality on effort, so maybe this is not top priority

- CommAtt x Expressibility - Higher expressibility should moderate the effect of repeated attempts, such that the increase in effort with each additional attempt is smaller (or bigger?) for more expressible concepts.

Note: Not priority I would say

- Modality × Expressibility_z - The influence of expressibility on effort could be modality-specific — perhaps effort increases less with expressibility in the combined modality.

Note: Not priority (especially since expressibiliy has already modality encoded)

- Familiarity x CommAtt - More familiar partners may guess faster (fewer attempts) and require less effort, but this effect could diminish over multiple attempts.

Note: Does not seem to be priority

- Big5 × Modality or Big5 × CommAtt - More open/extraverted participants might maintain higher effort over attempts, or adjust more dynamically depending on the communicative channel.

Note: Not priority, but it is interesting if we want to tap more into the interindividual variability

Let's try CommAtt x Modality and Big5 x CommAtt

```{r}

h1.m8 <- brm(Eff ~ 1 + CommAtt * Modality + Big5 * CommAtt + Familiarity + Expressibility_z +  (1 + CommAtt | Participant) + (1 + CommAtt | Concept) + (1 | TrialNumber_c), 
                data = final_data,
                family = lognormal(),
                prior = priors_h1m7, # we keep the priors from previous model
                iter = 4000,
                cores = 4)


# Add criterions for later diagnostics
h1.m8 <- add_criterion(h1.m8, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h1.m8_R2 <- bayes_R2(h1.m8)

# Save both as objects
saveRDS(h1.m8, here("09_Analysis_Modeling", "models", "h1.m8.rds"))
saveRDS(h1.m8_R2, here("09_Analysis_Modeling", "models", "h1.m8_R2.rds"))

beep(5)


```

```{r}

h1.m8 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m8.rds"))
h1.m8_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h1.m8_R2.rds"))


# Summary
summary(h1.m8)
# Coefficients remain mostly unchanged 
# Since we did not really focused on the interactions during the simulation, we also don't have strong expectations here. But for the real data, there is a good reason to expect some meaningful values


```


```{r}

plot(h1.m8)
# all good

plot(conditional_effects(h1.m8), points = TRUE)
# the effects all go in good direction
# we can see here that combined modality remains moderated across all commatts
# and also big5 seems to matter the most in the second attempt

pp_check(h1.m8, type = "dens_overlay")
# the problem with predicting negative values remains

pp_check(h1.m8, type = "error_scatter_avg")
# unchanged

h1.m8_R2
# explained variance remains around 87%

```


## Diagnostics II

### ESS

```{r}

effective_sample(h1.m6) 
effective_sample(h1.m6c)
effective_sample(h1.m7) 
effective_sample(h1.m8) 


# Now h1m6 looks as the weakest, while h1m8 looks much better - but ESS for Intercept is for some reason still quite low

```

### LOO & WAIC

Add the criteria to each model


```{r}

l <- loo_compare(h1.m6, h1.m6c, h1.m7, h1.m8, criterion = "loo")

print(l, simplify = F)

```

```{r}

w <- loo_compare(h1.m6, h1.m6c, h1.m7, h1.m8, criterion = "waic")

print(w, simplify = F)

# see Solomon Kurz
cbind(waic_diff = w[,1] * -2,
      se = w[,2] * 2)

```

So in terms of WAIC&LOO, interactions do not really add predictive power. This might be specific for the synthetic data, as we did not explicitly focused on the interaction coefficients there. At the same time, the difference of h1.m8 from h1.m6c is not so significant. For now, we stop here and with the real data, we will proceed with the same evaluation to pick up the best model


# H2: Data wrangling

Now we can need to account also for our second hypothesis, namely

*H2: A higher degree of misunderstanding will require a performer to engage in more effortful correction.*

The difficulty with our data structure is that PrevAn is a variable that has values only for second and third correction (i.e., there is no previous answer for the first performance). If we left it unchange, the model will eventually get rid of all NA values, which means we will loose the relationship between effort in second and first communicative attempt. To avoid this, we will create a new variable which we call *Effort Change Ratio*. We will simply calculate the ratio of change from effort in communicative attempt x to communicative attempt x+1. Like that, we will still get rid of communicative attempt 1, but the ratio that will be associated with CommAtt==2 already encapsulates the relationship towards this attempt.


```{r}

# load our data back if we lost them
final_data <- final_data_synt

```

Because we are dealing xxx

```{r}

final_data_2 <- final_data %>%
  group_by(Participant, Concept) %>%
  mutate(
    Effort_1 = Eff[CommAtt == 1][1],  # Effort for attempt 1
    Effort_2 = Eff[CommAtt == 2][1],  # Effort for attempt 2
    Effort_3 = Eff[CommAtt == 3][1],  # Effort for attempt 3
    Effort_Change_Ratio_1_to_2 = case_when(
      CommAtt == 2 & !is.na(Effort_1) ~ Eff / Effort_1,
      TRUE ~ NA_real_
    ),
    Effort_Change_Ratio_2_to_3 = case_when(
      CommAtt == 3 & !is.na(Effort_2) ~ Eff / Effort_2,
      TRUE ~ NA_real_
    )
  ) %>%
  ungroup()

# View the result
head(final_data_2)


```

```{r}

final_data_2 <- final_data_2 %>%
  mutate(
    Effort_Change_Ratio = coalesce(Effort_Change_Ratio_1_to_2, Effort_Change_Ratio_2_to_3)
  ) 

# Remove unnecessary columns
final_data_2 <- subset(final_data_2, select = -c(Effort_1, Effort_2, Effort_3, Effort_Change_Ratio_1_to_2, Effort_Change_Ratio_2_to_3)) 

# View the result
head(final_data_2)

```
## Exploring structure

Let's check in plots
```{r}

library(ggplot2)

# Filter out CommAtt == 1
filtered_data <- final_data_2[final_data_2$CommAtt != 1, ]

ggplot(filtered_data, aes(x = PrevAn, y = Eff)) +
  geom_point(alpha = 0.6, color = "blue") +  # Scatter points
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Regression line
  labs(x = "Previous Answer Similarity (PrevAn)", 
       y = "Effort (Eff)", 
       title = "Relationship between Effort and Previous Answer Similarity") +
  theme_minimal()


```

```{r}

ggplot(filtered_data, aes(x = PrevAn, y = Effort_Change_Ratio)) +
  geom_point(alpha = 0.6, color = "blue") +  # Scatter points
  geom_smooth(method = "lm", color = "red", se = FALSE) +  # Regression line
  labs(x = "Previous Answer Similarity (PrevAn)", 
       y = "Effort (Eff)", 
       title = "Relationship between Effort and Previous Answer Similarity") +
  theme_minimal()

```


```{r}

hist(filtered_data$Effort_Change_Ratio)

```
So we will have to work with bimodal distribution, as we either have a decrease in effort (Ratio < 1) or increase (Ratio > 1)

#H2: Stating causal model

We now also need a new DAG. Essentially, what we said will influence CommAtt in H1, will now also influence PrevAn because they are tightly related. For instance, more extroverted people can be expected to be better guessers, therefore the similarity of the previous answer will be higher.

```{r}

daggy_h2 <- dagitty('dag {
EffChange [outcome,pos="-0.102,0.025"]
PrevAn [exposure,pos="-1.033,0.028"]
Big5 [pos="-0.823,0.657"]
CommAtt [pos="-0.136,-0.848"]
Conc [pos="-1.136,-0.848"]
Expr [pos="-0.758,-0.850"]
Fam [pos="-0.379,0.663"]
Pcn [pos="-0.589,1.214"]
TrNum [pos="-1.686,-0.859"]
Mod[pos="-1.286,-0.859"]

Big5 -> PrevAn     
Big5 -> EffChange
CommAtt -> EffChange
Conc -> Expr
Expr -> PrevAn
Expr -> EffChange
Fam -> PrevAn
Fam -> EffChange
Mod -> EffChange
Mod -> PrevAn
Pcn -> Big5
Pcn -> PrevAn 
Pcn -> EffChange
Pcn -> Fam
TrNum -> PrevAn
TrNum -> EffChange
Conc -> PrevAn
Conc -> EffChange
PrevAn -> EffChange
PrevAn -> CommAtt
}')

plot(daggy_h2)

```
We will now see, that the adjustment set now also include PrevAn

```{r}

dagitty::adjustmentSets(daggy_h2, exposure = "PrevAn", outcome = "EffChange")

```
The adjustment set is identical to the one of H1. Note that we are here omitting arrows going from all these variables to ComAtt. However, since PrevAn affects CommAtt, and not the other way, we anyway do not need to block this path to avoid confounds. However, if we want to asses direct effect of PrevAn on EffChange, we might need to add it to the model 

# Modeling: H2

*H2: A higher degree of misunderstanding will require a performer to engage in more effortful correction.*

## Contrast coding

Convert collumns to factors
```{r}

filtered_data$CommAtt <- as.factor(filtered_data$CommAtt)
filtered_data$Modality <- as.factor(filtered_data$Modality)
filtered_data$Participant <- as.factor(filtered_data$Participant)
filtered_data$Concept <- as.factor(filtered_data$Concept)

filtered_data$TrialNumber <- as.numeric(filtered_data$TrialNumber)  # Ensure TrialNumber is numeric
```

Check contrasts of factors
```{r}
contrasts(filtered_data$CommAtt) <- MASS::contr.sdif(2) # but we don't need this one
contrasts(filtered_data$Modality) <- contr.sum(3)/2
```


Center trial number
```{r}
filtered_data$TrialNumber_c <- filtered_data$TrialNumber - median(range(filtered_data$TrialNumber))
range(filtered_data$TrialNumber_c)
range(filtered_data$TrialNumber)
```
The measures of familiarity, expressibility, and big5 are on a non-continuous rating scale, hence we can just subtract the median from these to centre them (instead of standardizing).

```{r}

filtered_data$Familiarity <- filtered_data$Familiarity - median(range(filtered_data$Familiarity))
filtered_data$Big5 <- filtered_data$Big5 - median(range(filtered_data$Big5))

```


Z-score expressibility (because it's continuous) within a modality #TODO- consider centering instead
```{r}

filtered_data <-
  filtered_data |>
  group_by(Modality) |>
  mutate(Expressibility_z = (Expressibility - mean(Expressibility))/ sd(filtered_data$Expressibility, na.rm = T)) |>
  ungroup()

filtered_data <-
  filtered_data |>
  #group_by(Modality) |>
  mutate(PrevAn_z = (PrevAn - mean(PrevAn))/ sd(filtered_data$PrevAn, na.rm = T)) |>
  ungroup()
```

To test, we will also center both expressibility and prevan to see what offers better interpretation
```{r}

filtered_data$PrevAn_c <- filtered_data$PrevAn - median(range(filtered_data$PrevAn))

filtered_data <- 
  filtered_data |>
  group_by(Modality) |>
  mutate(Expressibility_c = Expressibility - median(range(Expressibility))) |>
  ungroup()

```

## Model 1 - DAG 

```{r}


h2.m1 <- brm(Effort_Change_Ratio ~ 1 + PrevAn_z + CommAtt + Familiarity + Big5 + Expressibility_z + TrialNumber_c + Modality + (1 | Participant) + (1 | Concept),
                data = filtered_data,
                iter = 4000,
                cores = 4)

# Add criterions for later diagnostics
h2.m1 <- add_criterion(h2.m1, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h2.m1_R2 <- bayes_R2(h2.m1)

# Save both as objects
saveRDS(h2.m1, here("09_Analysis_Modeling", "models", "h2.m1.rds"))
saveRDS(h2.m1_R2, here("09_Analysis_Modeling", "models", "h2.m1_R2.rds"))

beep(5)


```

```{r}

h2.m1 <- readRDS(here("09_Analysis_Modeling", "models", "h2.m1.rds"))
h2.m1_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h2.m1_R2.rds"))


# Summary
summary(h2.m1)
# coefficients seem quite conservative, even for PrevAn (but remember, it's z-scored)

```

```{r}

plot(h2.m1)
# all looks ok

plot(conditional_effects(h2.m1), points = TRUE)
# the main effect seems ok, the rest of the predictors does not show nothing (maybe because of the transformation we lost the relations between them and the effort)

pp_check(h2.m1, type = "dens_overlay")
# this looks not great but not terrible

pp_check(h2.m1, type = "error_scatter_avg")
# so there seems to be quite high residual error for some extreme values (esp for the second mode)

h2.m1_R2
# explained variance around 98%

```

## Model 2 - log-normal

We will once again try lognormal distribution, for now leaving priors to default
```{r}

h2.m2 <- brm(Effort_Change_Ratio ~ 1 + PrevAn_z + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + PrevAn || Participant) + (1 + PrevAn || Concept) + (1 | TrialNumber_c), 
                data = filtered_data,
                family = lognormal(),
                iter = 4000,
                cores = 4)

# Add criterions for later diagnostics
h2.m2 <- add_criterion(h2.m2, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h2.m2_R2 <- bayes_R2(h2.m2)

# Save both as objects
saveRDS(h2.m2, here("09_Analysis_Modeling", "models", "h2.m2.rds"))
saveRDS(h2.m2_R2, here("09_Analysis_Modeling", "models", "h2.m2_R2.rds"))

beep(5)

```

```{r}

h2.m2 <- readRDS(here("09_Analysis_Modeling", "models", "h2.m2.rds"))
h2.m2_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h2.m2_R2.rds"))


# Summary
summary(h2.m2)
# 

```
### Converting to original scale

```{r}

# Extract posterior samples
posterior_samples <- as_draws_df(h2.m2)
alpha_samples <- posterior_samples$b_Intercept

# Create a list to store effects for each fixed factor
effect_list <- list()

# Helper function to calculate summary statistics
get_effect_summary <- function(effect_samples) {
  mean_effect <- mean(effect_samples)
  se_effect <- sd(effect_samples)
  ci_effect <- quantile(effect_samples, c(0.025, 0.975))
  post_prob <- mean(effect_samples > 0)
  c(mean = mean_effect, 
    se = se_effect, 
    lower_ci = ci_effect[1], 
    upper_ci = ci_effect[2], 
    post_prob = post_prob)
}

# Compute expected values on the log scale
mu_1 <- alpha_samples  # CommAtt 1

# Transform to original scale
effect_1 <- exp(mu_1)


# PrevAn (continuous)
if ("b_PrevAn_z" %in% colnames(posterior_samples)) {
  beta_prevAn <- posterior_samples$b_PrevAn_z
  effect_list$PrevAn_z <- get_effect_summary(exp(alpha_samples + beta_prevAn) - exp(alpha_samples))
}

effect_list$Intercept_mean <- mean(effect_1)

# Convert to a nicely formatted data frame
effect_df <- do.call(rbind, effect_list)

# View effects
effect_df

```
Still difficult to interpret as PrevAn is z-scored

```{r}

plot(h2.m2)
# looks good

plot(conditional_effects(h2.m2), points = TRUE)
# main predictor looks ok, but the rest still moderated 

pp_check(h2.m2, type = "dens_overlay")
# this looks very nice

pp_check(h2.m2, type = "error_scatter_avg")
# so higher values are kind blobby with no particular trend, but the lower values are still quite errorneous

h2.m2_R2
# explained variance 99%

```
## Splines 1 - fitting b-splines

We will use Richard McElreath's pipeline, and Solomon Kurz's adapted worksflow for brms

See more: https://bookdown.org/content/4857/geocentric-models.html#

```{r}

library(tidyverse)

d <-
  final_data_2 %>% 
  drop_na(PrevAn)

d$CommAtt <- as.factor(d$CommAtt)
d$Modality <- as.factor(d$Modality)
d$Participant <- as.factor(d$Participant)
d$Concept <- as.factor(d$Concept)

d$TrialNumber <- as.numeric(d$TrialNumber) 

```


```{r}

d %>% 
  select_if(is.numeric) %>%  # Select only numeric columns
  pivot_longer(cols = everything(), names_to = "key", values_to = "value") %>%
  group_by(key) %>%
  summarise(
    mean = mean(value, na.rm = TRUE),
    sd   = sd(value, na.rm = TRUE),
    ll   = quantile(value, probs = 0.055, na.rm = TRUE),
    ul   = quantile(value, probs = 0.945, na.rm = TRUE)
  ) %>%
  mutate(across(where(is.double), round, digits = 2))

print(d)

```


```{r}

d %>% 
  ggplot(aes(x = PrevAn, y = Effort_Change_Ratio)) +
  # color from here: https://www.colorhexa.com/ffb7c5
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        # color from here: https://www.colordic.org/w/, inspired by https://chichacha.netlify.com/2018/11/29/plotting-traditional-colours-of-japan/
        panel.background = element_rect(fill = "#4f455c"))
```

```{r}

num_knots <- 7
knot_list <- quantile(d$PrevAn, probs = seq(from = 0, to = 1, length.out = num_knots))
knot_list

```

```{r}

str(filtered_data)

```

```{r}

d %>% 
  ggplot(aes(x = PrevAn, y = Effort_Change_Ratio)) +
  geom_vline(xintercept = knot_list, color = "white", alpha = 1/2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())

```


```{r}

library(splines)

B <- bs(d$PrevAn,
        knots = knot_list[-c(1, num_knots)], 
        degree = 3, 
        intercept = TRUE)

```



```{r}
B %>% str()
```


```{r}

knot_list[c(1, num_knots)]
```




```{r}


# wrangle a bit
b <-
  B %>% 
  data.frame() %>% 
  set_names(str_c(0, 1:4), 5:9) %>%  
  bind_cols(select(d, PrevAn)) %>% 
  pivot_longer(-PrevAn,
               names_to = "bias_function",
               values_to = "bias")

# plot
b %>% 
  ggplot(aes(x = PrevAn, y = bias, group = bias_function)) +
  geom_vline(xintercept = knot_list, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  ylab("bias value") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())

```


```{r}

d2 <-
  d %>% 
  mutate(B = B) 

# take a look at the structure of `d3
d2 %>% glimpse()

```

```{r}

h2.s1 <- 
  brm(data = d2,
      family = gaussian,
      Effort_Change_Ratio ~ 1 + B,
      prior = c(prior(normal(100, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 4)


# Add criterions for later diagnostics
h2.s1 <- add_criterion(h2.s1, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h2.s1_R2 <- bayes_R2(h2.s1)

# Save both as objects
saveRDS(h2.s1, here("09_Analysis_Modeling", "models", "h2.s1.rds"))
saveRDS(h2.s1_R2, here("09_Analysis_Modeling", "models", "h2.s1_R2.rds"))


```

Model summary
```{r}

print(h2.s1)

```


```{r}

post <- as_draws_df(h2.s1)

glimpse(post)

```


```{r}

post %>% 
  select(b_B1:b_B9) %>% 
  set_names(c(str_c(0, 1:4), 5:9)) %>% 
  pivot_longer(everything(), names_to = "bias_function") %>% 
  group_by(bias_function) %>% 
  summarise(weight = mean(value)) %>% 
  full_join(b, by = "bias_function") %>% 
  
  # plot
  ggplot(aes(x = PrevAn, y = bias * weight, group = bias_function)) +
  geom_vline(xintercept = knot_list, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank()) 

```

```{r}

f <- fitted(h2.s1)

f %>% 
  data.frame() %>% 
  bind_cols(d2) %>% 
  
  ggplot(aes(x = PrevAn, y = Effort_Change_Ratio, ymin = Q2.5, ymax = Q97.5)) + 
  geom_vline(xintercept = knot_list, color = "white", alpha = 1/2) +
  geom_hline(yintercept = fixef(h2.s1)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(x = "Previous Answer",
       y = "Effort Change Ratio") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())

```

```{r}

plot(h2.s1)
# looks good

pp_check(h2.s1, type = "dens_overlay")
# same as before, ignoring bimodality

pp_check(h2.s1, type = "error_scatter_avg")
# still weird

h2.s1_R2
# explained variance 4%

```

## Splines 2 - Bayesian GAMs

We will use Richard McElreath's pipeline, or Solomon Kurz's adapted worksflow for brms

See more: https://bookdown.org/content/4857/geocentric-models.html#

Now GAM with specifying we want b-splines
```{r}

h2.s2 <-
  brm(data = d2,
      family = gaussian,
      Effort_Change_Ratio ~ 1 + s(PrevAn, bs = "bs", k = 19),
      prior = c(prior(normal(100, 10), class = Intercept),
                prior(normal(0, 10), class = b),
                prior(student_t(3, 0, 5.9), class = sds),
                prior(exponential(1), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4,
      seed = 4,
      control = list(adapt_delta = .99)
      )

# Add criterions for later diagnostics
h2.s2 <- add_criterion(h2.s2, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h2.s2_R2 <- bayes_R2(h2.s2)

# Save both as objects
saveRDS(h2.s2, here("09_Analysis_Modeling", "models", "h2.s2.rds"))
saveRDS(h2.s2_R2, here("09_Analysis_Modeling", "models", "h2.s2_R2.rds"))

```

```{r}

h2.s2 <- readRDS(here("09_Analysis_Modeling", "models", "h2.s2.rds"))
h2.s2_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h2.s2_R2.rds"))


# Summary
summary(h2.s2)

```


```{r}

plot(h2.s2)
# looks good

plot(conditional_effects(h2.s2), points = TRUE)

pp_check(h2.s2, type = "dens_overlay")
# same as before, ignoring bimodality

pp_check(h2.s2, type = "error_scatter_avg")
# still weird

h2.s2_R2
# explained variance 4%

```

## Splines 3 - GAMs with f+r effects
```{r}

h2.s3 <- 
  brm(
    data = filtered_data,
    family = gaussian,
    Effort_Change_Ratio ~ 1 +
      s(PrevAn_z, bs = "bs", k = 19) +  # Smooth for Previous Answer similarity
      + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  # Fixed effects
      (1 | Participant) + (1 | Concept),  # Random effects
    prior = c(
      prior(normal(100, 10), class = Intercept),
      prior(normal(0, 10), class = b),
      prior(student_t(3, 0, 5.9), class = sds),
      prior(exponential(1), class = sigma)
    ),
    iter = 4000, warmup = 2000, chains = 4, cores = 4,
    seed = 4,
    control = list(adapt_delta = .99)
  )


# Add criterions for later diagnostics
h2.s3 <- add_criterion(h2.s3, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h2.s3_R2 <- bayes_R2(h2.s3)

# Save both as objects
saveRDS(h2.s3, here("09_Analysis_Modeling", "models", "h2.s3.rds"))
saveRDS(h2.s3_R2, here("09_Analysis_Modeling", "models", "h2.s3_R2.rds"))


```

```{r}


h2.s3 <- readRDS(here("09_Analysis_Modeling", "models", "h2.s3.rds"))
h2.s3_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h2.s3_R2.rds"))


# Summary
summary(h2.s3)


```



```{r}


plot(h2.s3)
# looks ok

plot(conditional_effects(h2.s3), points = TRUE)

pp_check(h2.s3, type = "dens_overlay")
# still shit

pp_check(h2.s3, type = "error_scatter_avg")
# also

h2.s3_R2
# 98% variance is super over-confident, given the bad performance


```


## Splines 4 - GAMs with lognormal distribution

```{r}


h2.s4 <- 
  brm(
    Effort_Change_Ratio ~ 1 + 
      s(PrevAn_z, bs = "bs", k = 19) +  # Nonlinear effect for Previous Answer similarity
      + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  
      (1 + PrevAn_z || Participant) +  # Random slopes and intercepts for Participant
      (1 + PrevAn_z || Concept) +      # Random slopes and intercepts for Concept
      (1 | TrialNumber_c),           # Random intercept for Trial Number
    data = filtered_data,
    family = lognormal(),
    iter = 4000,
    warmup = 2000,
    chains = 4,
    cores = 4,
    seed = 4,
    control = list(adapt_delta = .999, 
                   max_treedepth = 12)
  )

# Add criterions for later diagnostics
h2.s4 <- add_criterion(h2.s4, criterion = c("loo", "waic"))

# Calculate also variance explained (R^2)
h2.s4_R2 <- bayes_R2(h2.s4)

# Save both as objects
saveRDS(h2.s4, here("09_Analysis_Modeling", "models", "h2.s4.rds"))
saveRDS(h2.s4_R2, here("09_Analysis_Modeling", "models", "h2.s4_R2.rds"))

```

```{r}


h2.s4 <- readRDS(here("09_Analysis_Modeling", "models", "h2.s4.rds"))
h2.s4_R2 <- readRDS(here("09_Analysis_Modeling", "models", "h2.s4_R2.rds"))


# Summary
summary(h2.s4)
#

```


```{r}


plot(h2.s4)
# looks ok

plot(conditional_effects(h2.s4), points = TRUE)
# for main predictor ok, other are strongly moderated

pp_check(h2.s4, type = "dens_overlay")
# but this looks very good

pp_check(h2.s4, type = "error_scatter_avg")
# so for the higher-values mode it looks ok (blob around 0), but the low values are still quite error-prone

h2.s4_R2
# 99% explained variance - might be we are overfitting here


```


## Diagnostics I

Before we proceed further with adding priors and/or interactions, let's do first round of diagnostics

```{r}

model_list <- list(h2.m1, h2.m2, h2.s1, h2.s2, h2.s3, h2.s4)

```

### Rhat

```{r}

# Extract R-hat values for each model
rhat_list <- lapply(model_list, function(model) {
  rhat_values <- rhat(model)
  data.frame(model = deparse(substitute(model)), 
             max_rhat = max(rhat_values), 
             min_rhat = min(rhat_values))
})

# Combine and inspect
do.call(rbind, rhat_list)

```
They all look good but the last one (lognormal GAMs) is reaching 1.01

RESULTS

### ESS

Effective sample size tells how many independent samples the model has effectively drawn from the PD. Low ESS suggests autocorrelation (i.e., sample explores one part of posterior), while high ESS means good mix

```{r}

# Extract n_eff values for each model
neff_ratio_list <- lapply(model_list, function(model) {
  neff_values <- neff_ratio(model)              # Here we calculate ratio (not the raw number of effective samples)
  data.frame(model = deparse(substitute(model)), 
             min_neff = min(neff_values), 
             max_neff = max(neff_values),
             mean_neff = mean(neff_values))
               
})

# Combine and inspect
do.call(rbind, neff_ratio_list)

```

RESULTS

```{r}

effective_sample(h2.s3) # this one looks actually very nice
effective_sample(h2.m1)
effective_sample(h2.m2) 


```


### LOO & WAIC

Leave-one-out (loo) validation
```{r}

l <- loo_compare(h2.m1, h2.m2, h2.s1, h2.s2, h2.s3, h2.s4, criterion = "loo")

print(l, simplify = F)

```

RESULTS


Information criterion (WAIC)
```{r}

w <- loo_compare(h2.m1, h2.m2, h2.s1, h2.s2, h2.s3, h2.s4, criterion = "waic")

print(w, simplify = F)

# see Solomon Kurz
cbind(waic_diff = w[,1] * -2,
      se = w[,2] * 2)

```

RESULTS

Plot the comparison
```{r}

library(tibble)
library(tidyverse)
library(rcartocolor)

w[, 7:8] %>% 
  data.frame() %>% 
  rownames_to_column("model_name") %>% 
  mutate(model_name = fct_reorder(model_name, waic, .desc = T)) %>% 
  
  ggplot(aes(x = waic, y = model_name, 
             xmin = waic - se_waic, 
             xmax = waic + se_waic)) +
  geom_pointrange(color = carto_pal(7, "BurgYl")[7], 
                  fill = carto_pal(7, "BurgYl")[5], shape = 21) +
  labs(title = "WAIC plot",
       x = NULL, y = NULL) +
  theme(axis.ticks.y = element_blank())

```

```{r}

model_weights(h2.m1, h2.m2, h2.s1, h2.s2, h2.s3, h2.s4, weights = "waic") %>% 
  round(digits = 2)

```

RESULTS

## ADDING PRIORS
## ADDING INTERACTIONS

Possible interactions include:

- PrevAn x Modality - similarity affects the change in effort differently (e.g., vocal might still require more effort)

- PrevAn x Expressibility - similarity in relation to effort might matter only for highly expressible concepts, and low expressible concepts are difficult to express, so also difficult to exaggerate

- Familiarity x PrevAn - if the guess is really bad, only very familiar people might be motivated enough to put more effort

- Big x PrevAn - same like with familiarity



