brm(
Effort_Change_Ratio ~ 1 +
s(PrevAn_z, bs = "bs", k = 19) +  # Nonlinear effect for Previous Answer similarity
Modality + Big5 + Familiarity + Expressibility_z +
(1 + CommAtt || Participant) +  # Random slopes and intercepts for Participant
(1 + CommAtt || Concept) +      # Random slopes and intercepts for Concept
(1 | TrialNumber_c),           # Random intercept for Trial Number
data = filtered_data,
family = lognormal(),
iter = 4000,
warmup = 2000,
chains = 4,
cores = 4,
seed = 4,
control = list(adapt_delta = .999,
max_treedepth = 12)
)
filtered_data
View(filtered_data)
View(filtered_data)
h2.s4 <-
brm(
Effort_Change_Ratio ~ 1 +
s(PrevAn_z, bs = "bs", k = 19) +  # Nonlinear effect for Previous Answer similarity
Modality + Big5 + Familiarity + Expressibility_z +
(1 + CommAtt || Participant) +  # Random slopes and intercepts for Participant
(1 + CommAtt || Concept) +      # Random slopes and intercepts for Concept
(1 | TrialNumber_c),           # Random intercept for Trial Number
data = filtered_data,
family = lognormal(),
iter = 4000,
warmup = 2000,
chains = 4,
cores = 4,
seed = 4,
control = list(adapt_delta = .999,
max_treedepth = 12)
)
# Add criterions for later diagnostics
h2.s4 <- add_criterion(h2.s4, criterion = c("loo", "waic"))
# Calculate also variance explained (R^2)
h2.s4_R2 <- bayes_R2(h2.s4)
# Save both as objects
saveRDS(h2.s4, here("09_Analysis_Modeling", "models", "h2.s4.rds"))
saveRDS(h2.s4_R2, here("09_Analysis_Modeling", "models", "h2.s4_R2.rds"))
h2.m3 <- brm(
Effort_Change_Ratio ~ 1 + PrevAn_z + Modality + Big5 + Familiarity + Expressibility_z +
(1 + PrevAn || Participant) + (1 + PrevAn || Concept) + (1 | TrialNumber_c),
data = filtered_data,
family = mixture(normal(), normal()),  # Mixture of two normal distributions
iter = 4000,
cores = 4
)
h2.m3 <- brm(
Effort_Change_Ratio ~ 1 + PrevAn_z + Modality + Big5 + Familiarity + Expressibility_z +
(1 + PrevAn || Participant) + (1 + PrevAn || Concept) + (1 | TrialNumber_c),
data = filtered_data,
family = mixture(gaussian(), gaussian()),  # Mixture of two normal distributions
iter = 4000,
cores = 4
)
h2.m3 <- brm(
Effort_Change_Ratio ~ 1 + PrevAn_z + Modality + Big5 + Familiarity + Expressibility_z +
(1 + PrevAn || Participant) + (1 + PrevAn || Concept) + (1 | TrialNumber_c),
data = filtered_data,
family = mixture(gaussian(), gaussian()),  # Mixture of two normal distributions
iter = 4000,
cores = 4,
control = list(adapt_delta = .999,
max_treedepth = 12)
)
# Summary
summary(h2.s4)
plot(h2.s4)
plot(conditional_effects(h2.s4), points = TRUE)
pp_check(h2.s4, type = "dens_overlay")
pp_check(h2.s4, type = "error_scatter_avg")
h2.s4_R2
pp_check(h2.s4, type = "dens_overlay")
model_list <- list(h2.m1, h2.m2, h2.s1, h2.s2, h2.s3, h2.s4)
# Extract R-hat values for each model
rhat_list <- lapply(model_list, function(model) {
rhat_values <- rhat(model)
data.frame(model = deparse(substitute(model)),
max_rhat = max(rhat_values),
min_rhat = min(rhat_values))
})
# Combine and inspect
do.call(rbind, rhat_list)
# Extract n_eff values for each model
neff_ratio_list <- lapply(model_list, function(model) {
neff_values <- neff_ratio(model)              # Here we calculate ratio (not the raw number of effective samples)
data.frame(model = deparse(substitute(model)),
min_neff = min(neff_values),
max_neff = max(neff_values),
mean_neff = mean(neff_values))
})
# Combine and inspect
do.call(rbind, neff_ratio_list)
l <- loo_compare(h2.m1, h2.m2, h2.s1, h2.s2, h2.s3, h2.s4, criterion = "loo")
# Add criterions for later diagnostics
h2.s3 <- add_criterion(h2.s3, criterion = c("loo", "waic"))
l <- loo_compare(h2.m1, h2.m2, h2.s1, h2.s2, h2.s3, h2.s4, criterion = "loo")
print(l, simplify = F)
effective_sample(h2.s3)
effective_sample(h2.m1)
effective_sample(h2.m2)
w <- loo_compare(h2.m1, h2.m2, h2.s1, h2.s2, h2.s3, h2.s4, criterion = "waic")
print(w, simplify = F)
# see Solomon Kurz
cbind(waic_diff = w[,1] * -2,
se = w[,2] * 2)
library(tibble)
library(tidyverse)
library(rcartocolor)
w[, 7:8] %>%
data.frame() %>%
rownames_to_column("model_name") %>%
mutate(model_name = fct_reorder(model_name, waic, .desc = T)) %>%
ggplot(aes(x = waic, y = model_name,
xmin = waic - se_waic,
xmax = waic + se_waic)) +
geom_pointrange(color = carto_pal(7, "BurgYl")[7],
fill = carto_pal(7, "BurgYl")[5], shape = 21) +
labs(title = "WAIC plot",
x = NULL, y = NULL) +
theme(axis.ticks.y = element_blank())
model_weights(h2.m1, h2.m2, h2.s1, h2.s2, h2.s3, h2.s4, weights = "waic") %>%
round(digits = 2)
pp_check(h2.s1, type = "dens_overlay")
pp_check(h2.m1, type = "dens_overlay")
h2.m1 <- brm(Effort_Change_Ratio ~ 1 + PrevAn_z + CommAtt + Familiarity + Big5 + Expressibility_z + TrialNumber_c + Modality + (1 | Participant) + (1 | Concept),
data = filtered_data,
iter = 4000,
cores = 4)
pp_check(h2.m1, type = "dens_overlay")
plot(conditional_effects(h2.m1), points = TRUE)
# Add criterions for later diagnostics
h2.m1 <- add_criterion(h2.m1, criterion = c("loo", "waic"))
# Add criterions for later diagnostics
h2.m1 <- add_criterion(h2.m1, criterion = c("loo", "waic"))
# Calculate also variance explained (R^2)
h2.m1_R2 <- bayes_R2(h2.m1)
# Save both as objects
saveRDS(h2.m1, here("09_Analysis_Modeling", "models", "h2.m1.rds"))
saveRDS(h2.m1_R2, here("09_Analysis_Modeling", "models", "h2.m1_R2.rds"))
# Summary
summary(h2.m1)
pp_check(h2.m1, type = "error_scatter_avg")
pp_check(h2.m1, type = "dens_overlay")
pp_check(h2.m1, type = "error_scatter_avg")
# so there seems to be quite high residual error for some extreme values (esp for the second mode)
h2.m1_R2
h2.m2 <- brm(Effort_Change_Ratio ~ 1 + PrevAn_z + CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  (1 + PrevAn || Participant) + (1 + PrevAn || Concept) + (1 | TrialNumber_c),
data = filtered_data,
family = lognormal(),
iter = 4000,
cores = 4)
# Add criterions for later diagnostics
h2.m2 <- add_criterion(h2.m2, criterion = c("loo", "waic"))
# Calculate also variance explained (R^2)
h2.m2_R2 <- bayes_R2(h2.m2)
# Save both as objects
saveRDS(h2.m2, here("09_Analysis_Modeling", "models", "h2.m2.rds"))
saveRDS(h2.m2_R2, here("09_Analysis_Modeling", "models", "h2.m2_R2.rds"))
beep(5)
# Summary
summary(h2.m2)
# Extract posterior samples
posterior_samples <- as_draws_df(h2.m2)
alpha_samples <- posterior_samples$b_Intercept
# Create a list to store effects for each fixed factor
effect_list <- list()
# Helper function to calculate summary statistics
get_effect_summary <- function(effect_samples) {
mean_effect <- mean(effect_samples)
se_effect <- sd(effect_samples)
ci_effect <- quantile(effect_samples, c(0.025, 0.975))
post_prob <- mean(effect_samples > 0)
c(mean = mean_effect,
se = se_effect,
lower_ci = ci_effect[1],
upper_ci = ci_effect[2],
post_prob = post_prob)
}
# Compute expected values on the log scale
mu_1 <- alpha_samples  # CommAtt 1
# Transform to original scale
effect_1 <- exp(mu_1)
# PrevAn (continuous)
if ("b_PrevAn_z" %in% colnames(posterior_samples)) {
beta_prevAn <- posterior_samples$b_PrevAn_z
effect_list$PrevAn_z <- get_effect_summary(exp(alpha_samples + beta_prevAn) - exp(alpha_samples))
}
effect_list$Intercept_mean <- mean(effect_1)
# Convert to a nicely formatted data frame
effect_df <- do.call(rbind, effect_list)
# View effects
effect_df
plot(conditional_effects(h2.m2), points = TRUE)
pp_check(h2.m2, type = "dens_overlay")
pp_check(h2.m2, type = "error_scatter_avg")
h2.m2_R2
h2.s3 <-
brm(
data = filtered_data,
family = gaussian,
Effort_Change_Ratio ~ 1 +
s(PrevAn_z, bs = "bs", k = 19) +  # Smooth for Previous Answer similarity
+ CommAtt + Modality + Big5 + Familiarity + Expressibility_z +  # Fixed effects
(1 | Participant) + (1 | Concept),  # Random effects
prior = c(
prior(normal(100, 10), class = Intercept),
prior(normal(0, 10), class = b),
prior(student_t(3, 0, 5.9), class = sds),
prior(exponential(1), class = sigma)
),
iter = 4000, warmup = 2000, chains = 4, cores = 4,
seed = 4,
control = list(adapt_delta = .99)
)
# Add criterions for later diagnostics
h2.s3 <- add_criterion(h2.s3, criterion = c("loo", "waic"))
# Calculate also variance explained (R^2)
h2.s3_R2 <- bayes_R2(h2.s3)
# Save both as objects
saveRDS(h2.s3, here("09_Analysis_Modeling", "models", "h2.s3.rds"))
saveRDS(h2.s3_R2, here("09_Analysis_Modeling", "models", "h2.s3_R2.rds"))
h2.s4 <-
brm(
Effort_Change_Ratio ~ 1 +
s(PrevAn_z, bs = "bs", k = 19) +  # Nonlinear effect for Previous Answer similarity
+ CommAtt + Modality + Big5 + Familiarity + Expressibility_z +
(1 + PrevAn_z || Participant) +  # Random slopes and intercepts for Participant
(1 + PrevAn_z || Concept) +      # Random slopes and intercepts for Concept
(1 | TrialNumber_c),           # Random intercept for Trial Number
data = filtered_data,
family = lognormal(),
iter = 4000,
warmup = 2000,
chains = 4,
cores = 4,
seed = 4,
control = list(adapt_delta = .999,
max_treedepth = 12)
)
# Add criterions for later diagnostics
h2.s4 <- add_criterion(h2.s4, criterion = c("loo", "waic"))
# Calculate also variance explained (R^2)
h2.s4_R2 <- bayes_R2(h2.s4)
# Save both as objects
saveRDS(h2.s4, here("09_Analysis_Modeling", "models", "h2.s4.rds"))
saveRDS(h2.s4_R2, here("09_Analysis_Modeling", "models", "h2.s4_R2.rds"))
pp_check(h2.s4, type = "dens_overlay")
h2.s4_R2
pp_check(h2.s4, type = "error_scatter_avg")
model_list <- list(h2.m1, h2.m2, h2.s1, h2.s2, h2.s3, h2.s4)
# Extract R-hat values for each model
rhat_list <- lapply(model_list, function(model) {
rhat_values <- rhat(model)
data.frame(model = deparse(substitute(model)),
max_rhat = max(rhat_values),
min_rhat = min(rhat_values))
})
# Combine and inspect
do.call(rbind, rhat_list)
# Extract n_eff values for each model
neff_ratio_list <- lapply(model_list, function(model) {
neff_values <- neff_ratio(model)              # Here we calculate ratio (not the raw number of effective samples)
data.frame(model = deparse(substitute(model)),
min_neff = min(neff_values),
max_neff = max(neff_values),
mean_neff = mean(neff_values))
})
# Combine and inspect
do.call(rbind, neff_ratio_list)
library(here)
library(dplyr) # for data-wrangling
library(lme4)  # for linear mixed-effects models
library(tidyr)  # for reshaping data (if needed)
library(ggplot2)
library(tibble)
library(rcartocolor)
library(ggdag) # for dag
library(dagitty)
library(ggplot2) # bayesian stuff
library(patchwork)
library(bayesplot)
library(brms)
library(beepr)
library(bayestestR)
library(tidyverse)
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())
featurefolder <- paste0(parentfolder, '/07_TS_featureExtraction/Datasets/')
datasets      <- paste0(parentfolder, '/09_Analysis_Modeling/datasets/')
models        <- paste0(parentfolder, '/09_Analysis_Modeling/models/')
plots         <- paste0(parentfolder, '/09_Analysis_Modeling/plots/')
data_orig <- read_csv(paste0(featurefolder, "features_df_final.csv"))
View(data_orig)
View(data_orig)
library(here)
library(dplyr) # for data-wrangling
library(lme4)  # for linear mixed-effects models
library(tidyr)  # for reshaping data (if needed)
library(ggplot2)
library(tibble)
library(rcartocolor)
library(ggdag) # for dag
library(dagitty)
library(ggplot2) # bayesian stuff
library(patchwork)
library(bayesplot)
library(brms)
library(beepr)
library(bayestestR)
library(tidyverse)
# current folder (first go to session -> set working directory -> to source file location)
parentfolder <- dirname(getwd())
featurefolder <- paste0(parentfolder, '/07_TS_featureExtraction/Datasets/')
demofolder    <- paste0(parentfolder, '/00_RAWDATA/Demographics_all/')
datasets      <- paste0(parentfolder, '/09_Analysis_Modeling/datasets/')
models        <- paste0(parentfolder, '/09_Analysis_Modeling/models/')
plots         <- paste0(parentfolder, '/09_Analysis_Modeling/plots/')
data_orig <- read_csv(paste0(featurefolder, "features_df_final.csv"))
data_demo <- read_csv(paste0(demofolder, "all_demodata.csv"))
View(data_demo)
View(data_demo)
data_feat <- read_csv(paste0(featurefolder, "features_df_final.csv"))
data_demo <- read_csv(paste0(demofolder, "all_demodata.csv"))
featstokeep <- c('COPc_pospeak_mean', 'envelope_pospeak_mean', 'arm_moment_sum_change_pospeak_mean', 'COPc_integral', 'envelope_integral', 'arm_moment_sum_change_integral', 'correction_info', 'concept', 'TrialID', 'modality', 'expressibility', 'answer_prev', 'answer_prev_dist')
demotokeep <- c('BFI_extra', 'BFI_agree', 'BFI_consc', 'BFI_negemo', 'BFI_open', 'Familiarity', 'pcnID')
# Subset the dataframe
data_feat <- data_feat %>% select(all_of(featstokeep))
demotokeep <- demotokeep %>% select(all_of(demotokeep))
featstokeep <- c('COPc_pospeak_mean', 'envelope_pospeak_mean', 'arm_moment_sum_change_pospeak_mean', 'COPc_integral', 'envelope_integral', 'arm_moment_sum_change_integral', 'correction_info', 'concept', 'TrialID', 'modality', 'expressibility', 'answer_prev', 'answer_prev_dist')
demotokeep <- c('BFI_extra', 'BFI_agree', 'BFI_consc', 'BFI_negemo', 'BFI_open', 'Familiarity', 'pcnID')
# Subset the dataframe
data_feat <- data_feat %>% select(all_of(featstokeep))
data_demo <- data_demo %>% select(all_of(demotokeep))
data_feat <- read_csv(paste0(featurefolder, "features_df_final.csv"))
data_demo <- read_csv(paste0(demofolder, "all_demodata.csv"))
featstokeep <- c('COPc_pospeak_mean', 'envelope_pospeak_mean', 'arm_moment_sum_change_pospeak_mean', 'COPc_integral', 'envelope_integral', 'arm_moment_sum_change_integral', 'correction_info', 'concept', 'TrialID', 'modality', 'expressibility', 'answer_prev', 'answer_prev_dist')
demotokeep <- c('BFI_extra', 'BFI_agree', 'BFI_consc', 'BFI_negemo', 'BFI_open', 'Familiarity', 'pcnID')
# Subset the dataframe
data_feat <- data_feat %>% select(all_of(featstokeep))
data_demo <- data_demo %>% select(all_of(demotokeep))
featstokeep <- c('COPc_pospeak_mean', 'envelope_pospeak_mean', 'arm_moment_sum_change_pospeak_mean', 'COPc_integral', 'envelope_integral', 'arm_moment_sum_change_integral', 'correction_info', 'concept', 'TrialID', 'modality', 'expressibility', 'answer_prev', 'answer_prev_dist')
demotokeep <- c('BFI_extra', 'BFI_agree', 'BFI_consc', 'BFI_negemo', 'BFI_open', 'Familiarity', 'pcn_ID')
# Subset the dataframe
data_feat <- data_feat %>% select(all_of(featstokeep))
data_demo <- data_demo %>% select(all_of(demotokeep))
View(data_feat)
View(data_feat)
# Extract participant part from TrialID
data_feat <- data_feat %>%
mutate(
pcn_ID = str_extract(TrialID, "^[0-9]+") %>%  # Extract first number (before first underscore)
paste0("_", str_extract(TrialID, "p[0-9]+") %>% str_remove("p")) # Extract last part and format as pcn_ID
)
# Extract participant part from TrialID
data_feat <- data_feat %>%
mutate(
pcn_ID = str_extract(TrialID, "^[0-9]+") %>%  # Extract first number (before first underscore)
paste0("_", str_extract(TrialID, "p[0-9]+") %>% str_remove("p")) # Extract last part and format as pcn_ID
)
final_data <- left_join(data_feat, data_demo, by = "pcn_ID")
View(final_data)
View(final_data)
View(final_data)
# Set seed for reproducibility
set.seed(0209)
# Set coefficients
b_exp_vocal <- 0.6  # Vocal has lower expressibility
b_exp_multi <- 1.5  # Multimodal has higher expressibility
b_bif <- 1.15  # More extroverted → more effort
b_fam <- 1.10  # More familiarity → more effort
b_exp <- 1.20  # More expressibility → more effort
b_multi <- 0.70  # Multimodal → slightly reduced effort
b_comatt2 <- 1.50  # Effort increases for second attempt
b_comatt3 <- 0.50  # Effort decreases for third attempt
b_prevan <- 0.50  # Higher previous answer similarity → less effort
# Define participants and concepts
n_participants <- 120
n_total_concepts <- 21  # Each participant works with all 21 concepts
n_modalities <- 3  # Gesture, vocal, combined
# Generate participant-level data
participants <- 1:n_participants
Big5 <- runif(n_participants, min = 0, max = 2)
Familiarity <- runif(n_participants, min = 0, max = 2)
# Generate expressibility scores for each concept across modalities
expressibility_matrix <- matrix(runif(n_total_concepts * n_modalities, min = 0, max = 1),
nrow = n_total_concepts, ncol = n_modalities)
# Initialize data storage
final_data_synt <- data.frame()
# Define function to simulate participant data
simulate_participant <- function(participant_id) {
participant_data <- data.frame()
trial_number <- 1
for (concept_id in 1:n_total_concepts) {
# Assign a random modality
modality <- sample(c("gesture", "vocal", "combined"), 1)
# Get expressibility score based on modality
expressibility_score <- ifelse(modality == "vocal", expressibility_matrix[concept_id, 1] * b_exp_vocal,
ifelse(modality == "gesture", expressibility_matrix[concept_id, 2],
expressibility_matrix[concept_id, 3] * b_exp_multi))
# Base effort before adjustments
base_effort <- b_bif * Big5[participant_id] +
b_fam * Familiarity[participant_id] +
b_exp * expressibility_score +
rnorm(1, mean = 1, sd = 0.5)
# Adjust effort for multimodal condition
if (modality == "combined") {
base_effort <- base_effort * b_multi
}
# Sample the number of communicative attempts (CommAtt)
adjusted_prob <- c(1 - Familiarity[participant_id],
1 - Familiarity[participant_id],
1 - Familiarity[participant_id]) *
c(1 - Big5[participant_id],
1 - Big5[participant_id],
1 - Big5[participant_id]) *
c(1 - expressibility_score,
1 - expressibility_score,
1 - expressibility_score)
adjusted_prob <- adjusted_prob / sum(adjusted_prob)
n_attempts <- sample(1:3, 1, prob = adjusted_prob)
prev_answer_similarity <- NA  # First attempt has no previous similarity
# Generate rows for each communicative attempt
for (attempt in 1:n_attempts) {
Eff <- base_effort  # Start with base effort
# Modify effort for second and third attempts
if (attempt == 2) {
Eff <- Eff * b_comatt2
} else if (attempt == 3) {
Eff <- Eff * b_comatt3
}
# Adjust effort based on previous answer similarity
if (!is.na(prev_answer_similarity)) {
Eff <- Eff * (1 + (1 - prev_answer_similarity) * b_prevan)
}
# Store row
participant_data <- rbind(participant_data, data.frame(
Participant = participant_id,
Concept = concept_id,
Modality = modality,
Big5 = Big5[participant_id],
Familiarity = Familiarity[participant_id],
Expressibility = expressibility_score,
CommAtt = attempt,
Eff = Eff,
TrialNumber = trial_number,
PrevAn = prev_answer_similarity
))
# Update for next attempt
trial_number <- trial_number + 1
prev_answer_similarity <- runif(1, min = 0, max = 1)  # Simulate next similarity
}
}
return(participant_data)
}
# Simulate data for all participants
final_data_synt <- do.call(rbind, lapply(participants, simulate_participant))
# Preview results
head(final_data_synt)
# load our data back if we lost them
final_data <- read_csv(paste0(datasets, "synthetic_data.csv"))
# Calculate Effort Change (Difference)
final_data_2 <- final_data %>%
group_by(Participant, Concept) %>%
mutate(
Effort_1 = Eff[CommAtt == 1][1],  # Effort for attempt 1
Effort_2 = Eff[CommAtt == 2][1],  # Effort for attempt 2
Effort_3 = Eff[CommAtt == 3][1],  # Effort for attempt 3
Effort_Change_1_to_2 = case_when(
CommAtt == 2 & !is.na(Effort_1) ~ Eff - Effort_1,  # Change from 1st to 2nd attempt
TRUE ~ NA_real_
),
Effort_Change_2_to_3 = case_when(
CommAtt == 3 & !is.na(Effort_2) ~ Eff - Effort_2,  # Change from 2nd to 3rd attempt
TRUE ~ NA_real_
)
) %>%
ungroup()
# Collide changes into a single column
final_data_2 <- final_data_2 %>%
mutate(
Effort_Change = coalesce(Effort_Change_1_to_2, Effort_Change_2_to_3)
)
# Remove unnecessary columns
final_data_2 <- subset(final_data_2, select = -c(Effort_1, Effort_2, Effort_3, Effort_Change_1_to_2, Effort_Change_2_to_3))
# View the result
head(final_data_2)
View(final_data_2)
View(final_data_2)
# Filter out CommAtt == 1
filtered_data <- final_data_2[final_data_2$CommAtt != 1, ]
ggplot(filtered_data, aes(x = PrevAn, y = Eff)) +
geom_point(alpha = 0.6, color = "blue") +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Regression line
labs(x = "Previous Answer Similarity (PrevAn)",
y = "Effort (Eff)",
title = "Relationship between Effort and Previous Answer Similarity") +
theme_minimal()
ggplot(filtered_data, aes(x = PrevAn, y = Effort_Change)) +
geom_point(alpha = 0.6, color = "blue") +  # Scatter points
geom_smooth(method = "lm", color = "red", se = FALSE) +  # Regression line
labs(x = "Previous Answer Similarity (PrevAn)",
y = "Effort (Eff)",
title = "Relationship between Effort and Previous Answer Similarity") +
theme_minimal()
hist(filtered_data$Effort_Change, breaks=100)
