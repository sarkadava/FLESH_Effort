---
title: "Modeling-effort"
author: "Sarka Kadava"
date: "2024-10-14"
output: html_document
---

# Introduction

The experimental design is described in the preregistration here: https://osf.io/m9xep (@Daniela, you can skip the section Lab Equipment)

In this markdown, we will be modelling the causal relation between effort and correction to confirm/reject our hypothesis.

These are:

H1: In corrections, people will enhance some effort-related kinematic and/or acoustic features of their behaviour

H2: The enhancement will depend on similarity of the guesser's answer and the original meaning. More similar answer will require/result in smaller enhancement (but still enhancement) than less similar answer.

# Setting up the environment

```{r warning=FALSE}

library(here)
library(dagitty) # for dag
library(dplyr) # for data-wrangling
library(lme4)  # for linear mixed-effects models
library(tidyr)  # for reshaping data (if needed)

```

# DAG

To make sure we know what variables should be in the model and why, we draw a DAG that illustrates our assumptions

Our relationship of interest is the causal effect of communicative attempt on effort.

Other assumptions include:

Personality traits (measured with Big5) will influence effort (e.g., people are more willing to put more effort if they are open-minded) and also communicative attempt (e.g., more extroverted people are better in this game, therefore they need less attempts)

Familiarity with guessing partner influences effort (ditto) as well as communicative attempt (e.g., friends are better in this game than strangers)

Similarly, participant will also directly infleunce effort and commAtt, because personality traits might not be capturing all variation

Expressibility of concepts is going to influence effort (e.g., more expressible concepts allow more enhancement - but could be also other direction) as well as CommAtt (e.g., more expressible concepts are more readily guessed and don't need more attempts)

Similarly, concept will also directly influence effort and commAtt, because expressibility might not be capturing all variation

Modality (uni vs. multi) will influence the value of effort. We assume that in unimodal condition, the feature does not need to account for synergic relations with the other modality, and may carry the whole signal. In multimodal condition, however, there may be synergic relations that moderate the full expressiveness of this feature

Lastly, trial number (characterising how for one is in the experiment) could be hinting on learning processes through out the experiment, or - the other direction - on increasing fatigue


```{r warning=FALSE}
dag <- dagitty('dag {
Big5 [adjusted,pos="-0.823,0.657"]
CommAtt [exposure,pos="-1.033,0.028"]
Conc [adjusted,pos="-1.136,-0.848"]
Eff [outcome,pos="-0.102,0.025"]
Expr [adjusted,pos="-0.758,-0.850"]
Fam [adjusted,pos="-0.379,0.663"]
Mod_cat [adjusted,pos="-0.374,-0.850"]
Pcn [adjusted,pos="-0.589,1.214"]
TrNum [adjusted,pos="-1.686,-0.859"]
Big5 -> CommAtt     
Big5 -> Eff
CommAtt -> Eff
Conc -> Expr
Expr -> CommAtt
Expr -> Eff
Fam -> CommAtt
Fam -> Eff
Mod_bin -> Eff
Mod_cat -> Expr
Pcn -> Big5
Pcn -> CommAtt 
Pcn -> Eff
Pcn -> Fam
TrNum -> CommAtt
TrNum -> Eff
Conc -> CommAtt
Conc -> Eff
}')

plot(dag)



```






# Synthetic data

We will now create synth data that will copy the relations we assume in our DAG. Assigning concrete coefficients will also help us to test our model - we should find exactly those causalities that we had in mind when creating these data

```{r}

# Set seed for reproducibility
set.seed(0209)

# Define participants, total unique concepts, and modalities
n_participants <- 120
n_total_concepts <- 21  # Total unique concepts
n_concepts_per_participant <- 21  # Each participant works with 21 concepts
n_modalities <- 3  # gesture, vocal, combined

# Generate participant IDs
participants <- 1:n_participants

# Simulate Big5 personality traits (standardized between 0 and 1) and Familiarity (between 0 and 1) for participants
Big5 <- runif(n_participants, min = 0, max = 1)  # Continuous values between 0 and 1
Familiarity <- runif(n_participants, min = 0, max = 1)  # Continuous values between 0 and 1

# Create a matrix to hold expressibility values for each concept in each modality
expressibility_matrix <- matrix(runif(n_total_concepts * n_modalities, min = 0, max = 1), nrow = n_total_concepts, ncol = n_modalities)

# Randomly sample 21 unique concepts for each participant
final_data <- data.frame()

# Define a function to assign CommAtt and Eff for a single participant
simulate_participant <- function(participant_id) {
  # Randomly sample 21 unique concepts from the total pool of 84
  selected_concepts <- sample(1:n_total_concepts, n_concepts_per_participant)
  
  participant_data <- data.frame()
  trial_number <- 1  # Initialize trial number
  
  for (concept_id in selected_concepts) {
    # Randomly determine the modality for the concept
    modality <- sample(c("gesture", "vocal", "combined"), 1)
    
    # Calculate expressibility based on modality
    expressibility_score <- ifelse(modality == "vocal", expressibility_matrix[concept_id, 1] * 0.6, 
                                    ifelse(modality == "gesture", expressibility_matrix[concept_id, 2], 
                                           expressibility_matrix[concept_id, 3] * 1.2))
    
    # Determine Communicative Attempts based solely on expressibility, familiarity, and Big5
    base_prob <- c(0.33, 0.33, 0.33)  # Equal chance for 1, 2, or 3 attempts
    
    # Modify probabilities based on familiarity, Big5, and expressibility
    adjusted_prob <- base_prob * c(1 - Familiarity[participant_id], # 3 times for each
                                    1 - Familiarity[participant_id],
                                    1 - Familiarity[participant_id]) * 
                     c(1 - Big5[participant_id],
                       1 - Big5[participant_id],
                       1 - Big5[participant_id]) * 
                     c(1 - expressibility_score,
                       1 - expressibility_score,
                       1 - expressibility_score)
    
    # Normalize the adjusted probabilities
    adjusted_prob <- adjusted_prob / sum(adjusted_prob)
    
    # Sample the number of communicative attempts based on adjusted probabilities
    n_attempts <- sample(1:3, 1, prob = adjusted_prob)
    
    # Loop through the number of attempts and increment CommAtt correctly
    for (attempt in 1:n_attempts) {
      # Calculate Eff for the first attempt
      if (attempt == 1) {
        Eff <- 1.15 * Big5[participant_id] + 
               1.10 * Familiarity[participant_id] + 
               1.20 * expressibility_score + 
               rnorm(1, mean = 1, sd = 0.5)
        
        # Adjust Eff based on modality
        if (modality == "combined") {
          Eff <- Eff * 0.7  # Slight moderation for combined modality
        }
      }
      
      # Adjust Eff for subsequent attempts
      if (attempt == 2) {
        Eff <- Eff * 1.50  # Multiply effort by 1.50 for the second attempt
      } else if (attempt == 3) {
        Eff <- Eff * 0.70  # Multiply effort by 0.70 for the third attempt
      }
      
      # Create row for each attempt
      participant_data <- rbind(participant_data, data.frame(
        Participant = participant_id,
        Concept = concept_id,
        Modality = modality,
        Big5 = Big5[participant_id],
        Familiarity = Familiarity[participant_id],
        Expressibility = expressibility_score,
        CommAtt = attempt,  # Correctly set the attempt number
        Eff = Eff,
        TrialNumber = trial_number  # Set trial number for this attempt
      ))
      
      # Increment the trial number after each attempt
      trial_number <- trial_number + 1
    }
  }
  
  return(participant_data)
}

# Simulate data for all participants
for (i in participants) {
  final_data <- rbind(final_data, simulate_participant(i))
}

# Preview the first few rows of the final data
head(final_data)



```
So now we have synthetic data where we exactly defined (using coefficients) what is the relationship between certain variables

1) CommAtt -> Eff

The effort for second attempt is multiplied by 1.25 (Beta = 1.25)
The effort for third attempts by 0.9 (ie decreases, Beta = 0.9)

2) Fam -> Eff & CommAtt

Beta = 1.10 for Eff
Negative effect on CommAtt


3) Big5 -> Eff & CommAtt

Beta = 1.15 for Eff
Negative effect on CommAtt

4) Expr -> Eff & CommAtt

Beta = 1.20 for Eff
Negative effect on CommAtt

5) TrNum -> Eff & CommAtt 

Beta = could be both neg (fatigue) or positive (learning)


# Simple linear mixed model

Before we do full Bayesian, we do quick LMM to see whether we are on good track

```{r}

# Convert necessary columns to factors
final_data$CommAtt <- as.factor(final_data$CommAtt)
final_data$Modality <- as.factor(final_data$Modality)
final_data$Participant <- as.factor(final_data$Participant)
final_data$Concept <- as.factor(final_data$Concept)
final_data$TrialNumber <- as.numeric(final_data$TrialNumber)  # Ensure TrialNumber is numeric

# Fit the linear mixed-effects model
model <- lmer(Eff ~ CommAtt + Familiarity + Big5 + Expressibility + TrialNumber + Modality + (1 | Participant) + (1 | Concept), data = final_data)

# Summary of the model
summary(model)

# Check model diagnostics
# Plot residuals vs fitted values
plot(model)

# Extract coefficients
coefficients <- summary(model)$coefficients
print(coefficients)

# If you want to save the model output
saveRDS(model, "linear_mixed_effects_model.rds")

summary(model)

```

The Betas seem quite close to how we create the synthetic data, probably there are some other moderations since the causal relations between the variables are quite complex


# Bayesian model

Potential ways:

- LMMs - but we assume there might be some non-linearity (e.g., effort will increase in 2nd attempt, but not third)
- b-splines - but our 'timeline' consists of only three points (attempts). We could try but maybe b-splines are unncessarilly complex
- ordinal regression
    - see https://solomonkurz.netlify.app/blog/2023-05-21-causal-inference-with-ordinal-regression/
    and Chapters 12-13 in McElreath



