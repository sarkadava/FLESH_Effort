
@inproceedings{chen_guestrin16,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  shorttitle = {{{XGBoost}}},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  date = {2016},
  series = {{{KDD}} '16},
  pages = {785--794},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2939672.2939785},
  url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
  urldate = {2025-02-12},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  isbn = {978-1-4503-4232-2}
}

@inproceedings{fuchsetal2016,
author = {Fuchs, Susanne and Reichel, Uwe and Rochet-Capellan, Amélie},
year = {2016},
month = {03},
pages = {},
title = {F0 declination and speech planning in face to face dialogues},
doi = {10.13140/RG.2.1.4909.0320}
}

@article{seth_etal18,
  title = {{{OpenSim}}: {{Simulating}} Musculoskeletal Dynamics and Neuromuscular Control to Study Human and Animal Movement},
  shorttitle = {{{OpenSim}}},
  author = {Seth, Ajay and Hicks, Jennifer L. and Uchida, Thomas K. and Habib, Ayman and Dembia, Christopher L. and Dunne, James J. and Ong, Carmichael F. and DeMers, Matthew S. and Rajagopal, Apoorva and Millard, Matthew and Hamner, Samuel R. and Arnold, Edith M. and Yong, Jennifer R. and Lakshmikanth, Shrinidhi K. and Sherman, Michael A. and Ku, Joy P. and Delp, Scott L.},
  year = {2018},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {14},
  number = {7},
  pages = {e1006223},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006223},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006223},
  urldate = {2025-03-24},
  abstract = {Movement is fundamental to human and animal life, emerging through interaction of complex neural, muscular, and skeletal systems. Study of movement draws from and contributes to diverse fields, including biology, neuroscience, mechanics, and robotics. OpenSim unites methods from these fields to create fast and accurate simulations of movement, enabling two fundamental tasks. First, the software can calculate variables that are difficult to measure experimentally, such as the forces generated by muscles and the stretch and recoil of tendons during movement. Second, OpenSim can predict novel movements from models of motor control, such as kinematic adaptations of human gait during loaded or inclined walking. Changes in musculoskeletal dynamics following surgery or due to human–device interaction can also be simulated; these simulations have played a vital role in several applications, including the design of implantable mechanical devices to improve human grasping in individuals with paralysis. OpenSim is an extensible and user-friendly software package built on decades of knowledge about computational modeling and simulation of biomechanical systems. OpenSim’s design enables computational scientists to create new state-of-the-art software tools and empowers others to use these tools in research and clinical applications. OpenSim supports a large and growing community of biomechanics and rehabilitation researchers, facilitating exchange of models and simulations for reproducing and extending discoveries. Examples, tutorials, documentation, and an active user forum support this community. The OpenSim software is covered by the Apache License 2.0, which permits its use for any purpose including both nonprofit and commercial applications. The source code is freely and anonymously accessible on GitHub, where the community is welcomed to make contributions. Platform-specific installers of OpenSim include a GUI and are available on simtk.org.},
  langid = {english},
  keywords = {Ankles,Computer software,Medical devices and equipment,Musculoskeletal mechanics,Reflexes,Simulation and modeling,Skeletal joints,Tendons}
}

@article{pagnon_etal22,
  title = {{{Pose2Sim}}: {{An End-to-End Workflow}} for {{3D Markerless Sports Kinematics}}—{{Part}} 2: {{Accuracy}}},
  shorttitle = {{{Pose2Sim}}},
  author = {Pagnon, David and Domalain, Mathieu and Reveret, Lionel},
  date = {2022},
  journaltitle = {Sensors},
  volume = {22},
  number = {7},
  pages = {2712},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s22072712},
  url = {https://www.mdpi.com/1424-8220/22/7/2712},
  urldate = {2025-03-24},
  abstract = {Two-dimensional deep-learning pose estimation algorithms can suffer from biases in joint pose localizations, which are reflected in triangulated coordinates, and then in 3D joint angle estimation. Pose2Sim, our robust markerless kinematics workflow, comes with a physically consistent OpenSim skeletal model, meant to mitigate these errors. Its accuracy was concurrently validated against a reference marker-based method. Lower-limb joint angles were estimated over three tasks (walking, running, and cycling) performed multiple times by one participant. When averaged over all joint angles, the coefficient of multiple correlation (CMC) remained above 0.9 in the sagittal plane, except for the hip in running, which suffered from a systematic 15° offset (CMC = 0.65), and for the ankle in cycling, which was partially occluded (CMC = 0.75). When averaged over all joint angles and all degrees of freedom, mean errors were 3.0°, 4.1°, and 4.0°, in walking, running, and cycling, respectively; and range of motion errors were 2.7°, 2.3°, and 4.3°, respectively. Given the magnitude of error traditionally reported in joint angles computed from a marker-based optoelectronic system, Pose2Sim is deemed accurate enough for the analysis of lower-body kinematics in walking, cycling, and running.},
  issue = {7},
  langid = {english},
  keywords = {accuracy,computer vision,concurrent validity,deep learning,kinematics,markerless motion capture,OpenPose,OpenSim,sports performance analysis}
}

@online{cao_etal19,
  title = {{{OpenPose}}: {{Realtime Multi-Person 2D Pose Estimation}} Using {{Part Affinity Fields}}},
  shorttitle = {{{OpenPose}}},
  author = {Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  date = {2019},
  eprint = {1812.08008},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.08008},
  url = {http://arxiv.org/abs/1812.08008},
  urldate = {2025-03-24},
  abstract = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}


@article{yan_etal20,
  title = {Unexpected Complexity of Everyday Manual Behaviors},
  author = {Yan, Yuke and Goodman, James M. and Moore, Dalton D. and Solla, Sara A. and Bensmaia, Sliman J.},
  date = {2020},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {3564},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17404-0},
  url = {https://www.nature.com/articles/s41467-020-17404-0},
  urldate = {2024-03-28},
  abstract = {How does the brain control an effector as complex and versatile as the hand? One possibility is that neural control is simplified by limiting the space of hand movements. Indeed, hand kinematics can be largely described within 8 to 10 dimensions. This oft replicated finding has been construed as evidence that hand postures are confined to this subspace. A prediction from this hypothesis is that dimensions outside of this subspace reflect noise. To address this question, we track the hand of human participants as they perform two tasks—grasping and signing in American Sign Language. We apply multiple dimension reduction techniques and replicate the finding that most postural variance falls within a reduced subspace. However, we show that dimensions outside of this subspace are highly structured and task dependent, suggesting they too are under volitional control. We propose that hand control occupies a higher dimensional space than previously considered.},
  langid = {english},
  keywords = {Brain–machine interface,Motor control},
  file = {C:\Users\Sarka Kadava\Zotero\storage\GW945IKZ\Yan et al. - 2020 - Unexpected complexity of everyday manual behaviors.pdf}
}

@article{trujillo_etal18,
  title = {Communicative Intent Modulates Production and Comprehension of Actions and Gestures: {{A Kinect}} Study},
  shorttitle = {Communicative Intent Modulates Production and Comprehension of Actions and Gestures},
  author = {Trujillo, James and Simanova, Irina and Bekkering, Harold and Özyürek, Asli},
  date = {2018},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {180},
  pages = {38--51},
  issn = {00100277},
  doi = {10.1016/j.cognition.2018.04.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027718300957},
  urldate = {2022-10-11},
  abstract = {Actions may be used to directly act on the world around us, or as a means of communication. Effective communication requires the addressee to recognize the act as being communicative. Humans are sensitive to ostensive communicative cues, such as direct eye gaze (Csibra \& Gergely, 2009). However, there may be additional cues present in the action or gesture itself. Here we investigate features that characterize the initiation of a communicative interaction in both production and comprehension.},
  langid = {english},
  file = {C:\Users\Sarka Kadava\Zotero\storage\ZJ2MXEVU\Trujillo et al. - 2018 - Communicative intent modulates production and comp.pdf}
}


@article{berisha_etal14,
  title = {Characterizing the Distribution of the Quadrilateral Vowel Space Area},
  author = {Berisha, Visar and Sandoval, Steven and Utianski, Rene and Liss, Julie and Spanias, Andreas},
  date = {2014},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {J Acoust Soc Am},
  volume = {135},
  number = {1},
  eprint = {24437782},
  eprinttype = {pmid},
  pages = {421--427},
  issn = {0001-4966},
  doi = {10.1121/1.4829528},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3985878/},
  urldate = {2025-03-19},
  abstract = {The vowel space area (VSA) has been studied as a quantitative index of intelligibility to the extent it captures articulatory working space and reductions therein. The majority of such studies have been empirical wherein measures of VSA are correlated with perceptual measures of intelligibility. However, the literature contains minimal mathematical analysis of the properties of this metric. This paper further develops the theoretical underpinnings of this metric by presenting a detailed analysis of the statistical properties of the VSA and characterizing its distribution through the moment generating function. The theoretical analysis is confirmed by a series of experiments where empirically estimated and theoretically predicted statistics of this function are compared. The results show that on the Hillenbrand and TIMIT data, the theoretically predicted values of the higher-order statistics of the VSA match very well with the empirical estimates of the same.},
  pmcid = {PMC3985878}
}

@article{zywiczynski_etal24,
  title = {Praxis, Demonstration and Pantomime: A Motion Capture Investigation of Differences in Action Performances},
  shorttitle = {Praxis, Demonstration and Pantomime},
  author = {Żywiczyński, Przemysław and Placiński, Marek and Sibierska, Marta and Boruta-Żywiczyńska, Monika and Wacewicz, Sławomir and Meina, Michał and Gärdenfors, Peter},
  date = {2024},
  journaltitle = {Language and Cognition},
  pages = {1--28},
  issn = {1866-9808, 1866-9859},
  doi = {10.1017/langcog.2024.8},
  url = {https://www.cambridge.org/core/journals/language-and-cognition/article/praxis-demonstration-and-pantomime-a-motion-capture-investigation-of-differences-in-action-performances/F3BC1FC80945EFAB141C24D6A3267D4D},
  urldate = {2024-05-28},
  abstract = {A commonly held assumption is that demonstration and pantomime differ from ordinary action in that the movements are slowed down and exaggerated to be better understood by intended receivers. This claim has, however, been based on meagre empirical support. This article provides direct evidence that the different functional demands of demonstration and pantomime result in motion characteristics that differ from those for praxic action. In the experiment, participants were dressed in motion capture suits and asked to (1) perform an action, (2) demonstrate this action so that somebody else could learn how to perform it, (3) pantomime this action without using the object so that somebody else could learn how to perform it, and (4) pantomime this action without using the object so that somebody else could distinguish it from another action. The results confirm that actors slow down and exaggerate their movements in demonstrations and pantomimes when compared to ordinary actions.},
  langid = {english},
  keywords = {demonstration,evolution of language,motion capture,pantomime,teaching},
  file = {C:\Users\Sarka Kadava\Zotero\storage\28883NAK\Żywiczyński et al. - 2024 - Praxis, demonstration and pantomime a motion capt.pdf}
}

@article{daffertshofer_etal04,
  title = {{{PCA}} in Studying Coordination and Variability: A Tutorial},
  shorttitle = {{{PCA}} in Studying Coordination and Variability},
  author = {Daffertshofer, Andreas and Lamoth, Claudine J. C. and Meijer, Onno G. and Beek, Peter J.},
  date = {2004},
  journaltitle = {Clinical Biomechanics (Bristol, Avon)},
  shortjournal = {Clin Biomech (Bristol)},
  volume = {19},
  number = {4},
  eprint = {15109763},
  eprinttype = {pmid},
  pages = {415--428},
  issn = {0268-0033},
  doi = {10.1016/j.clinbiomech.2004.01.005},
  abstract = {OBJECTIVE: To explain and underscore the use of principal component analysis in clinical biomechanics as an expedient, unbiased means for reducing high-dimensional data sets to a small number of modes or structures, as well as for teasing apart structural (invariant) and variable components in such data sets. DESIGN: The method is explained formally and then applied to both simulated and real (kinematic and electromyographic) data for didactical purposes, thus illustrating possible applications (and pitfalls) in the study of coordinated movement. BACKGROUND: In the sciences at large, principal component analysis is a well-known method to remove redundant information in multidimensional data sets by means of mode reduction. At present, principal component analysis is starting to penetrate the fundamental and clinical study of human movement, which amplifies the need for an accessible explanation of the method and its possibilities and limitations. Besides mode reduction, we discuss principal component analysis in its capacity as a data-driven filter, allowing for a separation of invariant and variant properties of coordination, which, arguably, is essential in studies of motor variability. METHODS: Principal component analysis is applied to kinematic and electromyographic time series obtained during treadmill walking by healthy humans. RESULTS: Common signal structures or modes are identified in the time series that turn out to be readily interpretable. In addition, the identified coherent modes are eliminated from the data, leaving a filtered, residual pattern from which useful information may be gleaned regarding motor variability. CONCLUSIONS: Principal component analysis allows for the detection of modes (information reduction) in both kinematic and electromyographic data sets, as well as for the separation of invariant structure and variance in those data sets. RELEVANCE: Principal component analysis can be successfully applied to movement data, both as feature extractor and as data-driven filter. Its potential for the (clinical) study of human movement sciences (e.g., diagnostics and evaluation of interventions) is evident but still largely untapped.},
  langid = {english},
  keywords = {Algorithms,Electromyography,Humans,Models Biological,Models Statistical,Movement,Postural Balance,Principal Component Analysis,Reproducibility of Results,Sensitivity and Specificity,Signal Processing Computer-Assisted,Walking},
  file = {C:\Users\Sarka Kadava\Zotero\storage\WH5VR44B\Daffertshofer et al. - 2004 - PCA in studying coordination and variability a tu.pdf}
}

@article{tadmorBorrowabilityNotionBasic2010,
  title = {Borrowability and the Notion of Basic Vocabulary},
  author = {Tadmor, Uri and Haspelmath, Martin and Taylor, Bradley},
  date = {2010-01-01},
  journaltitle = {Diachronica},
  volume = {27},
  number = {2},
  pages = {226--246},
  publisher = {John Benjamins},
  issn = {0176-4225, 1569-9714},
  doi = {10.1075/dia.27.2.04tad},
  url = {https://www.jbe-platform.com/content/journals/10.1075/dia.27.2.04tad},
  urldate = {2023-01-26},
  abstract = {This paper reports on a collaborative quantitative study of loanwords in 41 languages, aimed at identifying meanings and groups of meanings that are borrowing-resistant. We find that nouns are more borrowable than adjectives or verbs, that content words are more borrowable than function words, and that different semantic fields also show different proportions of loanwords. Several issues arise when one tries to establish a list of the most borrowing-resistant meanings: Our data include degrees of likelihood of borrowing, not all meanings have counterparts in all languages, many words are compounds or derivatives and hence almost by definition non-loanwords. We also have data on the age of words. There are thus multiple factors that play a role, and we propose a way of combining the factors to yield a new 100-item list of basic vocabulary, called the Leipzig-Jakarta list.},
  langid = {english},
  file = {C:\Users\Sarka Kadava\Zotero\storage\5GCL3AMT\dia.27.2.html}
}

@software{nalbantoglu_kadava25,
  title = {Multi-{{Scenario Video}} and {{Audio Synchronization}} and {{Segmentation}}},
  author = {Nalbantoğlu, Hamza and Kadavá, Šárka},
  date = {2025},
  url = {https://github.com/hamzanalbantoglu/flexible_audio_video_sync},
  version = {1.0.0}
}


@article{lynottModalityExclusivityNorms2009,
  title = {Modality Exclusivity Norms for 423 Object Properties},
  author = {Lynott, Dermot and Connell, Louise},
  date = {2009-05-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behavior Research Methods},
  volume = {41},
  number = {2},
  pages = {558--564},
  issn = {1554-3528},
  doi = {10.3758/BRM.41.2.558},
  url = {https://doi.org/10.3758/BRM.41.2.558},
  urldate = {2023-01-19},
  abstract = {Recent work has shown that people routinely use perceptual information during language comprehension and conceptual processing, from single-word recognition to modality-switching costs in property verification. In investigating such links between perceptual and conceptual representations, the use of modality-specific stimuli plays a central role. To aid researchers working in this area, we provide a set of norms for 423 adjectives, each describing an object property, with mean ratings of how strongly that property is experienced through each of five perceptual modalities (visual, haptic, auditory, olfactory, and gustatory). The data set also contains estimates of modality exclusivity—that is, a measure of the extent to which a particular property may be considered unimodal (i.e., perceived through one sense alone). Although there already exists a number of sets of word and object norms, we provide the first set to categorize words describing object properties along the dimensions of the five perceptual modalities. We hope that the norms will be of use to researchers working at the interface between linguistic, conceptual, and perceptual systems. The modality exclusivity norms may be downloaded as supplemental materials for this article from brm.psychonomic-journals.org/ content/supplemental.},
  langid = {english}
}

@software{kadava_etal24a,
  title = {Recording from {{Multiple Webcams Synchronously}} While {{LSL Streaming}}},
  author = {Kadavá, Šárka and Snelders, Justin and Pouw, Wim},
  date = {2024},
  url = {https://github.com/sarkadava/multiple_webcam_recording_for3Dtracking},
  version = {1.0.0}
}

@article{lynottLancasterSensorimotorNorms2020,
  title = {The {{Lancaster Sensorimotor Norms}}: Multidimensional Measures of Perceptual and Action Strength for 40,000 {{English}} Words},
  shorttitle = {The {{Lancaster Sensorimotor Norms}}},
  author = {Lynott, Dermot and Connell, Louise and Brysbaert, Marc and Brand, James and Carney, James},
  date = {2020-06},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {52},
  number = {3},
  pages = {1271--1291},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01316-z},
  url = {http://link.springer.com/10.3758/s13428-019-01316-z},
  urldate = {2022-10-11},
  abstract = {Sensorimotor information plays a fundamental role in cognition. However, the existing materials that measure the sensorimotor basis of word meanings and concepts have been restricted in terms of their sample size and breadth of sensorimotor experience. Here we present norms of sensorimotor strength for 39,707 concepts across six perceptual modalities (touch, hearing, smell, taste, vision, and interoception) and five action effectors (mouth/throat, hand/arm, foot/leg, head excluding mouth/throat, and torso), gathered from a total of 3,500 individual participants using Amazon’s Mechanical Turk platform. The Lancaster Sensorimotor Norms are unique and innovative in a number of respects: They represent the largest-ever set of semantic norms for English, at 40,000 words × 11 dimensions (plus several informative cross-dimensional variables), they extend perceptual strength norming to the new modality of interoception, and they include the first norming of action strength across separate bodily effectors. In the first study, we describe the data collection procedures, provide summary descriptives of the dataset, and interpret the relations observed between sensorimotor dimensions. We then report two further studies, in which we (1) extracted an optimal singlevariable composite of the 11-dimension sensorimotor profile (Minkowski 3 strength) and (2) demonstrated the utility of both perceptual and action strength in facilitating lexical decision times and accuracy in two separate datasets. These norms provide a valuable resource to researchers in diverse areas, including psycholinguistics, grounded cognition, cognitive semantics, knowledge representation, machine learning, and big-data approaches to the analysis of language and conceptual representations. The data are accessible via the Open Science Framework (http://osf.io/7emr6/) and an interactive web application (https://www. lancaster.ac.uk/psychology/lsnorms/).},
  langid = {english},
  file = {C:\Users\Sarka Kadava\Zotero\storage\Y2IIZWSB\Lynott et al. - 2020 - The Lancaster Sensorimotor Norms multidimensional.pdf}
}

@article{werneretal2024,
  author = {Raphael Werner  and Susanne Fuchs  and Jürgen Trouvain  and Steffen Kürbis  and Bernd Möbius  and Peter Birkholz },
  title = {Acoustics of Breath Noises in Human Speech: Descriptive and Three-Dimensional Modeling Approaches},
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {67},
  number = {10S},
  pages  = {3947-3961},
  year = {2024},
  doi = {10.1044/2023\_JSLHR-23-00112},
  URL = {https://pubs.asha.org/doi/abs/10.1044/2023_JSLHR-23-00112},
  eprint = {https://pubs.asha.org/doi/pdf/10.1044/2023_JSLHR-23-00112}
  }

@inproceedings{xiongetal2002,
author = {Xiong, Yingen and Quek, Francis and Mcneill, David},
year = {2002},
month = {02},
pages = {179- 184},
title = {Hand gesture symmetric behavior detection and analysis in natural conversation},
isbn = {0-7695-1834-6},
doi = {10.1109/ICMI.2002.1166989}
}


@inproceedings{pouw_etal21b,
  title = {Semantically {{Related Gestures Move Alike}}: {{Towards}} a {{Distributional Semantics}} of {{Gesture Kinematics}}},
  shorttitle = {Semantically {{Related Gestures Move Alike}}},
  booktitle = {Digital {{Human Modeling}} and {{Applications}} in {{Health}}, {{Safety}}, {{Ergonomics}} and {{Risk Management}}. {{Human Body}}, {{Motion}} and {{Behavior}}},
  author = {Pouw, Wim and family=Wit, given=Jan, prefix=de, useprefix=true and Bögels, Sara and Rasenberg, Marlou and Milivojevic, Branka and Ozyurek, Asli},
  editor = {Duffy, Vincent G.},
  date = {2021},
  pages = {269--287},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-77817-0_20},
  abstract = {Most manual communicative gestures that humans produce cannot be looked up in a dictionary, as these manual gestures inherit their meaning in large part from the communicative context and are not conventionalized. However, it is understudied to what extent the communicative signal as such—bodily postures in movement, or kinematics—can inform about gesture semantics. Can we construct, in principle, a distribution-based semantics of gesture kinematics, similar to how word vectorization methods in NLP (Natural language Processing) are now widely used to study semantic properties in text and speech? For such a project to get off the ground, we need to know the extent to which semantically similar gestures are more likely to be kinematically similar. In study 1 we assess whether semantic word2vec distances between the conveyed concepts participants were explicitly instructed to convey in silent gestures, relate to the kinematic distances of these gestures as obtained from Dynamic Time Warping (DTW). In a second director-matcher dyadic study we assess kinematic similarity between spontaneous co-speech gestures produced between interacting participants. Participants were asked before and after they interacted how they would name the objects. The semantic distances between the resulting names were related to the gesture kinematic distances of gestures that were made in the context of conveying those objects in the interaction. We find that the gestures’ semantic relatedness is reliably predictive of kinematic relatedness across these highly divergent studies, which suggests that the development of an NLP method of deriving semantic relatedness from kinematics is a promising avenue for future developments in automated multimodal recognition. Deeper implications for statistical learning processes in multimodal language are discussed.},
  isbn = {978-3-030-77817-0},
  langid = {english}
}

@software{pouw24,
  title = {Wim {{Pouw}}'s {{EnvisionBOX}} Modules for Social Signal Processing},
  author = {Pouw, Wim},
  date = {2024},
  url = {https://github.com/WimPouw/envisionBOX_modulesWP},
  version = {Version 1.0.0}
}

@article{trujilloSpeakersExhibitMultimodal2021,
  title = {Speakers Exhibit a Multimodal {{Lombard}} Effect in Noise},
  author = {Trujillo, James and Özyürek, Asli and Holler, Judith and Drijvers, Linda},
  date = {2021-08-18},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {11},
  number = {1},
  pages = {16721},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-95791-0},
  url = {https://www.nature.com/articles/s41598-021-95791-0},
  urldate = {2022-10-21},
  abstract = {In everyday conversation, we are often challenged with communicating in non-ideal settings, such as in noise. Increased speech intensity and larger mouth movements are used to overcome noise in constrained settings (the Lombard effect). How we adapt to noise in face-to-face interaction, the natural environment of human language use, where manual gestures are ubiquitous, is currently unknown. We asked Dutch adults to wear headphones with varying levels of multi-talker babble while attempting to communicate action verbs to one another. Using quantitative motion capture and acoustic analyses, we found that (1) noise is associated with increased speech intensity and enhanced gesture kinematics and mouth movements, and (2) acoustic modulation only occurs when gestures are not present, while kinematic modulation occurs regardless of co-occurring speech. Thus, in face-to-face encounters the Lombard effect is not constrained to speech but is a multimodal phenomenon where the visual channel carries most of the communicative burden.},
  issue = {1},
  langid = {english},
  keywords = {Human behaviour,Social behaviour},
  file = {C\:\\Users\\Sarka Kadava\\Zotero\\storage\\ZVHP7EU9\\Trujillo et al. - 2021 - Speakers exhibit a multimodal Lombard effect in no.pdf;C\:\\Users\\Sarka Kadava\\Zotero\\storage\\PL7FC79E\\s41598-021-95791-0.html}
}

@article{kadava_etal24,
  title = {What Do We Mean When We Say Gestures Are More Expressive than Vocalizations? {{An}} Experimental and Simulation Study},
  shorttitle = {What Do We Mean When We Say Gestures Are More Expressive than Vocalizations?},
  author = {Kadavá, Šárka and Ćwiek, Aleksandra and Fuchs, Susanne and Pouw, Wim},
  date = {2024},
  journaltitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {46},
  number = {0},
  url = {https://escholarship.org/uc/item/2mp1v3v5},
  urldate = {2025-03-04},
  abstract = {of human language. We focus on the debate between gesture-first and vocalization-first theories. While some evidence supports the idea that gestures played a primary role in early communication, others argue that vocalizations are equally expressive. We think that methodological differences and biases in the choice of concepts may contribute to the challenge of comparing these modalities directly. For example, to what extent does selecting a certain concept from a semantic category matter to reproduce an effect? This and similar questions are explored in a data-driven way. First, we provide ratings on imagined expressibility of 207 concepts from an online experiment showing that people tend to rate gesture modality as better in expressing meaning compared to vocal modality. Second, we use the Bayesian posterior predictive distribution of these ratings to simulate new experiments where we vary the number of participants, number of concepts, and semantic categories to investigate how robust is the difference between gesture and vocal modality. Our results show that gesture modality is reliably different (i.e., affords higher expressibility) than vocal modality. However, the difference between the two is limited in terms of effect size (medium sizes by common standards) so one may question whether this difference is meaningful for bigger claims about early language evolution. This study further provides valuable information for further research on how to select stimuli and how to set up one's design in a balanced way.},
  langid = {english}
}

@online{speer_etal18,
  title = {{{ConceptNet}} 5.5: {{An Open Multilingual Graph}} of {{General Knowledge}}},
  shorttitle = {{{ConceptNet}} 5.5},
  author = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  date = {2018},
  eprint = {1612.03975},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1612.03975},
  url = {http://arxiv.org/abs/1612.03975},
  urldate = {2025-03-04},
  abstract = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language}
}

@software{boersma_weenink25,
  title = {Praat: Doing Phonetics by Computer},
  author = {Boersma, Paul and Weenink, David},
  date = {2025},
  url = {http://www.praat.org/},
  version = {6.4.27}
}

@article{hogan_sternad09,
  title = {Sensitivity of {{Smoothness Measures}} to {{Movement Duration}}, {{Amplitude}}, and {{Arrests}}},
  author = {Hogan, Neville and Sternad, Dagmar},
  date = {2009},
  journaltitle = {Journal of Motor Behavior},
  volume = {41},
  number = {6},
  eprint = {19892658},
  eprinttype = {pmid},
  pages = {529--534},
  publisher = {Routledge},
  issn = {0022-2895},
  doi = {10.3200/35-09-004-RC},
  url = {https://doi.org/10.3200/35-09-004-RC},
  urldate = {2025-03-04},
  abstract = {Studies of sensory-motor performance, including those concerned with changes because of age, disease, or therapeutic intervention, often use measures based on jerk, the time derivative of acceleration, to quantify smoothness and coordination. However, results have been mixed: some researchers report sensitive discrimination of subtle differences, whereas others fail to find significant differences even when they are obviously present. One reason for this is that different measures have been used with different scaling factors. These measures are sensitive to movement amplitude or duration to different degrees. The authors show that jerk-based measures with dimensions vary counterintuitively with movement smoothness, whereas a dimensionless jerk-based measure properly quantifies common deviations from smooth, coordinated movement.},
  keywords = {fluctuations,jerk,learning and recovery,smoothness,submovements}
}


@article{jadoul_etal18,
  title = {Introducing {{Parselmouth}}: {{A Python}} Interface to {{Praat}}},
  shorttitle = {Introducing {{Parselmouth}}},
  author = {Jadoul, Yannick and Thompson, Bill and family=Boer, given=Bart, prefix=de, useprefix=true},
  date = {2018},
  journaltitle = {Journal of Phonetics},
  shortjournal = {Journal of Phonetics},
  volume = {71},
  pages = {1--15},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2018.07.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0095447017301389},
  urldate = {2025-03-31},
  abstract = {This paper introduces Parselmouth, an open-source Python library that facilitates access to core functionality of Praat in Python, in an efficient and programmer-friendly way. We introduce and motivate the package, and present simple usage examples. Specifically, we focus on applications in data visualisation, file manipulation, audio manipulation, statistical analysis, and integration of Parselmouth into a Python-based experimental design for automated, in-the-loop manipulation of acoustic data. Parselmouth is available at https://github.com/YannickJadoul/Parselmouth.},
  keywords = {Acoustics,Data analysis,Phonetics,Praat,Python,Software}
}

@article{tilsen_arvaniti13,
  title = {Speech Rhythm Analysis with Decomposition of the Amplitude Envelope: {{Characterizing}} Rhythmic Patterns within and across Languages},
  shorttitle = {Speech Rhythm Analysis with Decomposition of the Amplitude Envelope},
  author = {Tilsen, Sam and Arvaniti, Amalia},
  date = {2013},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {134},
  number = {1},
  pages = {628--639},
  issn = {0001-4966},
  doi = {10.1121/1.4807565},
  url = {https://doi.org/10.1121/1.4807565},
  urldate = {2025-03-04},
  abstract = {This study presents a method for analyzing speech rhythm using empirical mode decomposition of the speech amplitude envelope, which allows for extraction and quantification of syllabic- and supra-syllabic time-scale components of the envelope. The method of empirical mode decomposition of a vocalic energy amplitude envelope is illustrated in detail, and several types of rhythm metrics derived from this method are presented. Spontaneous speech extracted from the Buckeye Corpus is used to assess the effect of utterance length on metrics, and it is shown how metrics representing variability in the supra-syllabic time-scale components of the envelope can be used to identify stretches of speech with targeted rhythmic characteristics. Furthermore, the envelope-based metrics are used to characterize cross-linguistic differences in speech rhythm in the UC San Diego Speech Lab corpus of English, German, Greek, Italian, Korean, and Spanish speech elicited in read sentences, read passages, and spontaneous speech. The envelope-based metrics exhibit significant effects of language and elicitation method that argue for a nuanced view of cross-linguistic rhythm patterns.}
}

@article{wittenburg_etal06,
  title = {{{ELAN}}: {{A}} Professional Framework for Multimodality Research},
  shorttitle = {{{ELAN}}},
  author = {Wittenburg, Peter and Brugman, Hennie and Russel, Albert and Klassmann, Alex and Sloetjes, Han},
  date = {2006},
  journaltitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006)},
  shortjournal = {Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006)},
  abstract = {Utilization of computer tools in linguistic research has gained importance with the maturation of media frameworks for the handling of digital audio and video. The increased use of these tools in gesture, sign language and multimodal interaction studies has led to stronger requirements on the flexibility, the efficiency and in particular the time accuracy of annotation tools. This paper describes the efforts made to make ELAN a tool that meets these requirements, with special attention to the developments in the area of time accuracy. In subsequent sections an overview will be given of other enhancements in the latest versions of ELAN, that make it a useful tool in multimodality research.}
}

@article{holle_rein15,
  title = {{{EasyDIAg}}: {{A}} Tool for Easy Determination of Interrater Agreement},
  shorttitle = {{{EasyDIAg}}},
  author = {Holle, Henning and Rein, Robert},
  date = {2015},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {47},
  number = {3},
  pages = {837--847},
  issn = {1554-3528},
  doi = {10.3758/s13428-014-0506-7},
  url = {https://doi.org/10.3758/s13428-014-0506-7},
  urldate = {2025-03-03},
  abstract = {Reliable measurements are fundamental for the empirical sciences. In observational research, measurements often consist of observers categorizing behavior into nominal-scaled units. Since the categorization is the outcome of a complex judgment process, it is important to evaluate the extent to which these judgments are reproducible, by having multiple observers independently rate the same behavior. A challenge in determining interrater agreement for timed-event sequential data is to develop clear objective criteria to determine whether two raters’ judgments relate to the same event (the linking problem). Furthermore, many studies presently report only raw agreement indices, without considering the degree to which agreement can occur by chance alone. Here, we present a novel, free, and open-source toolbox (EasyDIAg) designed to assist researchers with the linking problem, while also providing chance-corrected estimates of interrater agreement. Additional tools are included to facilitate the development of coding schemes and rater training.},
  langid = {english},
  keywords = {Agreement,Annotation,Coding,Cohen’s kappa,Rater,Toolbox}
}

@article{landis_koch77,
  title = {The Measurement of Observer Agreement for Categorical Data},
  author = {Landis, J. R. and Koch, G. G.},
  date = {1977},
  journaltitle = {Biometrics},
  volume = {33},
  number = {1},
  eprint = {843571},
  eprinttype = {pmid},
  publisher = {Biometrics},
  issn = {0006-341X},
  url = {https://pubmed.ncbi.nlm.nih.gov/843571/},
  urldate = {2025-03-31},
  abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agre …},
  langid = {english}
}

@article{cwiek_etal_inprep,
  title = {The relationship between expectation and performance: Methodological evaluation},
  author = {Ćwiek, Aleksandra and Fuchs, Susanne and Pouw, Wim and Kadavá, Šárka},
  date = {in prep.},
  journaltitle = {},
  volume = {},
  number = {},
  eprint = {},
  eprinttype = {},
  publisher = {},
  issn = {},
  url = {},
  urldate = {},
  abstract = {},
  langid = {}
}  

@article{cwiek_etal_inprep2,
  title = {Acoustic correlates of perceived phrase-level prosodic prominence in Catalan and German},
  author = {Ćwiek, Aleksandra and Gregori, Alina and Sánchez-Ramón, Paula Ginesa and Prieto, Pilar and Kügler, Frank},
  date = {in prep.},
  journaltitle = {},
  volume = {},
  number = {},
  eprint = {},
  eprinttype = {},
  publisher = {},
  issn = {},
  url = {},
  urldate = {},
  abstract = {},
  langid = {}
}  

@Article{burkner17,
  title = {{brms}: An {R} Package for {Bayesian} Multilevel Models
    Using {Stan}},
  author = {Paul-Christian Bürkner},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01},
  encoding = {UTF-8},
}

@Book{wood17,
  title = {Generalized {A}dditive {M}odels: An Introduction with
    {R}},
  year = {2017},
  author = {S. N. Wood},
  edition = {2},
  publisher = {Chapman and Hall/CRC},
}

@book{mcelreath18,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  date = {2018},
  publisher = {{Chapman and Hall/CRC}},
  location = {New York},
  doi = {10.1201/9781315372495},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds readers’ knowledge of and confidence in statistical modeling. Reflecting the need for even minor programming in today’s model-based statistics, the book pushes readers to perform step-by-step calculations that are usually automated. This unique computational approach ensures that readers understand enough of the details to make reasonable choices and interpretations in their own modeling work. The text presents generalized linear multilevel models from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. It covers from the basics of regression to multilevel models. The author also discusses measurement error, missing data, and Gaussian process models for spatial and network autocorrelation. By using complete R code examples throughout, this book provides a practical foundation for performing statistical inference. Designed for both PhD students and seasoned professionals in the natural and social sciences, it prepares them for more advanced or specialized statistical modeling.  Web ResourceThe book is accompanied by an R package (rethinking) that is available on the author’s website and GitHub. The two core functions (map and map2stan) of this package allow a variety of statistical models to be constructed from standard model formulas.},
  isbn = {978-1-315-37249-5},
  pagetotal = {505}
}

@online{pouw_etal23,
  title = {The Human Voice Aligns with Whole-Body Kinetics},
  author = {Pouw, Wim and Werner, Raphael and Burchardt, Lara and Selen, Luc},
  date = {2023},
  eprinttype = {bioRxiv},
  eprintclass = {Confirmatory Results},
  pages = {2023.11.28.568991},
  doi = {10.1101/2023.11.28.568991},
  url = {https://www.biorxiv.org/content/10.1101/2023.11.28.568991v1},
  urldate = {2025-04-08},
  abstract = {Humans use their voice concurrently with upper limb movements, known as hand gestures. Recently it has been shown that fluctuations in intensity and the tone of the human voice synchronizes with upper limb movement (including gesticulation). In this research direct evidence is provided that the voice changes with arm movements because it interacts with whole-body muscle activity (measured through surface EMG and postural measurements). We show that certain muscles (e.g., pectoralis major) that are associated with posture and upper limb movement are especially likely to interact with the voice. Adding wrist weights to increase the mass of the moving upper limb segment led to increased coupling between movement and voice. These results show that the voice co-patterns with whole-body kinetics relating to force, rather than kinematics, invoking several implications how the voice is biomechanically modeled, how it should be simulated, and importantly how the human voice must have evolved in relation to the whole-body motor system. We concluded that the human voice is animated by the kinetics of the whole body.},
  langid = {english},
  pubstate = {prepublished}
}

@software{rcoreteam16,
  title = {R: {{A}} Language and Environment for Statistical Computing. {{R Foundation}} for {{Statistical Computing}}, {{Vienna}}, {{Austria}}},
  shorttitle = {R},
  author = {R Core Team},
  date = {2016},
  url = {http://www.R-project.org/},
  urldate = {2025-03-31}
}

@article{pouw_etal21,
  title = {A {{Systematic Investigation}} of {{Gesture Kinematics}} in {{Evolving Manual Languages}} in the {{Lab}}},
  author = {Pouw, Wim and Dingemanse, Mark and Motamedi, Yasamin and Özyürek, Aslı},
  date = {2021},
  journaltitle = {Cognitive Science},
  volume = {45},
  number = {7},
  pages = {e13014},
  issn = {1551-6709},
  doi = {10.1111/cogs.13014},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13014},
  urldate = {2025-04-08},
  abstract = {Silent gestures consist of complex multi-articulatory movements but are now primarily studied through categorical coding of the referential gesture content. The relation of categorical linguistic content with continuous kinematics is therefore poorly understood. Here, we reanalyzed the video data from a gestural evolution experiment (Motamedi, Schouwstra, Smith, Culbertson, \& Kirby, 2019), which showed increases in the systematicity of gesture content over time. We applied computer vision techniques to quantify the kinematics of the original data. Our kinematic analyses demonstrated that gestures become more efficient and less complex in their kinematics over generations of learners. We further detect the systematicity of gesture form on the level of thegesture kinematic interrelations, which directly scales with the systematicity obtained on semantic coding of the gestures. Thus, from continuous kinematics alone, we can tap into linguistic aspects that were previously only approachable through categorical coding of meaning. Finally, going beyond issues of systematicity, we show how unique gesture kinematic dialects emerged over generations as isolated chains of participants gradually diverged over iterations from other chains. We, thereby, conclude that gestures can come to embody the linguistic system at the level of interrelationships between communicative tokens, which should calibrate our theories about form and linguistic content.},
  langid = {english},
  keywords = {Iterated learning,Kinematics,Language evolution,Silent gesture,Systematicity}
}

@article{morris_etal19,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  date = {2019},
  journaltitle = {Statistics in Medicine},
  volume = {38},
  number = {11},
  pages = {2074--2102},
  issn = {1097-0258},
  doi = {10.1002/sim.8086},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8086},
  urldate = {2025-04-08},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
  langid = {english},
  keywords = {graphics for simulation,Monte Carlo,simulation design,simulation reporting,simulation studies}
}


